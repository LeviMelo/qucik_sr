{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa50803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:55:59  B0 query: (\"pectus excavatum\"[tiab] OR Nuss[tiab] OR MIRPE[tiab]) AND (\"intercostal nerve cryoablation\"[tiab] OR cryoanalgesia[tiab] OR cryoablation[tiab] OR INC[tiab] OR cryotherapy[tiab])\n",
      "22:56:02  B0 total=108 rq=4.049 stats={'n_sample': 80, 'pi_rate': 0.988, 'outcome_rate': 0.925, 'design_rate': 0.075, 'median_score': 3.248, 'mean_score': 3.203}\n",
      "22:56:05  broad_refined total=542 rq=4.099 stats={'n_sample': 78, 'pi_rate': 0.885, 'outcome_rate': 0.872, 'design_rate': 0.192, 'median_score': 3.316, 'mean_score': 3.133}\n",
      "22:56:07  broad_I_tight total=140 rq=4.053 stats={'n_sample': 80, 'pi_rate': 0.963, 'outcome_rate': 0.925, 'design_rate': 0.075, 'median_score': 3.259, 'mean_score': 3.175}\n",
      "22:56:10  broad_P_core total=231 rq=4.097 stats={'n_sample': 80, 'pi_rate': 0.988, 'outcome_rate': 0.95, 'design_rate': 0.087, 'median_score': 3.279, 'mean_score': 3.272}\n",
      "22:56:13  focused total=112 rq=3.993 stats={'n_sample': 79, 'pi_rate': 0.848, 'outcome_rate': 0.937, 'design_rate': 0.418, 'median_score': 3.213, 'mean_score': 3.12}\n",
      "\n",
      "=== SUMMARY ===\n",
      "Chosen BROAD: (\"pectus excavatum\"[tiab] OR Nuss[tiab] OR MIRPE[tiab] OR \"Thoracic Wall\"[tiab] OR Thoracoscopy[tiab]) AND (\"intercostal nerve cryoablation\"[tiab] OR cryoanalgesia[tiab] OR cryoablation[tiab] OR INC[tiab] OR cryotherapy[tiab] OR Cryosurgery[tiab] OR \"Intercostal Nerves\"[tiab] OR \"Pain Management\"[tiab] OR \"Nerve Block\"[tiab] OR Analgesia[tiab] OR \"Anesthesia, Conduction\"[tiab])\n",
      "  total: 542 rq: 4.099 window_ok: True\n",
      "Chosen FOCUSED: ((\"pectus excavatum\"[tiab] OR Nuss[tiab] OR MIRPE[tiab] OR \"Thoracic Wall\"[tiab] OR Thoracoscopy[tiab]) AND (\"intercostal nerve cryoablation\"[tiab] OR cryoanalgesia[tiab] OR cryoablation[tiab] OR INC[tiab] OR cryotherapy[tiab] OR Cryosurgery[tiab] OR \"Intercostal Nerves\"[tiab] OR \"Pain Management\"[tiab] OR \"Nerve Block\"[tiab] OR Analgesia[tiab] OR \"Anesthesia, Conduction\"[tiab])) AND (randomized[tiab] OR randomised[tiab] OR randomization[tiab] OR \"random allocation\"[tiab])\n",
      "  total: 112 rq: 3.993 window_ok: True\n",
      "Artifacts written under: sniff_poc_out\n"
     ]
    }
   ],
   "source": [
    "# --- SNIFF POC: deterministic, no-LLM boolean building, evidence-driven MeSH mining ---\n",
    "# Copy this entire cell into Jupyter and run. Adjust CONFIG at the top.\n",
    "\n",
    "import os, re, time, json, math, statistics as stats\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "CONFIG = {\n",
    "    \"out_dir\": \"sniff_poc_out\",\n",
    "    \"year_min\": 2015,\n",
    "    \"languages\": [\"English\",\"Portuguese\",\"Spanish\"],\n",
    "    # seed lexical tokens (phrases) — no qualifiers here; we’ll append [tiab] later\n",
    "    \"population_terms_seed\": [\"pectus excavatum\", \"Nuss\", \"MIRPE\"],  # avoid \"adult\" in query; assess age in records if needed\n",
    "    \"intervention_terms_seed\": [\"intercostal nerve cryoablation\", \"cryoanalgesia\", \"cryoablation\", \"INC\", \"cryotherapy\"],\n",
    "    # optional outcome tokens for quality scoring (NOT added to queries)\n",
    "    \"outcome_tokens\": [\"opioid\", \"oxycodone\", \"morphine\", \"hydromorphone\", \"pain\", \"VAS\", \"NRS\"],\n",
    "    # probe sizes & caps\n",
    "    \"probe_ids\": 80,         # how many IDs to sample per candidate when probing\n",
    "    \"fetch_chunk\": 200,\n",
    "    \"broad_target_min\": 10,  # query-size soft window (year-aware)\n",
    "    \"broad_target_max\": 10000,\n",
    "    \"focused_target_min\": 1,\n",
    "    \"focused_target_max\": 2000,\n",
    "    \"max_probe_attempts\": 3,\n",
    "    # scoring weights (heuristic)\n",
    "    \"w_pi\": 1.8,         # P∧I conjunction in TA\n",
    "    \"w_outcome\": 0.9,    # outcome tokens present\n",
    "    \"w_design\": 0.7,     # pubtype primary-ish\n",
    "    \"w_recency\": 0.6,    # scaled [0..1] vs year_min\n",
    "    \"w_tfidf\": 1.2,      # tf-idf sim to lexical seed text (not raw user NLQ)\n",
    "    # pubtype hints\n",
    "    \"primary_pubtypes\": {\n",
    "        \"Randomized Controlled Trial\",\"Clinical Trial\",\"Controlled Clinical Trial\",\n",
    "        \"Prospective Studies\",\"Cohort Studies\",\"Case-Control Studies\",\"Comparative Study\"\n",
    "    },\n",
    "    # focused hedge (lexical only; no [Publication Type] to keep indexing robust)\n",
    "    \"rct_hedge\": ['randomized', 'randomised', 'randomization', 'random allocation'],\n",
    "    # HTTP\n",
    "    \"http_timeout\": 30,\n",
    "    \"email\": \"you@example.com\",  # E-utilities polite usage\n",
    "    \"api_key\": os.environ.get(\"ENTREZ_API_KEY\", \"\")\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# UTIL\n",
    "# =========================\n",
    "\n",
    "def ensure_out_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def save_json(path: str, obj):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def write_tsv(path: str, rows: List[Dict[str, str]], header: List[str]):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\t\".join(header) + \"\\n\")\n",
    "        for r in rows:\n",
    "            f.write(\"\\t\".join(str(r.get(h, \"\")) for h in header) + \"\\n\")\n",
    "\n",
    "def now():\n",
    "    return time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "def norm_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def year_recency(y: Optional[int], ymin: int, current: int = 2025) -> float:\n",
    "    if y is None:\n",
    "        return 0.0\n",
    "    span = max(1, current - ymin)\n",
    "    return max(0.0, min(1.0, (y - ymin) / span))\n",
    "\n",
    "def text_hits(text: str, tokens: List[str]) -> int:\n",
    "    tl = (text or \"\").lower()\n",
    "    n=0\n",
    "    for t in tokens:\n",
    "        t=t.lower()\n",
    "        if t in tl:\n",
    "            n+=1\n",
    "    return n\n",
    "\n",
    "def has_any(text: str, tokens: List[str]) -> bool:\n",
    "    return text_hits(text, tokens) > 0\n",
    "\n",
    "# =========================\n",
    "# PUBMED E-UTILITIES\n",
    "# =========================\n",
    "\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "UA = \"sniff-poc/0.1 (+local)\"\n",
    "HEADERS = {\"User-Agent\": UA, \"Accept\": \"application/json\"}\n",
    "\n",
    "def esearch(query: str, mindate: Optional[int], retmax: int) -> Tuple[int, List[str]]:\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": str(retmax),\n",
    "        \"term\": query,\n",
    "        \"email\": CONFIG[\"email\"]\n",
    "    }\n",
    "    if CONFIG[\"api_key\"]:\n",
    "        params[\"api_key\"] = CONFIG[\"api_key\"]\n",
    "    if mindate:\n",
    "        params[\"mindate\"] = str(mindate)\n",
    "    r = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params=params, timeout=CONFIG[\"http_timeout\"])\n",
    "    r.raise_for_status()\n",
    "    js = r.json().get(\"esearchresult\", {})\n",
    "    count = int(js.get(\"count\", \"0\"))\n",
    "    ids = [str(x) for x in js.get(\"idlist\", [])]\n",
    "    return count, ids\n",
    "\n",
    "def efetch_xml(pmids: List[str], chunk: int = 200) -> str:\n",
    "    xml_all=[]\n",
    "    for i in range(0, len(pmids), chunk):\n",
    "        sub = pmids[i:i+chunk]\n",
    "        params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"id\":\",\".join(sub),\"email\":CONFIG[\"email\"]}\n",
    "        if CONFIG[\"api_key\"]:\n",
    "            params[\"api_key\"] = CONFIG[\"api_key\"]\n",
    "        r = requests.get(f\"{EUTILS}/efetch.fcgi\", headers={\"User-Agent\": UA}, params=params, timeout=CONFIG[\"http_timeout\"])\n",
    "        r.raise_for_status()\n",
    "        xml_all.append(r.text)\n",
    "        time.sleep(0.08)\n",
    "    return \"\\n\".join(xml_all)\n",
    "\n",
    "def parse_pubmed_xml(xml_text: str) -> List[Dict]:\n",
    "    # lightweight XML parsing via regex/ElementTree hybrid to avoid heavy deps\n",
    "    import xml.etree.ElementTree as ET\n",
    "    out=[]\n",
    "    root = ET.fromstring(xml_text)\n",
    "    def join_text(node) -> str:\n",
    "        if node is None: return \"\"\n",
    "        try: return \"\".join(node.itertext())\n",
    "        except Exception: return node.text or \"\"\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = (art.findtext(\".//PMID\") or \"\").strip()\n",
    "        title = join_text(art.find(\".//ArticleTitle\")).strip()\n",
    "        abst_nodes = art.findall(\".//Abstract/AbstractText\")\n",
    "        abstract = \" \".join(join_text(n).strip() for n in abst_nodes) if abst_nodes else \"\"\n",
    "        year = None\n",
    "        for path in (\".//ArticleDate/Year\",\".//PubDate/Year\",\".//DateCreated/Year\",\".//PubDate/MedlineDate\"):\n",
    "            s = art.findtext(path)\n",
    "            if s:\n",
    "                m = re.search(r\"\\b(19|20)\\d{2}\\b\", s)\n",
    "                if m:\n",
    "                    year = int(m.group(0)); break\n",
    "        lang = art.findtext(\".//Language\") or None\n",
    "        pubtypes = [pt.text.strip() for pt in art.findall(\".//PublicationTypeList/PublicationType\") if pt.text]\n",
    "        mesh=[]\n",
    "        for mh in art.findall(\".//MeshHeadingList/MeshHeading\"):\n",
    "            desc = mh.findtext(\"./DescriptorName\") or \"\"\n",
    "            majr = (mh.find(\"./DescriptorName\").attrib.get(\"MajorTopicYN\",\"N\") == \"Y\") if mh.find(\"./DescriptorName\") is not None else False\n",
    "            mesh.append({\"term\": desc, \"majr\": bool(majr)})\n",
    "        out.append({\n",
    "            \"pmid\": pmid, \"title\": title, \"abstract\": abstract, \"year\": year,\n",
    "            \"language\": lang, \"publication_types\": pubtypes, \"mesh\": mesh\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# QUERY BUILDING (LEXICAL-ONLY)\n",
    "# =========================\n",
    "\n",
    "def token_to_tiab(tok: str) -> str:\n",
    "    tok = tok.strip()\n",
    "    if \" \" in tok or \"-\" in tok or \"/\" in tok:\n",
    "        return f\"\\\"{tok}\\\"[tiab]\"\n",
    "    return f\"{tok}[tiab]\"\n",
    "\n",
    "def or_block(terms: List[str]) -> str:\n",
    "    terms = [t for t in terms if t.strip()]\n",
    "    if not terms: return \"\"\n",
    "    return \"(\" + \" OR \".join(token_to_tiab(t) for t in terms) + \")\"\n",
    "\n",
    "def build_broad(pop_terms: List[str], int_terms: List[str]) -> str:\n",
    "    pb = or_block(pop_terms)\n",
    "    ib = or_block(int_terms)\n",
    "    if not pb or not ib:\n",
    "        return \"\"\n",
    "    return f\"{pb} AND {ib}\"\n",
    "\n",
    "def build_focused_from(broad_query: str, rct_hedge: List[str]) -> str:\n",
    "    hedge = \"(\" + \" OR \".join(token_to_tiab(h) for h in rct_hedge) + \")\"\n",
    "    return f\"({broad_query}) AND {hedge}\"\n",
    "\n",
    "# =========================\n",
    "# SCORING\n",
    "# =========================\n",
    "\n",
    "def tfidf_sim(records: List[Dict], query_text: str) -> List[float]:\n",
    "    docs = [norm_space((r[\"title\"] or \"\") + \" \" + (r[\"abstract\"] or \"\")) for r in records]\n",
    "    vec = TfidfVectorizer(ngram_range=(1,3), lowercase=True, max_features=120000)\n",
    "    X = vec.fit_transform(docs)\n",
    "    q = vec.transform([query_text])\n",
    "    Xa = X.toarray(); qa = q.toarray()\n",
    "    denom = (np.linalg.norm(Xa, axis=1) * (np.linalg.norm(qa) + 1e-12) + 1e-12)\n",
    "    sims = (Xa @ qa.T).ravel() / denom\n",
    "    return sims.tolist()\n",
    "\n",
    "def build_seed_query_text(pop_terms: List[str], int_terms: List[str]) -> str:\n",
    "    # For tf-idf, build a lexical seed string (not the user NLQ).\n",
    "    return \" \".join(pop_terms + int_terms)\n",
    "\n",
    "def record_score(rec: Dict, pop_terms: List[str], int_terms: List[str], outcome_tokens: List[str],\n",
    "                 tfidf_val: float, ymin: int, w: Dict[str,float], primary_pubtypes: set) -> Tuple[float, Dict[str,float]]:\n",
    "    t = norm_space((rec[\"title\"] or \"\") + \" \" + (rec[\"abstract\"] or \"\"))\n",
    "    pi = 1.0 if (has_any(t, pop_terms) and has_any(t, int_terms)) else 0.0\n",
    "    outc = 1.0 if has_any(t, outcome_tokens) else 0.0\n",
    "    design = 1.0 if set(rec.get(\"publication_types\", [])) & primary_pubtypes else 0.0\n",
    "    recn = year_recency(rec.get(\"year\"), ymin)\n",
    "    score = (w[\"pi\"]*pi + w[\"outcome\"]*outc + w[\"design\"]*design + w[\"recency\"]*recn + w[\"tfidf\"]*float(tfidf_val))\n",
    "    feats = {\"pi\":pi,\"outcome\":outc,\"design\":design,\"recency\":recn,\"tfidf\":float(tfidf_val)}\n",
    "    return float(score), feats\n",
    "\n",
    "def query_quality(sample_scores: List[float]) -> float:\n",
    "    if not sample_scores: return 0.0\n",
    "    # robust: median + 0.25*mean (caps very spiky distributions)\n",
    "    return float(np.median(sample_scores) + 0.25*np.mean(sample_scores))\n",
    "\n",
    "# =========================\n",
    "# MeSH MINING (evidence-driven)\n",
    "# =========================\n",
    "\n",
    "INTERVENTION_PRIOR = {\n",
    "    \"Cryosurgery\", \"Anesthesia, Epidural\", \"Nerve Block\", \"Analgesia\", \"Intercostal Nerves\",\n",
    "    \"Anesthesia, Conduction\", \"Pain Management\", \"Thoracic Surgery, Video-Assisted\"\n",
    "}\n",
    "POPULATION_PRIOR = {\n",
    "    \"Pectus Excavatum\", \"Thoracoscopy\", \"Thoracic Wall\", \"Pectus Carinatum\"\n",
    "}\n",
    "\n",
    "def mine_mesh(records: List[Dict], pop_terms: List[str], int_terms: List[str]) -> Dict:\n",
    "    # Count MeSH frequency and correlate with P/I lexical hits (for weak role classification)\n",
    "    freq = Counter(); majr = Counter()\n",
    "    assoc_pop = Counter(); assoc_int = Counter()\n",
    "    for r in records:\n",
    "        t = norm_space((r[\"title\"] or \"\") + \" \" + (r[\"abstract\"] or \"\"))\n",
    "        p_hit = has_any(t, pop_terms)\n",
    "        i_hit = has_any(t, int_terms)\n",
    "        for m in r.get(\"mesh\", []):\n",
    "            term = m[\"term\"]\n",
    "            freq[term]+=1\n",
    "            if m.get(\"majr\"): majr[term]+=1\n",
    "            if p_hit: assoc_pop[term]+=1\n",
    "            if i_hit: assoc_int[term]+=1\n",
    "    # role score: prior + association differential + majr bonus\n",
    "    candidates=[]\n",
    "    for term, n in freq.items():\n",
    "        ap = assoc_pop[term]; ai = assoc_int[term]\n",
    "        diff = (ai - ap) / max(1.0, n)  # positive → intervention-ish\n",
    "        prior_i = 1.0 if term in INTERVENTION_PRIOR else 0.0\n",
    "        prior_p = 1.0 if term in POPULATION_PRIOR else 0.0\n",
    "        role_raw = diff + 0.5*prior_i - 0.5*prior_p + 0.2*(majr[term]>0)\n",
    "        candidates.append({\n",
    "            \"term\": term,\n",
    "            \"freq\": int(n),\n",
    "            \"majr\": int(majr[term]),\n",
    "            \"assoc_pop\": int(ap),\n",
    "            \"assoc_int\": int(ai),\n",
    "            \"role_score\": float(role_raw),\n",
    "            \"role\": \"I\" if role_raw>0.2 else (\"P\" if role_raw<-0.2 else \"U\")\n",
    "        })\n",
    "    # rank\n",
    "    candidates.sort(key=lambda x: (-x[\"freq\"], -x[\"role_score\"], x[\"term\"]))\n",
    "    # select shortlists\n",
    "    top_p = [c for c in candidates if c[\"role\"]==\"P\"][:12]\n",
    "    top_i = [c for c in candidates if c[\"role\"]==\"I\"][:12]\n",
    "    return {\n",
    "        \"summary\": {\"unique\": len(candidates)},\n",
    "        \"top_population\": top_p,\n",
    "        \"top_intervention\": top_i,\n",
    "        \"all\": candidates\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# TOKEN REFINEMENT (from evidence)\n",
    "# =========================\n",
    "\n",
    "def refine_tokens(pop_terms: List[str], int_terms: List[str], mesh_mining: Dict) -> Tuple[List[str], List[str], Dict]:\n",
    "    # Add high-confidence MeSH-derived surface forms as lexical tokens (no MeSH qualifiers in final query)\n",
    "    p_add = []\n",
    "    for m in mesh_mining.get(\"top_population\", []):\n",
    "        t = m[\"term\"]\n",
    "        if t.lower() not in [x.lower() for x in pop_terms] and m[\"freq\"]>=2:\n",
    "            p_add.append(t)\n",
    "    i_add = []\n",
    "    for m in mesh_mining.get(\"top_intervention\", []):\n",
    "        t = m[\"term\"]\n",
    "        if t.lower() not in [x.lower() for x in int_terms] and m[\"freq\"]>=2:\n",
    "            i_add.append(t)\n",
    "\n",
    "    # Normalize: keep concise tokens (1–3 words)\n",
    "    def filt(tokens):\n",
    "        out=[]\n",
    "        for tok in tokens:\n",
    "            w = tok.split()\n",
    "            if 1 <= len(w) <= 4:\n",
    "                out.append(tok)\n",
    "        return out\n",
    "\n",
    "    p_new = pop_terms + filt(p_add)\n",
    "    i_new = int_terms + filt(i_add)\n",
    "\n",
    "    # Deduplicate (case-insensitive), preserve order\n",
    "    def dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            k=x.lower()\n",
    "            if k not in seen:\n",
    "                out.append(x); seen.add(k)\n",
    "        return out\n",
    "\n",
    "    p_final = dedup(p_new)\n",
    "    i_final = dedup(i_new)\n",
    "\n",
    "    meta = {\"added_population\": [t for t in p_final if t not in pop_terms],\n",
    "            \"added_intervention\": [t for t in i_final if t not in int_terms]}\n",
    "    return p_final, i_final, meta\n",
    "\n",
    "# =========================\n",
    "# PROBE / EVALUATE CANDIDATES\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class ProbeResult:\n",
    "    query: str\n",
    "    total_count: int\n",
    "    sampled_ids: int\n",
    "    rq: float\n",
    "    stats: Dict[str,float]\n",
    "    sample_rows: List[Dict]\n",
    "\n",
    "def probe_query(query: str, pop_terms: List[str], int_terms: List[str], outcome_tokens: List[str],\n",
    "                ymin: int, target_sample: int) -> ProbeResult:\n",
    "    # esearch\n",
    "    total, ids = esearch(query, mindate=ymin, retmax=target_sample)\n",
    "    if total == 0 or not ids:\n",
    "        return ProbeResult(query=query, total_count=total, sampled_ids=0, rq=0.0, stats={}, sample_rows=[])\n",
    "    # efetch over ids\n",
    "    xml = efetch_xml(ids, chunk=CONFIG[\"fetch_chunk\"])\n",
    "    recs = parse_pubmed_xml(xml)\n",
    "    # tf-idf sims vs seed text\n",
    "    seed_text = build_seed_query_text(pop_terms, int_terms)\n",
    "    tfidf_vals = tfidf_sim(recs, seed_text)\n",
    "    # score records\n",
    "    ws = {\"pi\":CONFIG[\"w_pi\"],\"outcome\":CONFIG[\"w_outcome\"],\"design\":CONFIG[\"w_design\"],\"recency\":CONFIG[\"w_recency\"],\"tfidf\":CONFIG[\"w_tfidf\"]}\n",
    "    sample_scores=[]; rows=[]\n",
    "    pi_hits=0; outcome_hits=0; design_hits=0\n",
    "    for rec, tv in zip(recs, tfidf_vals):\n",
    "        s, feats = record_score(rec, pop_terms, int_terms, outcome_tokens, tv, ymin, ws, CONFIG[\"primary_pubtypes\"])\n",
    "        sample_scores.append(s)\n",
    "        pi_hits += feats[\"pi\"]\n",
    "        outcome_hits += feats[\"outcome\"]\n",
    "        design_hits += feats[\"design\"]\n",
    "        rows.append({\n",
    "            \"pmid\": rec[\"pmid\"],\n",
    "            \"year\": rec[\"year\"] or \"\",\n",
    "            \"language\": rec[\"language\"] or \"\",\n",
    "            \"pubtypes\": \";\".join(rec.get(\"publication_types\", [])),\n",
    "            \"title\": norm_space(rec[\"title\"])[:180],\n",
    "            \"pi_hit\": int(feats[\"pi\"]),\n",
    "            \"outcome_hit\": int(feats[\"outcome\"]),\n",
    "            \"design_hit\": int(feats[\"design\"]),\n",
    "            \"recency\": round(feats[\"recency\"],3),\n",
    "            \"tfidf\": round(feats[\"tfidf\"],3),\n",
    "            \"score\": round(s,3)\n",
    "        })\n",
    "    rq = query_quality(sample_scores)\n",
    "    n = len(recs)\n",
    "    stats_out = {\n",
    "        \"n_sample\": n,\n",
    "        \"pi_rate\": round(pi_hits/max(1,n),3),\n",
    "        \"outcome_rate\": round(outcome_hits/max(1,n),3),\n",
    "        \"design_rate\": round(design_hits/max(1,n),3),\n",
    "        \"median_score\": round(float(np.median(sample_scores)) if sample_scores else 0.0,3),\n",
    "        \"mean_score\": round(float(np.mean(sample_scores)) if sample_scores else 0.0,3)\n",
    "    }\n",
    "    return ProbeResult(query=query, total_count=total, sampled_ids=len(ids), rq=float(rq), stats=stats_out, sample_rows=rows)\n",
    "\n",
    "def target_window(year_min: int) -> Tuple[int,int,int,int]:\n",
    "    # make caps year-aware (older windows → allow a bit larger)\n",
    "    years = max(1, 2025 - year_min)\n",
    "    scale = 1.0 + min(1.0, years/15.0)*0.4  # up to +40%\n",
    "    bmin = int(CONFIG[\"broad_target_min\"]*scale)\n",
    "    bmax = int(CONFIG[\"broad_target_max\"]*scale)\n",
    "    fmin = int(CONFIG[\"focused_target_min\"]*scale)\n",
    "    fmax = int(CONFIG[\"focused_target_max\"]*scale)\n",
    "    return bmin, bmax, fmin, fmax\n",
    "\n",
    "def in_range(count: int, lo: int, hi: int) -> bool:\n",
    "    return lo <= count <= hi\n",
    "\n",
    "# =========================\n",
    "# MAIN ORCHESTRATION\n",
    "# =========================\n",
    "\n",
    "def sniff_poc():\n",
    "    out_dir = CONFIG[\"out_dir\"]; ensure_out_dir(out_dir)\n",
    "    year_min = CONFIG[\"year_min\"]\n",
    "    pop_terms = CONFIG[\"population_terms_seed\"][:]\n",
    "    int_terms = CONFIG[\"intervention_terms_seed\"][:]\n",
    "    outcomes  = CONFIG[\"outcome_tokens\"][:]\n",
    "    rct_hedge = CONFIG[\"rct_hedge\"][:]\n",
    "    bmin,bmax,fmin,fmax = target_window(year_min)\n",
    "\n",
    "    # 1) Build B0 (lexical-only) and probe\n",
    "    broad_0 = build_broad(pop_terms, int_terms)\n",
    "    print(f\"{now()}  B0 query:\", broad_0)\n",
    "    probe_log=[]\n",
    "    pr0 = probe_query(broad_0, pop_terms, int_terms, outcomes, year_min, CONFIG[\"probe_ids\"])\n",
    "    probe_log.append({\"stage\":\"B0\",\"total\":pr0.total_count,\"rq\":round(pr0.rq,3),\"stats\":pr0.stats})\n",
    "    print(f\"{now()}  B0 total={pr0.total_count} rq={pr0.rq:.3f} stats={pr0.stats}\")\n",
    "\n",
    "    # Write initial candidates tsv\n",
    "    write_tsv(os.path.join(out_dir, \"sniff_candidates_b0.tsv\"),\n",
    "              pr0.sample_rows, [\"pmid\",\"year\",\"language\",\"pubtypes\",\"pi_hit\",\"outcome_hit\",\"design_hit\",\"recency\",\"tfidf\",\"score\",\"title\"])\n",
    "\n",
    "    # 2) MeSH mining from B0 sample\n",
    "    # fetch fuller for mining if sample small\n",
    "    ids_for_mesh = [r[\"pmid\"] for r in pr0.sample_rows][:CONFIG[\"probe_ids\"]]\n",
    "    mesh_records = parse_pubmed_xml(efetch_xml(ids_for_mesh, chunk=CONFIG[\"fetch_chunk\"])) if ids_for_mesh else []\n",
    "    mesh_info = mine_mesh(mesh_records, pop_terms, int_terms)\n",
    "    save_json(os.path.join(out_dir, \"mesh_mining.json\"), mesh_info)\n",
    "\n",
    "    # 3) Token refinement (deterministic, MeSH → lexical surface)\n",
    "    pop_terms2, int_terms2, meta_add = refine_tokens(pop_terms, int_terms, mesh_info)\n",
    "    save_json(os.path.join(out_dir, \"token_refinement.json\"), {\"before\":{\"P\":pop_terms,\"I\":int_terms},\"after\":{\"P\":pop_terms2,\"I\":int_terms2},\"added\":meta_add})\n",
    "    pop_terms = pop_terms2; int_terms = int_terms2\n",
    "\n",
    "    # 4) Candidate generation & evaluation loop (a few deterministic variants)\n",
    "    candidates = []\n",
    "    # base refined broad\n",
    "    candidates.append((\"broad_refined\", build_broad(pop_terms, int_terms)))\n",
    "    # intervention-tight (favor cryo forms)\n",
    "    tight_I = [t for t in int_terms if \"cryo\" in t.lower() or \"intercostal nerve cryoablation\" in t.lower() or t.lower()==\"inc\"]\n",
    "    if len(tight_I)>=1:\n",
    "        candidates.append((\"broad_I_tight\", build_broad(pop_terms, tight_I)))\n",
    "    # population-tight (drop MIRPE/Nuss if too noisy? keep core PE phrase only)\n",
    "    if \"pectus excavatum\" in [p.lower() for p in pop_terms]:\n",
    "        p_core = [\"pectus excavatum\"]\n",
    "        candidates.append((\"broad_P_core\", build_broad(p_core, int_terms)))\n",
    "\n",
    "    trials=[]\n",
    "    best_broad = None\n",
    "    best_broad_rq = -1.0\n",
    "\n",
    "    for name, q in candidates:\n",
    "        pr = probe_query(q, pop_terms, int_terms, outcomes, year_min, CONFIG[\"probe_ids\"])\n",
    "        probe_log.append({\"stage\":name,\"total\":pr.total_count,\"rq\":round(pr.rq,3),\"stats\":pr.stats})\n",
    "        trials.append({\"name\": name, \"query\": q, \"total\": pr.total_count, \"rq\": pr.rq, \"stats\": pr.stats})\n",
    "        print(f\"{now()}  {name} total={pr.total_count} rq={pr.rq:.3f} stats={pr.stats}\")\n",
    "        # choose best broad by rq within soft window; if none in-range, pick highest rq anyway\n",
    "        if pr.rq > best_broad_rq and (in_range(pr.total_count, bmin, bmax) or best_broad is None):\n",
    "            best_broad = (q, pr)\n",
    "            best_broad_rq = pr.rq\n",
    "\n",
    "    # fallback to B0 if everything worse\n",
    "    if best_broad is None or (best_broad[1].rq < pr0.rq and in_range(pr0.total_count, bmin, bmax)):\n",
    "        best_broad = (broad_0, pr0)\n",
    "\n",
    "    # 5) Build focused from chosen broad (lexical hedge) + probe\n",
    "    focused_q = build_focused_from(best_broad[0], rct_hedge)\n",
    "    pr_f = probe_query(focused_q, pop_terms, int_terms, outcomes, year_min, CONFIG[\"probe_ids\"])\n",
    "    probe_log.append({\"stage\":\"focused_lex\", \"total\":pr_f.total_count, \"rq\":round(pr_f.rq,3), \"stats\":pr_f.stats})\n",
    "    print(f\"{now()}  focused total={pr_f.total_count} rq={pr_f.rq:.3f} stats={pr_f.stats}\")\n",
    "\n",
    "    # 6) Decide accept / minor repair\n",
    "    retrieval_plan = {\"broad\": best_broad[0], \"focused\": focused_q}\n",
    "    plan_meta = {\n",
    "        \"broad_eval\": {\"total\": best_broad[1].total_count, \"rq\": best_broad[1].rq, \"stats\": best_broad[1].stats,\n",
    "                       \"window_ok\": in_range(best_broad[1].total_count, bmin, bmax), \"target_window\": [bmin,bmax]},\n",
    "        \"focused_eval\": {\"total\": pr_f.total_count, \"rq\": pr_f.rq, \"stats\": pr_f.stats,\n",
    "                         \"window_ok\": in_range(pr_f.total_count, fmin, fmax), \"target_window\": [fmin,fmax]},\n",
    "        \"probe_log\": probe_log\n",
    "    }\n",
    "\n",
    "    # 7) Write artifacts\n",
    "    save_json(os.path.join(out_dir, \"retrieval_plan.json\"), retrieval_plan)\n",
    "    save_json(os.path.join(out_dir, \"retrieval_plan_eval.json\"), plan_meta)\n",
    "    write_tsv(os.path.join(out_dir, \"sniff_candidates_best_broad.tsv\"),\n",
    "              best_broad[1].sample_rows, [\"pmid\",\"year\",\"language\",\"pubtypes\",\"pi_hit\",\"outcome_hit\",\"design_hit\",\"recency\",\"tfidf\",\"score\",\"title\"])\n",
    "    write_tsv(os.path.join(out_dir, \"sniff_candidates_focused.tsv\"),\n",
    "              pr_f.sample_rows, [\"pmid\",\"year\",\"language\",\"pubtypes\",\"pi_hit\",\"outcome_hit\",\"design_hit\",\"recency\",\"tfidf\",\"score\",\"title\"])\n",
    "\n",
    "    # 8) Console summary\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(\"Chosen BROAD:\", retrieval_plan[\"broad\"])\n",
    "    print(\"  total:\", plan_meta[\"broad_eval\"][\"total\"], \"rq:\", round(plan_meta[\"broad_eval\"][\"rq\"],3), \"window_ok:\", plan_meta[\"broad_eval\"][\"window_ok\"])\n",
    "    print(\"Chosen FOCUSED:\", retrieval_plan[\"focused\"])\n",
    "    print(\"  total:\", plan_meta[\"focused_eval\"][\"total\"], \"rq:\", round(plan_meta[\"focused_eval\"][\"rq\"],3), \"window_ok:\", plan_meta[\"focused_eval\"][\"window_ok\"])\n",
    "    print(f\"Artifacts written under: {out_dir}\")\n",
    "\n",
    "# Uncomment to run in notebook immediately:\n",
    "sniff_poc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb92b22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:15:55  B0 query: (adults[tiab] OR \"pectus excavatum\"[tiab] OR \"Nuss surgery\"[tiab] OR MIRPE[tiab] OR \"minimally invasive repair\"[tiab]) AND (\"intercostal nerve cryoablation (INC)\"[tiab] OR cryoanalgesia[tiab] OR \"intraoperative analgesia\"[tiab])\n",
      "02:15:59  B0 total=53 rq=4.325 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.6, 'design_rate': 0.2, 'median_score': 3.0, 'mean_score': 2.7}\n",
      "02:18:42  broad_refined total=1037 rq=2.7 stats={'n_sample': 5, 'pi_rate': 0.6, 'outcome_rate': 0.2, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 1.4}\n",
      "02:18:48  broad_I_tight total=3309 rq=4.3 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.6, 'design_rate': 0.0, 'median_score': 3.0, 'mean_score': 2.6}\n",
      "02:18:50  broad_P_core total=96 rq=2.9 stats={'n_sample': 5, 'pi_rate': 0.8, 'outcome_rate': 0.2, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 1.8}\n",
      "02:18:54  focused total=10 rq=3.775 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.4, 'design_rate': 0.6, 'median_score': 2.5, 'mean_score': 2.7}\n",
      "\n",
      "=== SUMMARY ===\n",
      "Chosen BROAD: (adults[tiab] OR \"pectus excavatum\"[tiab] OR \"Nuss surgery\"[tiab] OR MIRPE[tiab] OR \"minimally invasive repair\"[tiab]) AND (\"intercostal nerve cryoablation (INC)\"[tiab] OR cryoanalgesia[tiab] OR \"intraoperative analgesia\"[tiab])\n",
      "  total: 53 rq: 4.325 window_ok: True\n",
      "Chosen FOCUSED: ((adults[tiab] OR \"pectus excavatum\"[tiab] OR \"Nuss surgery\"[tiab] OR MIRPE[tiab] OR \"minimally invasive repair\"[tiab]) AND (\"intercostal nerve cryoablation (INC)\"[tiab] OR cryoanalgesia[tiab] OR \"intraoperative analgesia\"[tiab])) AND (randomized[tiab] OR randomised[tiab] OR randomization[tiab] OR \"random allocation\"[tiab])\n",
      "  total: 10 rq: 3.775 window_ok: True\n",
      "Artifacts written under: sniff_poc_out\n"
     ]
    }
   ],
   "source": [
    "# SNIPPET: end-to-end \"sniff\" PoC with semantic LLM oversight (single cell)\n",
    "# - Deterministic retrieval + query assembly\n",
    "# - Qwen 4B-thinking for: term extraction, MeSH role-tagging, reprompts\n",
    "# - Gemma-mini for: cheap title/abstract sanity screen of samples\n",
    "# - Writes artifacts into sniff_poc_out/\n",
    "#\n",
    "# CONFIGURE before running:\n",
    "#   - LM Studio at http://127.0.0.1:1234\n",
    "#   - QWEN_MODEL and GEMMA_MODEL\n",
    "#   - ENTREZ_EMAIL (and optionally ENTREZ_API_KEY via env)\n",
    "#\n",
    "# Usage:\n",
    "#   1) Paste your natural-language question into USER_NLQ below.\n",
    "#   2) Run the cell. Inspect printed summary + files in sniff_poc_out/.\n",
    "\n",
    "import os, json, time, re, textwrap, random, pathlib\n",
    "from collections import Counter, defaultdict\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "LMSTUDIO_BASE = os.getenv(\"LMSTUDIO_BASE\", \"http://127.0.0.1:1234\")\n",
    "QWEN_MODEL    = os.getenv(\"QWEN_MODEL\", \"qwen/qwen3-4b\")\n",
    "GEMMA_MODEL   = os.getenv(\"GEMMA_MODEL\", \"gemma-3n-e2b-it\")\n",
    "\n",
    "ENTREZ_EMAIL   = os.getenv(\"ENTREZ_EMAIL\", \"you@example.com\")\n",
    "ENTREZ_API_KEY = os.getenv(\"ENTREZ_API_KEY\", \"\")\n",
    "HTTP_TIMEOUT   = int(os.getenv(\"HTTP_TIMEOUT\", \"300\"))\n",
    "\n",
    "OUT_DIR = pathlib.Path(\"sniff_poc_out\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Windows / caps (soft)\n",
    "YEAR_MIN_DEFAULT = 2015\n",
    "BROAD_TARGET = (50, 5000)       # ok 10–10k\n",
    "FOCUSED_TARGET = (3, 500)       # ok 1–2000\n",
    "BROAD_OK = (10, 10000)\n",
    "FOCUSED_OK = (1, 2000)\n",
    "\n",
    "SAMPLE_N = 5                   # sample size for T/A sanity screen per candidate\n",
    "RCT_HEDGE_LEX = '(randomized[tiab] OR randomised[tiab] OR randomization[tiab] OR \"random allocation\"[tiab])'\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: LM chat + JSON fences (+ robust JSON repair)\n",
    "# ----------------------------\n",
    "def lm_chat(model: str, system: str, user: str, temperature=0.0, max_tokens=8000, response_format=None, stop=None):\n",
    "    url = f\"{LMSTUDIO_BASE.rstrip('/')}/v1/chat/completions\"\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "        \"temperature\": float(temperature),\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"stream\": False\n",
    "    }\n",
    "    if response_format is not None:\n",
    "        body[\"response_format\"] = response_format\n",
    "    if stop is not None:\n",
    "        body[\"stop\"] = stop\n",
    "    r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "_BEGIN = re.compile(r\"BEGIN_JSON\\s*\", re.I)\n",
    "_END   = re.compile(r\"\\s*END_JSON\", re.I)\n",
    "FENCE  = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)```\", re.I)\n",
    "\n",
    "def _sanitize_json_str(s: str) -> str:\n",
    "    # normalize curly quotes; remove trailing commas before } or ]\n",
    "    s = s.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n",
    "    s = re.sub(r\",\\s*(\\}|\\])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_json_block_or_fence(txt: str) -> str:\n",
    "    # prefer BEGIN_JSON...END_JSON\n",
    "    blocks = []\n",
    "    pos=0\n",
    "    while True:\n",
    "        m1 = _BEGIN.search(txt, pos)\n",
    "        if not m1: break\n",
    "        m2 = _END.search(txt, m1.end())\n",
    "        if not m2: break\n",
    "        blocks.append(txt[m1.end():m2.start()])\n",
    "        pos = m2.end()\n",
    "    if blocks:\n",
    "        return _sanitize_json_str(blocks[-1])\n",
    "\n",
    "    # then fenced code\n",
    "    fences = FENCE.findall(txt)\n",
    "    if fences:\n",
    "        return _sanitize_json_str(fences[-1])\n",
    "\n",
    "    # last {...} by brace scan (balanced)\n",
    "    s = txt\n",
    "    last_obj = None\n",
    "    stack = 0; start = None\n",
    "    for i,ch in enumerate(s):\n",
    "        if ch == '{':\n",
    "            if stack == 0: start = i\n",
    "            stack += 1\n",
    "        elif ch == '}':\n",
    "            if stack > 0:\n",
    "                stack -= 1\n",
    "                if stack == 0 and start is not None:\n",
    "                    last_obj = s[start:i+1]\n",
    "    if last_obj:\n",
    "        return _sanitize_json_str(last_obj)\n",
    "    raise ValueError(\"No JSON-like content found\")\n",
    "\n",
    "# Minimal JSON-repair via LLM against a template\n",
    "REPAIR_SYSTEM = \"You repair malformed JSON to exactly match the given template keys. Return ONLY one JSON object between BEGIN_JSON/END_JSON.\"\n",
    "def repair_user(template_json: str, bad_output: str) -> str:\n",
    "    return f\"\"\"TEMPLATE_JSON:\n",
    "{template_json}\n",
    "\n",
    "BAD_OUTPUT:\n",
    "{bad_output}\n",
    "\n",
    "TASK: Output valid JSON matching TEMPLATE_JSON keys (fill missing with empty arrays/strings). No prose.\n",
    "\n",
    "BEGIN_JSON\n",
    "{{}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "TERMS_TEMPLATE   = {\"population\":[],\"intervention\":[],\"comparators\":[],\"outcomes\":[],\"must_have\":[],\"avoid\":[]}\n",
    "MESH_TAG_TEMPLATE= {\"labels\":[{\"mesh\":\"\",\"role\":\"G\",\"keep\":False,\"why\":\"\"}]}\n",
    "PASSA_TEMPLATE   = {\"pmid\":\"\",\"decision\":\"\",\"reason\":\"\",\"confidence\":0.0,\"population_quote\":\"\",\"intervention_quote\":\"\"}\n",
    "\n",
    "STRICT_JSON_RULES = (\n",
    "  \"Return ONLY one JSON object. No analysis, no preface, no notes. \"\n",
    "  \"Wrap it EXACTLY with:\\nBEGIN_JSON\\n{...}\\nEND_JSON\"\n",
    ")\n",
    "\n",
    "def ask_json_strict(model: str, system: str, user: str, template: dict, max_tokens=8000):\n",
    "    # 1) Try a strict call that forbids prose and stops at END_JSON\n",
    "    user_strict = f\"{user}\\n\\n{STRICT_JSON_RULES}\"\n",
    "    raw = lm_chat(model, system, user_strict, temperature=0.0, max_tokens=max_tokens, stop=[\"END_JSON\"])\n",
    "    try:\n",
    "        return json.loads(extract_json_block_or_fence(raw))\n",
    "    except Exception:\n",
    "        # 2) Repair pass: same strictness + template\n",
    "        repaired = lm_chat(\n",
    "            model,\n",
    "            REPAIR_SYSTEM,\n",
    "            repair_user(json.dumps(template, ensure_ascii=False, indent=2), raw) + \"\\n\\n\" + STRICT_JSON_RULES,\n",
    "            temperature=0.0,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=[\"END_JSON\"]\n",
    "        )\n",
    "        return json.loads(extract_json_block_or_fence(repaired))\n",
    "\n",
    "\n",
    "def ask_json(model: str, system: str, user: str, template: dict, max_tokens=8000):\n",
    "    raw = lm_chat(model, system, user, temperature=0.0, max_tokens=max_tokens)\n",
    "    try:\n",
    "        return json.loads(extract_json_block_or_fence(raw))\n",
    "    except Exception:\n",
    "        # attempt repair\n",
    "        repaired = lm_chat(\n",
    "            model,\n",
    "            REPAIR_SYSTEM,\n",
    "            repair_user(json.dumps(template, ensure_ascii=False, indent=2), raw),\n",
    "            temperature=0.0,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return json.loads(extract_json_block_or_fence(repaired))\n",
    "    \n",
    "ask_json = ask_json_strict\n",
    "\n",
    "# ----------------------------\n",
    "# PubMed E-utilities (esearch/efetch)\n",
    "# ----------------------------\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "HEADERS = {\"User-Agent\": \"sniff-poc/0.1 (+local)\", \"Accept\": \"application/json\"}\n",
    "\n",
    "def esearch_count_and_ids(term: str, mindate: int|None):\n",
    "    p = {\n",
    "        \"db\":\"pubmed\",\"retmode\":\"json\",\"term\":term,\"retmax\":5000,\n",
    "        \"email\":ENTREZ_EMAIL,\"usehistory\":\"y\"\n",
    "    }\n",
    "    if ENTREZ_API_KEY: p[\"api_key\"]=ENTREZ_API_KEY\n",
    "    if mindate: p[\"mindate\"]=str(mindate)\n",
    "    r = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params=p, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    js = r.json().get(\"esearchresult\", {})\n",
    "    count = int(js.get(\"count\",\"0\"))\n",
    "    webenv = js.get(\"webenv\")\n",
    "    qk = js.get(\"querykey\")\n",
    "    if not count or not webenv or not qk:\n",
    "        return 0, []\n",
    "    # fetch up to 5k IDs (enough for sniff)\n",
    "    r2 = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params={\n",
    "        \"db\":\"pubmed\",\"retmode\":\"json\",\"retmax\":5000,\"retstart\":0,\"email\":ENTREZ_EMAIL,\n",
    "        \"WebEnv\":webenv,\"query_key\":qk, **({\"api_key\":ENTREZ_API_KEY} if ENTREZ_API_KEY else {})\n",
    "    }, timeout=HTTP_TIMEOUT)\n",
    "    r2.raise_for_status()\n",
    "    ids = r2.json().get(\"esearchresult\",{}).get(\"idlist\",[])\n",
    "    return count, [str(x) for x in ids]\n",
    "\n",
    "def efetch_xml(pmids):\n",
    "    if not pmids: return \"\"\n",
    "    params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"id\":\",\".join(pmids),\"email\":ENTREZ_EMAIL}\n",
    "    if ENTREZ_API_KEY: params[\"api_key\"]=ENTREZ_API_KEY\n",
    "    r = requests.get(f\"{EUTILS}/efetch.fcgi\", headers={\"User-Agent\": \"sniff-poc/0.1\"}, params=params, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_pubmed_xml(xml_text: str):\n",
    "    out = []\n",
    "    if not xml_text.strip(): return out\n",
    "    root = ET.fromstring(xml_text)\n",
    "    def _join(node): \n",
    "        if node is None: return \"\"\n",
    "        try: return \"\".join(node.itertext())\n",
    "        except Exception: return node.text or \"\"\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = art.findtext(\".//PMID\") or \"\"\n",
    "        title = _join(art.find(\".//ArticleTitle\")).strip()\n",
    "        abs_nodes = art.findall(\".//Abstract/AbstractText\")\n",
    "        abstract = \" \".join(_join(n).strip() for n in abs_nodes) if abs_nodes else \"\"\n",
    "        year = None\n",
    "        for path in (\".//ArticleDate/Year\",\".//PubDate/Year\",\".//DateCreated/Year\",\".//PubDate/MedlineDate\"):\n",
    "            s = art.findtext(path)\n",
    "            if s:\n",
    "                m = re.search(r\"\\d{4}\", s)\n",
    "                if m: year = int(m.group(0)); break\n",
    "        lang = art.findtext(\".//Language\") or None\n",
    "        pubtypes = [pt.text for pt in art.findall(\".//PublicationTypeList/PublicationType\") if pt.text]\n",
    "        mesh = [mh.findtext(\"./DescriptorName\") for mh in art.findall(\".//MeshHeadingList/MeshHeading\") if mh.findtext(\"./DescriptorName\")]\n",
    "        out.append({\n",
    "            \"pmid\": pmid, \"title\": title, \"abstract\": abstract, \"year\": year, \"language\": lang,\n",
    "            \"publication_types\": pubtypes, \"mesh\": mesh\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# LLM prompts (semantic steps)\n",
    "# ----------------------------\n",
    "TERMS_SYSTEM = \"You extract controlled, compact term lists for biomedical retrieval. Return strict JSON only.\"\n",
    "def terms_user(nlq: str):\n",
    "    return f\"\"\"From the natural-language question below, produce compact term lists.\n",
    "\n",
    "NATURAL_LANGUAGE_QUESTION:\n",
    "<<<\n",
    "{nlq}\n",
    ">>>\n",
    "\n",
    "Rules:\n",
    "- Return JSON with arrays of P/I/C/O strings: {{ \"population\":[], \"intervention\":[], \"comparators\":[], \"outcomes\":[] }}\n",
    "- Strings must be concise phrases (no boolean, no field tags, no quotes/brackets).\n",
    "- Include common synonyms and acronyms (e.g., Nuss, MIRPE, cryoanalgesia).\n",
    "- Add 2–5 must_have tokens in \"must_have\" that anchor topicality (e.g., MIRPE, Nuss, cryoablation).\n",
    "- Add 2–5 avoid tokens in \"avoid\" if obvious confounders (e.g., pediatric oncology if off-topic).\n",
    "- Keep each list ≤ 12 items.\n",
    "\n",
    "Return ONLY:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{...}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "MESH_TAG_SYSTEM = \"You classify MeSH descriptors into roles relative to a PICOS.\"\n",
    "def mesh_tag_user(p_terms, i_terms, descriptors):\n",
    "    return f\"\"\"Classify each MeSH descriptor as one of: P (population/procedure context), I (intervention/analgesia), O (outcome), C (comparator/technique), G (generic context), X (irrelevant).\n",
    "Also provide a 'keep' boolean (true if useful for building search), and a 1-line rationale.\n",
    "\n",
    "P_TERMS = {p_terms}\n",
    "I_TERMS = {i_terms}\n",
    "\n",
    "DESCRIPTORS = {descriptors}\n",
    "\n",
    "Return ONLY:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{ \"labels\": [{{\"mesh\":\"...\", \"role\":\"P|I|O|C|G|X\", \"keep\": true|false, \"why\": \"...\"}}] }}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "PASSA_SYS = \"You are a strict PRISMA title/abstract screener for effects triage. Return JSON only.\"\n",
    "def passa_user(proto_p, proto_i, proto_outcomes, record):\n",
    "    return f\"\"\"Protocol (simplified):\n",
    "Population: {proto_p}\n",
    "Intervention: {proto_i}\n",
    "Outcomes (signals): {proto_outcomes}\n",
    "Include primary/comparative human studies; exclude admin/guidelines.\n",
    "\n",
    "Record:\n",
    "PMID: {record['pmid']}\n",
    "Title: {record['title']}\n",
    "Abstract: {record['abstract']}\n",
    "PubTypes: {record['publication_types']}\n",
    "Year: {record['year']}\n",
    "Lang: {record['language']}\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{record['pmid']}\",\n",
    "  \"decision\":\"include|borderline|exclude\",\n",
    "  \"reason\":\"population_mismatch|intervention_mismatch|design_ineligible|off_topic|language|year|insufficient_info\",\n",
    "  \"confidence\": 0.0,\n",
    "  \"population_quote\":\"\", \"intervention_quote\":\"\"\n",
    "}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "REPROMPT_SYS = \"You write crisp, actionable reprompts (≤2 sentences) to fix information gaps.\"\n",
    "def reprompt_user(summary_problem: str):\n",
    "    return f\"\"\"Context of failure:\n",
    "{summary_problem}\n",
    "\n",
    "Write ≤2 sentences telling the user exactly what to clarify or relax (no noise).\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"reprompt\":\"...\"}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Deterministic: build queries from term lists\n",
    "# ----------------------------\n",
    "def or_block(terms, field=\"tiab\"):\n",
    "    toks=[]\n",
    "    for t in terms:\n",
    "        t=t.strip()\n",
    "        if not t: continue\n",
    "        if \" \" in t or \"-\" in t:\n",
    "            toks.append(f\"\\\"{t}\\\"[{field}]\")\n",
    "        else:\n",
    "            toks.append(f\"{t}[{field}]\")\n",
    "    if not toks: return \"\"\n",
    "    return \"(\" + \" OR \".join(toks) + \")\"\n",
    "\n",
    "def build_broad(p_syn, i_syn, extra=None, field=\"tiab\"):\n",
    "    P = or_block(p_syn, field)\n",
    "    I = or_block(i_syn, field)\n",
    "    X = (\" AND \" + or_block(extra, field)) if extra else \"\"\n",
    "    if not P or not I:\n",
    "        return None\n",
    "    return f\"{P} AND {I}{X}\"\n",
    "\n",
    "def build_focused(broad_core):\n",
    "    return f\"({broad_core}) AND {RCT_HEDGE_LEX}\"\n",
    "\n",
    "# ----------------------------\n",
    "# Lexical stats + cheap “quality” score\n",
    "# ----------------------------\n",
    "PRIMARY_HINTS = {\"Randomized Controlled Trial\",\"Clinical Trial\",\"Controlled Clinical Trial\",\n",
    "                 \"Prospective Studies\",\"Cohort Studies\",\"Case-Control Studies\",\"Comparative Study\"}\n",
    "\n",
    "def lexical_stats(records, p_terms, i_terms, outcomes):\n",
    "    def hits(text, terms): \n",
    "        tl=(text or \"\").lower()\n",
    "        return sum(1 for t in terms if t and t.lower() in tl)\n",
    "    n = min(SAMPLE_N, len(records))\n",
    "    sample = records[:n]\n",
    "    pi_rate=0; out_rate=0; design_rate=0; scores=[]\n",
    "    for r in sample:\n",
    "        t = (r['title'] or \"\") + \"\\n\" + (r['abstract'] or \"\")\n",
    "        pi = (hits(t, p_terms)+hits(t, i_terms))>0\n",
    "        po = hits(t, outcomes)>0\n",
    "        de = len(set(r['publication_types']) & PRIMARY_HINTS)>0\n",
    "        pi_rate += 1 if pi else 0\n",
    "        out_rate+= 1 if po else 0\n",
    "        design_rate+= 1 if de else 0\n",
    "        # quick score: weighted signals\n",
    "        s = (2.0*(1 if pi else 0) + 1.0*(1 if po else 0) + 0.5*(1 if de else 0))\n",
    "        scores.append(s)\n",
    "    if n==0: \n",
    "        return {\"n_sample\":0,\"pi_rate\":0,\"outcome_rate\":0,\"design_rate\":0,\"median_score\":0,\"mean_score\":0}\n",
    "    scores.sort()\n",
    "    med = scores[n//2]\n",
    "    mean = sum(scores)/n\n",
    "    return {\"n_sample\":n,\"pi_rate\":round(pi_rate/n,3),\"outcome_rate\":round(out_rate/n,3),\n",
    "            \"design_rate\":round(design_rate/n,3),\"median_score\":round(med,3),\"mean_score\":round(mean,3)}\n",
    "\n",
    "def rq_quality(stats):\n",
    "    # emphasize central tendency and P/I presence\n",
    "    return round( stats[\"median_score\"] + 0.25*stats[\"mean_score\"] + 0.5*stats[\"pi_rate\"] + 0.25*stats[\"outcome_rate\"], 3)\n",
    "\n",
    "# ----------------------------\n",
    "# Main SNiff runner\n",
    "# ----------------------------\n",
    "def sniff_nlq(USER_NLQ: str, year_min: int = YEAR_MIN_DEFAULT):\n",
    "    # 1) Qwen: extract seed term lists\n",
    "    terms_js = ask_json(QWEN_MODEL, TERMS_SYSTEM, terms_user(USER_NLQ), TERMS_TEMPLATE, max_tokens=8000)\n",
    "    P0 = terms_js.get(\"population\", []) or []\n",
    "    I0 = terms_js.get(\"intervention\", []) or []\n",
    "    O0 = terms_js.get(\"outcomes\", []) or []\n",
    "    MUST = terms_js.get(\"must_have\", []) or []\n",
    "    AVOID = terms_js.get(\"avoid\", []) or []\n",
    "    # Persist\n",
    "    (OUT_DIR/\"seed_terms.json\").write_text(json.dumps(terms_js, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # 2) First broad (pure TIAB, P ∧ I)\n",
    "    candidates = []\n",
    "    def try_query(name, q):\n",
    "        cnt, ids = esearch_count_and_ids(q, year_min)\n",
    "        # sample up to SAMPLE_N for eval\n",
    "        xml = efetch_xml(ids[:SAMPLE_N])\n",
    "        recs = parse_pubmed_xml(xml)\n",
    "        stats = lexical_stats(recs, P0, I0, O0)\n",
    "        rq = rq_quality(stats)\n",
    "        candidates.append({\"name\":name,\"query\":q,\"total\":cnt,\"stats\":stats,\"rq\":rq,\"ids\":ids})\n",
    "        print(f\"{time.strftime('%H:%M:%S')}  {name} total={cnt} rq={rq} stats={stats}\")\n",
    "        return cnt, recs\n",
    "\n",
    "    q_b0 = build_broad(P0, I0)\n",
    "    if not q_b0:\n",
    "        rp = ask_json(lm_chat(QWEN_MODEL, REPROMPT_SYS, reprompt_user(\"Failed to construct core P and I term groups from your question.\")))\n",
    "        raise SystemExit(\"REPROMPT: \" + rp.get(\"reprompt\",\"need clarification\"))\n",
    "    print(f\"{time.strftime('%H:%M:%S')}  B0 query: {q_b0}\")\n",
    "    _, recs_b0 = try_query(\"B0\", q_b0)\n",
    "\n",
    "    # 3) Mine MeSH from real hits\n",
    "    mesh_all = Counter()\n",
    "    for r in recs_b0:\n",
    "        for m in r.get(\"mesh\", []) or []:\n",
    "            mesh_all[m] += 1\n",
    "    top_mesh = [m for m,_ in mesh_all.most_common(40)]\n",
    "    (OUT_DIR/\"mesh_raw.json\").write_text(json.dumps({\"top_mesh\":top_mesh, \"counts\":mesh_all.most_common(100)}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # 4) Qwen: role-tag mined MeSH relative to P/I\n",
    "    mesh_tag_js = ask_json(QWEN_MODEL, MESH_TAG_SYSTEM, mesh_tag_user(P0, I0, top_mesh), MESH_TAG_TEMPLATE, max_tokens=8000)\n",
    "    (OUT_DIR/\"mesh_tagged.json\").write_text(json.dumps(mesh_tag_js, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    keep_mesh = [x[\"mesh\"] for x in mesh_tag_js.get(\"labels\", []) if x.get(\"keep\") and x.get(\"role\") in (\"P\",\"I\",\"O\")]\n",
    "    keepP = [x[\"mesh\"] for x in mesh_tag_js.get(\"labels\", []) if x.get(\"keep\") and x.get(\"role\")==\"P\"]\n",
    "    keepI = [x[\"mesh\"] for x in mesh_tag_js.get(\"labels\", []) if x.get(\"keep\") and x.get(\"role\")==\"I\"]\n",
    "\n",
    "    # 5) Generate refined broads (TIAB only), mixing mined words (as surface tokens) — no fielded MeSH\n",
    "    #    (We use MeSH words as plain tokens, respecting your “broad shouldn’t rely on field qualifiers”.)\n",
    "    def expand_terms(base, extra):\n",
    "        seen=set([b.lower() for b in base])\n",
    "        out=list(base)\n",
    "        for e in extra:\n",
    "            w=e.strip()\n",
    "            if not w: continue\n",
    "            if w.lower() not in seen:\n",
    "                out.append(w)\n",
    "                seen.add(w.lower())\n",
    "        return out[:12]\n",
    "\n",
    "    P_core = expand_terms(P0, keepP[:6])\n",
    "    I_core = expand_terms(I0, keepI[:8])\n",
    "    # Several candidate mixes\n",
    "    variants = [\n",
    "        (\"broad_refined\", build_broad(P_core, I_core, extra=MUST)),\n",
    "        (\"broad_I_tight\", build_broad(P0, I_core)),\n",
    "        (\"broad_P_core\",  build_broad(P_core, I0)),\n",
    "    ]\n",
    "    for name, q in variants:\n",
    "        if q:\n",
    "            try_query(name, q)\n",
    "\n",
    "    # choose broad: best rq within BROAD_TARGET else within BROAD_OK\n",
    "    def choose_broad():\n",
    "        cands = [c for c in candidates if c[\"name\"].startswith(\"broad\") or c[\"name\"]==\"B0\"]\n",
    "        # prefer in-target window\n",
    "        in_target = [c for c in cands if BROAD_TARGET[0] <= c[\"total\"] <= BROAD_TARGET[1]]\n",
    "        pool = in_target if in_target else [c for c in cands if BROAD_OK[0] <= c[\"total\"] <= BROAD_OK[1]]\n",
    "        if not pool:\n",
    "            return max(cands, key=lambda x: x[\"rq\"]) if cands else None\n",
    "        return max(pool, key=lambda x: x[\"rq\"])\n",
    "    chosen_broad = choose_broad()\n",
    "    if not chosen_broad:\n",
    "        rp = ask_json(lm_chat(QWEN_MODEL, REPROMPT_SYS, reprompt_user(\"No broad query produced viable hit counts or relevance.\")))\n",
    "        (OUT_DIR/\"reprompt.json\").write_text(json.dumps(rp, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "        print(\"\\nREPROMPT:\", rp.get(\"reprompt\",\"need clarification\"))\n",
    "        return\n",
    "\n",
    "    # 6) Build focused: lexical RCT hedge layered on chosen broad\n",
    "    q_focused = build_focused(chosen_broad[\"query\"])\n",
    "    cntF, idsF = esearch_count_and_ids(q_focused, year_min)\n",
    "    xmlF = efetch_xml(idsF[:SAMPLE_N])\n",
    "    recsF = parse_pubmed_xml(xmlF)\n",
    "    statsF = lexical_stats(recsF, P0, I0, O0); rqF = rq_quality(statsF)\n",
    "    candidates.append({\"name\":\"focused\",\"query\":q_focused,\"total\":cntF,\"stats\":statsF,\"rq\":rqF,\"ids\":idsF})\n",
    "    print(f\"{time.strftime('%H:%M:%S')}  focused total={cntF} rq={rqF} stats={statsF}\")\n",
    "\n",
    "    # 7) Gemma: cheap T/A sanity screen on samples (both chosen sets)\n",
    "    def sanity_screen(records):\n",
    "        if not records: return {\"include\":0,\"borderline\":0,\"exclude\":0,\"n\":0}\n",
    "        inc=bor=exc=0\n",
    "        for r in records[:min(SAMPLE_N,len(records))]:\n",
    "            js = ask_json(GEMMA_MODEL, PASSA_SYS, passa_user(P0, I0, O0, r), PASSA_TEMPLATE, max_tokens=8000)\n",
    "            d = (js.get(\"decision\",\"\") or \"\").lower()\n",
    "            if d==\"include\": inc+=1\n",
    "            elif d==\"borderline\": bor+=1\n",
    "            else: exc+=1\n",
    "        n=inc+bor+exc\n",
    "        return {\"include\":inc,\"borderline\":bor,\"exclude\":exc,\"n\":n}\n",
    "\n",
    "    chosen_broad_sample = parse_pubmed_xml(efetch_xml(chosen_broad[\"ids\"][:SAMPLE_N]))\n",
    "    sanity_broad = sanity_screen(chosen_broad_sample)\n",
    "    sanity_focused = sanity_screen(recsF)\n",
    "\n",
    "    # 8) Persist artifacts\n",
    "    artifacts = {\n",
    "        \"nlq\": USER_NLQ,\n",
    "        \"year_min\": year_min,\n",
    "        \"seed_terms\": terms_js,\n",
    "        \"mesh_top\": top_mesh,\n",
    "        \"mesh_tagged\": mesh_tag_js,\n",
    "        \"candidates\": [\n",
    "            {k:v for k,v in c.items() if k not in (\"ids\",)} for c in candidates\n",
    "        ],\n",
    "        \"chosen\": {\n",
    "            \"broad\": {\"query\": chosen_broad[\"query\"], \"total\": chosen_broad[\"total\"], \"rq\": chosen_broad[\"rq\"], \"stats\": chosen_broad[\"stats\"], \"sanity\": sanity_broad},\n",
    "            \"focused\": {\"query\": q_focused, \"total\": cntF, \"rq\": rqF, \"stats\": statsF, \"sanity\": sanity_focused},\n",
    "        }\n",
    "    }\n",
    "    (OUT_DIR/\"sniff_artifacts.json\").write_text(json.dumps(artifacts, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    (OUT_DIR/\"broad.txt\").write_text(chosen_broad[\"query\"], encoding=\"utf-8\")\n",
    "    (OUT_DIR/\"focused.txt\").write_text(q_focused, encoding=\"utf-8\")\n",
    "\n",
    "    # 9) Print summary\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(\"Chosen BROAD:\", chosen_broad[\"query\"])\n",
    "    print(f\"  total: {chosen_broad['total']} rq: {chosen_broad['rq']} window_ok: {BROAD_OK[0] <= chosen_broad['total'] <= BROAD_OK[1]}\")\n",
    "    print(\"Chosen FOCUSED:\", q_focused)\n",
    "    print(f\"  total: {cntF} rq: {rqF} window_ok: {FOCUSED_OK[0] <= cntF <= FOCUSED_OK[1]}\")\n",
    "    print(\"Artifacts written under:\", OUT_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# RUN: put your NLQ here\n",
    "# ----------------------------\n",
    "USER_NLQ = \"\"\"Population = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE (the intervention of interest is INC, not the surgery). Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia. Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days. Study designs = RCTs preferred; if RCTs absent, include comparative cohort/case-control. Year_min = 2015. Languages = English, Portuguese, Spanish.\"\"\"\n",
    "sniff_nlq(USER_NLQ, year_min=YEAR_MIN_DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f4bd519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:45:23  B0_seed hits=20809 rq=3.1 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.2, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 2.2}\n",
      "02:48:25  B1_broad hits=87 rq=3.2 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.4, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 2.4}\n",
      "02:48:42  B2_I_tight hits=2232 rq=3.0 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.0, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 2.0}\n",
      "02:48:44  B3_P_core hits=960 rq=3.1 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.2, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 2.2}\n",
      "\n",
      "==================== SNIFF REPORT ====================\n",
      "NLQ:\n",
      "   Population = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE (th...\n",
      "\n",
      "SEED TERMS (Qwen):\n",
      "  P: adults, minimally invasive repair, pectus excavatum, Nuss, MIRPE\n",
      "  I: intercostal nerve cryoablation, cryoanalgesia, INC, intraoperative, postoperative\n",
      "  C: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia\n",
      "  O: postoperative opioid consumption, opioid use, pain scores, 0-7 days post-op, acute pain\n",
      "  must_have: MIRPE, Nuss, cryoablation\n",
      "  avoid: case-control study\n",
      "\n",
      "DISTILLED FOR TIAB (anchors enforced):\n",
      "  Pq: minimally invasive repair, pectus excavatum, Nuss, MIRPE\n",
      "  Iq: intercostal nerve cryoablation, cryoanalgesia, INC\n",
      "  anchors: MIRPE, Nuss, pectus excavatum\n",
      "\n",
      "CANDIDATES:\n",
      "  B0_seed: hits=20809, rq=3.100, PI=1.00, OUT=0.20, DESIGN=0.00, reason=count_out_of_window; lower_rq\n",
      "  B1_broad: hits=87, rq=3.200, PI=1.00, OUT=0.40, DESIGN=0.00, reason=chosen\n",
      "  B2_I_tight: hits=2232, rq=3.000, PI=1.00, OUT=0.00, DESIGN=0.00, reason=lower_rq\n",
      "  B3_P_core: hits=960, rq=3.100, PI=1.00, OUT=0.20, DESIGN=0.00, reason=lower_rq\n",
      "\n",
      "CHOSEN BROAD\n",
      "  Query: (MIRPE[tiab] OR Nuss[tiab] OR \"pectus excavatum\"[tiab] OR \"minimally invasive repair\"[tiab]) AND (\"intercostal nerve cryoablation\"[tiab] OR cryoanalgesia[tiab] OR INC[tiab]) AND (MIRPE[tiab] OR Nuss[tiab] OR cryoablation[tiab])\n",
      "  Hits=87  in_window=True  target_window=True\n",
      "  Signals: PI=1.00  OUT=0.40  DESIGN=0.00  RQ=3.200\n",
      "  Quick triage (Gemma): include=5 borderline=0 exclude=0 (n=5)\n",
      "  Top titles:\n",
      "   • [40818798] (2025)  Effects of gabapentin following minimally invasive repair of pectus excavatum with intercostal ...  | PI OUT  | s=3.0\n",
      "   • [40300759] (2025)  Cryoanalgesia Plus Nerve Block Strategies versus Cryoanalgesia Alone in Patients with Pectus Ex...  | PI OUT  | s=3.0\n",
      "   • [40803414] (2025)  Feeling the difference: Long-term sensory outcomes following the minimally invasive repair of p...  | PI  | s=2.0\n",
      "   • [40784579] (2025)  Effect of activity restrictions on pectus bar displacement following minimally invasive repair ...  | PI  | s=2.0\n",
      "   • [40720602] (2025)  Intercostal Nerve Cryoablation During Pectus Excavatum Surgery for Postoperative Pain Managemen...  | PI  | s=2.0\n",
      "\n",
      "FOCUSED (BROAD ∧ RCT hedge)\n",
      "  Query: ((MIRPE[tiab] OR Nuss[tiab] OR \"pectus excavatum\"[tiab] OR \"minimally invasive repair\"[tiab]) AND (\"intercostal nerve cryoablation\"[tiab] OR cryoanalgesia[tiab] OR INC[tiab]) AND (MIRPE[tiab] OR Nuss[tiab] OR cryoablation[tiab])) AND (randomized[tiab] OR randomised[tiab] OR randomization[tiab] OR \"random allocation\"[tiab])\n",
      "  Hits=9  in_window=True  target_window=True\n",
      "  Signals: PI=1.00  OUT=0.60  DESIGN=0.40  RQ=4.350\n",
      "  Quick triage (Gemma): include=5 borderline=0 exclude=0 (n=5)\n",
      "  Top titles:\n",
      "   • [39701138] (2024)  Intercostal Nerve Cryoablation as an Effective Pain Management Strategy in the Nuss Procedure: ...  | PI OUT DES  | s=3.5\n",
      "   • [40300759] (2025)  Cryoanalgesia Plus Nerve Block Strategies versus Cryoanalgesia Alone in Patients with Pectus Ex...  | PI OUT  | s=3.0\n",
      "   • [37407278] (2024)  Combined erector spinae plane block with surgical intercostal nerve cryoablation for Nuss proce...  | PI OUT  | s=3.0\n",
      "   • [37364610] (2023)  A Randomized Controlled Trial of Cryoanalgesia for Pain Management following Pectus Excavatum R...  | PI DES  | s=2.5\n",
      "   • [38531584] (2024)  Intercostal nerve cryoablation versus thoracic epidural analgesia for minimal invasive Nuss rep...  | PI  | s=2.0\n",
      "\n",
      "MESH mined (top kept vs dropped):\n",
      "  kept  : adults, minimally invasive repair, pectus excavatum, Nuss, MIRPE, intercostal nerve cryoablation, cryoanalgesia, INC, intraoperative, postoperative\n",
      "  dropped: \n",
      "\n",
      "================== END OF REPORT =====================\n",
      "\n",
      "Artifacts: broad.txt, focused.txt, sniff_report.json  -> C:\\Users\\Galaxy\\LEVI\\projects\\Python\\qucik_sr\\sniff_poc_out\n"
     ]
    }
   ],
   "source": [
    "# SNIPPET: end-to-end \"sniff\" with concise report + idle model eviction\n",
    "# - Deterministic retrieval + query assembly\n",
    "# - Qwen 4B-thinking: term extraction, MeSH role-tagging, reprompts\n",
    "# - Gemma-mini: quick title/abstract triage for chosen BROAD & FOCUSED\n",
    "# - Console-first reporting; minimal artifacts (optional)\n",
    "#\n",
    "# CONFIGURE (env):\n",
    "#   LMSTUDIO_BASE=http://127.0.0.1:1234\n",
    "#   QWEN_MODEL=qwen/qwen3-4b\n",
    "#   GEMMA_MODEL=gemma-3n-e2b-it\n",
    "#   ENTREZ_EMAIL=you@example.com\n",
    "#   ENTREZ_API_KEY=...\n",
    "#   HTTP_TIMEOUT=300\n",
    "#   SAMPLE_N=5\n",
    "#   REPORT_TOP_K=8\n",
    "#   WRITE_ARTIFACTS=1\n",
    "#   LMSTUDIO_EVICT_SECS=5   (try to unload models after idle; best-effort)\n",
    "#\n",
    "# Usage:\n",
    "#   1) Put NLQ into USER_NLQ at the bottom.\n",
    "#   2) Run cell; read the printed report.\n",
    "\n",
    "import os, json, time, re, threading, pathlib, requests\n",
    "from collections import Counter\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "LMSTUDIO_BASE = os.getenv(\"LMSTUDIO_BASE\", \"http://127.0.0.1:1234\").rstrip(\"/\")\n",
    "QWEN_MODEL    = os.getenv(\"QWEN_MODEL\", \"qwen/qwen3-4b\")\n",
    "GEMMA_MODEL   = os.getenv(\"GEMMA_MODEL\", \"gemma-3n-e2b-it\")\n",
    "\n",
    "ENTREZ_EMAIL   = os.getenv(\"ENTREZ_EMAIL\", \"you@example.com\")\n",
    "ENTREZ_API_KEY = os.getenv(\"ENTREZ_API_KEY\", \"\")\n",
    "HTTP_TIMEOUT   = int(os.getenv(\"HTTP_TIMEOUT\", \"300\"))\n",
    "\n",
    "SAMPLE_N       = int(os.getenv(\"SAMPLE_N\", \"5\"))\n",
    "REPORT_TOP_K   = int(os.getenv(\"REPORT_TOP_K\", \"8\"))\n",
    "WRITE_ARTIFACTS= bool(int(os.getenv(\"WRITE_ARTIFACTS\", \"1\")))\n",
    "\n",
    "# Windows / caps\n",
    "YEAR_MIN_DEFAULT = 2015\n",
    "BROAD_TARGET  = (50, 5000)     # sweet spot\n",
    "FOCUSED_TARGET= (3, 500)\n",
    "BROAD_OK      = (10, 10000)\n",
    "FOCUSED_OK    = (1, 2000)\n",
    "\n",
    "RCT_HEDGE_LEX = '(randomized[tiab] OR randomised[tiab] OR randomization[tiab] OR \"random allocation\"[tiab])'\n",
    "\n",
    "OUT_DIR = pathlib.Path(\"sniff_poc_out\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# LM Studio idle evictor (best-effort)\n",
    "# ----------------------------\n",
    "_LM_LAST_USE = time.time()\n",
    "_LM_EVICT_SECS = int(os.getenv(\"LMSTUDIO_EVICT_SECS\", \"5\"))\n",
    "_LM_EVICTOR_STOP = False\n",
    "\n",
    "def _lmstudio_list_models():\n",
    "    try:\n",
    "        r = requests.get(f\"{LMSTUDIO_BASE}/v1/models\", timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"data\", [])\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _lmstudio_try_unload(model_id: str):\n",
    "    # Try a few likely endpoints; ignore failures (LM Studio versions vary)\n",
    "    endpoints = [\n",
    "        (\"POST\", f\"{LMSTUDIO_BASE}/v1/unload\", {\"model\": model_id}),\n",
    "        (\"POST\", f\"{LMSTUDIO_BASE}/v1/models/unload\", {\"model\": model_id}),\n",
    "        (\"POST\", f\"{LMSTUDIO_BASE}/v1/models/unload_all\", {}),\n",
    "        (\"POST\", f\"{LMSTUDIO_BASE}/unload\", {\"model\": model_id}),\n",
    "    ]\n",
    "    for method, url, payload in endpoints:\n",
    "        try:\n",
    "            if method == \"POST\":\n",
    "                rr = requests.post(url, json=payload, timeout=5)\n",
    "            else:\n",
    "                rr = requests.get(url, params=payload, timeout=5)\n",
    "            if rr.status_code < 400:\n",
    "                print(f\"[LM STUDIO] Unload request OK: {url} ({model_id})\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def _lmstudio_idle_evictor():\n",
    "    # Periodically checks for idle and tries to unload loaded models\n",
    "    while not _LM_EVICTOR_STOP:\n",
    "        try:\n",
    "            if _LM_EVICT_SECS > 0 and (time.time() - _LM_LAST_USE) >= _LM_EVICT_SECS:\n",
    "                models = _lmstudio_list_models()\n",
    "                any_loaded = False\n",
    "                for m in models:\n",
    "                    mid = m.get(\"id\") or m.get(\"name\") or \"\"\n",
    "                    loaded = m.get(\"loaded\") or m.get(\"isLoaded\") or False\n",
    "                    if mid and loaded:\n",
    "                        any_loaded = True\n",
    "                        _lmstudio_try_unload(mid)\n",
    "                if any_loaded:\n",
    "                    # give a small grace; then reset last-use so we don't spam\n",
    "                    time.sleep(1.0)\n",
    "                    globals()['_LM_LAST_USE'] = time.time()\n",
    "            time.sleep(1.0)\n",
    "        except Exception:\n",
    "            time.sleep(2.0)\n",
    "\n",
    "if _LM_EVICT_SECS > 0:\n",
    "    _t = threading.Thread(target=_lmstudio_idle_evictor, daemon=True)\n",
    "    _t.start()\n",
    "\n",
    "def _touch_lm():\n",
    "    globals()['_LM_LAST_USE'] = time.time()\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: LM chat + JSON fences (+ robust JSON repair)\n",
    "# ----------------------------\n",
    "def lm_chat(model: str, system: str, user: str, temperature=0.0, max_tokens=None, response_format=None, stop=None):\n",
    "    _touch_lm()\n",
    "    url = f\"{LMSTUDIO_BASE}/v1/chat/completions\"\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "        \"temperature\": float(temperature),\n",
    "        \"stream\": False\n",
    "    }\n",
    "    if max_tokens is not None:\n",
    "        body[\"max_tokens\"] = int(max_tokens)\n",
    "    if response_format is not None:\n",
    "        body[\"response_format\"] = response_format\n",
    "    if stop is not None:\n",
    "        body[\"stop\"] = stop\n",
    "    r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    _touch_lm()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "_BEGIN = re.compile(r\"BEGIN_JSON\\s*\", re.I)\n",
    "_END   = re.compile(r\"\\s*END_JSON\", re.I)\n",
    "FENCE  = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)```\", re.I)\n",
    "\n",
    "def _sanitize_json_str(s: str) -> str:\n",
    "    s = s.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n",
    "    s = re.sub(r\",\\s*(\\}|\\])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_json_block_or_fence(txt: str) -> str:\n",
    "    # prefer last BEGIN_JSON ... END_JSON block\n",
    "    blocks, pos = [], 0\n",
    "    while True:\n",
    "        m1 = _BEGIN.search(txt, pos)\n",
    "        if not m1: break\n",
    "        m2 = _END.search(txt, m1.end())\n",
    "        if not m2: break\n",
    "        blocks.append(txt[m1.end():m2.start()])\n",
    "        pos = m2.end()\n",
    "    if blocks:\n",
    "        return _sanitize_json_str(blocks[-1])\n",
    "    fences = FENCE.findall(txt)\n",
    "    if fences:\n",
    "        return _sanitize_json_str(fences[-1])\n",
    "    # last balanced {...}\n",
    "    s = txt; last_obj=None; stack=0; start=None\n",
    "    for i,ch in enumerate(s):\n",
    "        if ch == '{':\n",
    "            if stack == 0: start = i\n",
    "            stack += 1\n",
    "        elif ch == '}':\n",
    "            if stack > 0:\n",
    "                stack -= 1\n",
    "                if stack == 0 and start is not None:\n",
    "                    last_obj = s[start:i+1]\n",
    "    if last_obj:\n",
    "        return _sanitize_json_str(last_obj)\n",
    "    raise ValueError(\"No JSON-like content found\")\n",
    "\n",
    "REPAIR_SYSTEM = \"You repair malformed JSON to exactly match the given template keys. Return ONLY one JSON object between BEGIN_JSON/END_JSON.\"\n",
    "def repair_user(template_json: str, bad_output: str) -> str:\n",
    "    return f\"\"\"TEMPLATE_JSON:\n",
    "{template_json}\n",
    "\n",
    "BAD_OUTPUT:\n",
    "{bad_output}\n",
    "\n",
    "TASK: Output valid JSON matching TEMPLATE_JSON keys (fill missing with empty arrays/strings). No prose.\n",
    "\n",
    "BEGIN_JSON\n",
    "{{}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "TERMS_TEMPLATE    = {\"population\":[],\"intervention\":[],\"comparators\":[],\"outcomes\":[],\"must_have\":[],\"avoid\":[]}\n",
    "MESH_TAG_TEMPLATE = {\"labels\":[{\"mesh\":\"\",\"role\":\"G\",\"keep\":False,\"why\":\"\"}]}\n",
    "PASSA_TEMPLATE    = {\"pmid\":\"\",\"decision\":\"\",\"reason\":\"\",\"confidence\":0.0,\"population_quote\":\"\",\"intervention_quote\":\"\"}\n",
    "\n",
    "STRICT_JSON_RULES = (\n",
    "  \"Return ONLY one JSON object. No analysis, no preface, no notes. \"\n",
    "  \"Wrap it EXACTLY with:\\nBEGIN_JSON\\n{...}\\nEND_JSON\"\n",
    ")\n",
    "\n",
    "def ask_json_strict(model: str, system: str, user: str, template: dict, max_tokens=None):\n",
    "    user_strict = f\"{user}\\n\\n{STRICT_JSON_RULES}\"\n",
    "    raw = lm_chat(model, system, user_strict, temperature=0.0, max_tokens=max_tokens, stop=[\"END_JSON\"])\n",
    "    try:\n",
    "        return json.loads(extract_json_block_or_fence(raw))\n",
    "    except Exception:\n",
    "        repaired = lm_chat(\n",
    "            model, REPAIR_SYSTEM,\n",
    "            repair_user(json.dumps(template, ensure_ascii=False, indent=2), raw) + \"\\n\\n\" + STRICT_JSON_RULES,\n",
    "            temperature=0.0, max_tokens=max_tokens, stop=[\"END_JSON\"]\n",
    "        )\n",
    "        return json.loads(extract_json_block_or_fence(repaired))\n",
    "\n",
    "ask_json = ask_json_strict  # force strict\n",
    "\n",
    "# ----------------------------\n",
    "# PubMed E-utilities\n",
    "# ----------------------------\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "HEADERS = {\"User-Agent\": \"sniff/0.2 (+local)\", \"Accept\": \"application/json\"}\n",
    "\n",
    "def esearch_count_and_ids(term: str, mindate: int|None):\n",
    "    p = {\"db\":\"pubmed\",\"retmode\":\"json\",\"term\":term,\"retmax\":5000,\"email\":ENTREZ_EMAIL,\"usehistory\":\"y\"}\n",
    "    if ENTREZ_API_KEY: p[\"api_key\"]=ENTREZ_API_KEY\n",
    "    if mindate: p[\"mindate\"]=str(mindate)\n",
    "    r = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params=p, timeout=HTTP_TIMEOUT); r.raise_for_status()\n",
    "    js = r.json().get(\"esearchresult\", {})\n",
    "    count = int(js.get(\"count\",\"0\")); webenv = js.get(\"webenv\"); qk = js.get(\"querykey\")\n",
    "    if not count or not webenv or not qk: return 0, []\n",
    "    r2 = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params={\n",
    "        \"db\":\"pubmed\",\"retmode\":\"json\",\"retmax\":5000,\"retstart\":0,\"email\":ENTREZ_EMAIL,\n",
    "        \"WebEnv\":webenv,\"query_key\":qk, **({\"api_key\":ENTREZ_API_KEY} if ENTREZ_API_KEY else {})\n",
    "    }, timeout=HTTP_TIMEOUT); r2.raise_for_status()\n",
    "    ids = r2.json().get(\"esearchresult\",{}).get(\"idlist\",[])\n",
    "    return count, [str(x) for x in ids]\n",
    "\n",
    "def efetch_xml(pmids):\n",
    "    if not pmids: return \"\"\n",
    "    params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"id\":\",\".join(pmids),\"email\":ENTREZ_EMAIL}\n",
    "    if ENTREZ_API_KEY: params[\"api_key\"]=ENTREZ_API_KEY\n",
    "    r = requests.get(f\"{EUTILS}/efetch.fcgi\", headers={\"User-Agent\": \"sniff/0.2\"}, params=params, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_pubmed_xml(xml_text: str):\n",
    "    out=[]\n",
    "    if not xml_text.strip(): return out\n",
    "    root = ET.fromstring(xml_text)\n",
    "    def _join(node):\n",
    "        if node is None: return \"\"\n",
    "        try: return \"\".join(node.itertext())\n",
    "        except Exception: return node.text or \"\"\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = art.findtext(\".//PMID\") or \"\"\n",
    "        title = _join(art.find(\".//ArticleTitle\")).strip()\n",
    "        abs_nodes = art.findall(\".//Abstract/AbstractText\")\n",
    "        abstract = \" \".join(_join(n).strip() for n in abs_nodes) if abs_nodes else \"\"\n",
    "        year = None\n",
    "        for path in (\".//ArticleDate/Year\",\".//PubDate/Year\",\".//DateCreated/Year\",\".//PubDate/MedlineDate\"):\n",
    "            s = art.findtext(path)\n",
    "            if s:\n",
    "                m = re.search(r\"\\d{4}\", s)\n",
    "                if m: year = int(m.group(0)); break\n",
    "        lang = art.findtext(\".//Language\") or None\n",
    "        pubtypes = [pt.text for pt in art.findall(\".//PublicationTypeList/PublicationType\") if pt.text]\n",
    "        mesh = [mh.findtext(\"./DescriptorName\") for mh in art.findall(\".//MeshHeadingList/MeshHeading\") if mh.findtext(\"./DescriptorName\")]\n",
    "        out.append({\"pmid\": pmid,\"title\": title,\"abstract\": abstract,\"year\": year,\"language\": lang,\"publication_types\": pubtypes,\"mesh\": mesh})\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# LLM prompts (semantic steps)\n",
    "# ----------------------------\n",
    "TERMS_SYSTEM = \"You extract controlled, compact term lists for biomedical retrieval. Return strict JSON only.\"\n",
    "def terms_user(nlq: str):\n",
    "    return f\"\"\"From the natural-language question below, produce compact term lists.\n",
    "\n",
    "NATURAL_LANGUAGE_QUESTION:\n",
    "<<<\n",
    "{nlq}\n",
    ">>>\n",
    "\n",
    "Rules:\n",
    "- Return JSON with arrays of P/I/C/O strings: {{ \"population\":[], \"intervention\":[], \"comparators\":[], \"outcomes\":[] }}\n",
    "- Strings must be concise phrases (no boolean, no field tags, no quotes/brackets).\n",
    "- Include common synonyms and acronyms (e.g., Nuss, MIRPE, cryoanalgesia).\n",
    "- Add 2–5 must_have tokens in \"must_have\" that anchor topicality (e.g., MIRPE, Nuss, cryoablation).\n",
    "- Add 2–5 avoid tokens in \"avoid\" if obvious confounders.\n",
    "- Keep each list ≤ 12 items.\n",
    "\n",
    "Return ONLY:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{...}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "MESH_TAG_SYSTEM = \"You classify MeSH descriptors into roles relative to a PICOS.\"\n",
    "def mesh_tag_user(p_terms, i_terms, descriptors):\n",
    "    return f\"\"\"Classify each MeSH descriptor as one of: P (population/procedure context), I (intervention/analgesia), O (outcome), C (comparator/technique), G (generic context), X (irrelevant).\n",
    "Also give 'keep' (true if useful for building search) and a 1-line rationale.\n",
    "\n",
    "P_TERMS = {p_terms}\n",
    "I_TERMS = {i_terms}\n",
    "DESCRIPTORS = {descriptors}\n",
    "\n",
    "Return ONLY:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{ \"labels\": [{{\"mesh\":\"...\", \"role\":\"P|I|O|C|G|X\", \"keep\": true|false, \"why\": \"...\"}}] }}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "PASSA_SYS = \"You are a strict PRISMA title/abstract screener for effects triage. Return JSON only.\"\n",
    "def passa_user(proto_p, proto_i, proto_outcomes, record):\n",
    "    return f\"\"\"Protocol (simplified):\n",
    "Population: {proto_p}\n",
    "Intervention: {proto_i}\n",
    "Outcomes (signals): {proto_outcomes}\n",
    "Include primary/comparative human studies; exclude admin/guidelines.\n",
    "\n",
    "Record:\n",
    "PMID: {record['pmid']}\n",
    "Title: {record['title']}\n",
    "Abstract: {record['abstract']}\n",
    "PubTypes: {record['publication_types']}\n",
    "Year: {record['year']}\n",
    "Lang: {record['language']}\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{record['pmid']}\",\n",
    "  \"decision\":\"include|borderline|exclude\",\n",
    "  \"reason\":\"population_mismatch|intervention_mismatch|design_ineligible|off_topic|language|year|insufficient_info\",\n",
    "  \"confidence\": 0.0,\n",
    "  \"population_quote\":\"\", \"intervention_quote\":\"\"\n",
    "}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "REPROMPT_SYS = \"You write crisp, actionable reprompts (≤2 sentences) to fix information gaps.\"\n",
    "def reprompt_user(summary_problem: str):\n",
    "    return f\"\"\"Context of failure:\n",
    "{summary_problem}\n",
    "\n",
    "Write ≤2 sentences telling the user exactly what to clarify or relax.\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"reprompt\":\"...\"}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Term distillation & query building\n",
    "# ----------------------------\n",
    "DEMOGRAPHIC_STOP = {\"adult\",\"adults\",\"aged\",\"male\",\"female\",\"child\",\"children\",\"adolescent\",\"young adult\",\"infant\"}\n",
    "GENERIC_I_STOP   = {\"analgesia\",\"pain management\",\"nerve block\",\"anesthesia, conduction\",\"intraoperative analgesia\",\"analgesics\",\"analgesics, opioid\",\"patient-controlled analgesia\",\"perioperative care\"}\n",
    "\n",
    "ANCHORS_P = [\"MIRPE\",\"Nuss\",\"pectus excavatum\"]\n",
    "\n",
    "def normalize_phrase(t: str) -> list[str]:\n",
    "    t = (t or \"\").strip()\n",
    "    if not t: return []\n",
    "    out = {t}\n",
    "    t2 = re.sub(r\"\\s*\\([^)]*\\)\\s*\", \" \", t).strip()\n",
    "    if t2 and t2 != t: out.add(t2)\n",
    "    return list(out)\n",
    "\n",
    "def distilled_terms_for_query(seed_terms: dict, mesh_tagged: dict|None = None):\n",
    "    P_raw = list(seed_terms.get(\"population\") or [])\n",
    "    I_raw = list(seed_terms.get(\"intervention\") or [])\n",
    "\n",
    "    role_keep = set()\n",
    "    if mesh_tagged and \"labels\" in mesh_tagged:\n",
    "        for l in mesh_tagged[\"labels\"]:\n",
    "            if (l.get(\"role\") or \"\") in {\"P\",\"I\"} and l.get(\"keep\"):\n",
    "                role_keep.add((l.get(\"mesh\") or \"\").lower())\n",
    "\n",
    "    def keep_P(t):\n",
    "        tl = t.lower()\n",
    "        if tl in DEMOGRAPHIC_STOP: return False\n",
    "        return True\n",
    "\n",
    "    def keep_I(t):\n",
    "        tl = t.lower()\n",
    "        if tl in GENERIC_I_STOP: return False\n",
    "        if role_keep and tl not in role_keep and not (tl.startswith(\"cryo\") or \"intercostal\" in tl or tl==\"inc\"):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    P, I = [], []\n",
    "    for p in P_raw:\n",
    "        for v in normalize_phrase(p):\n",
    "            if keep_P(v): P.append(v)\n",
    "    for i in I_raw:\n",
    "        for v in normalize_phrase(i):\n",
    "            if keep_I(v): I.append(v)\n",
    "\n",
    "    # ensure anchors in P\n",
    "    anchors = []\n",
    "    Pl = [x.lower() for x in P]\n",
    "    for a in ANCHORS_P:\n",
    "        if a.lower() in Pl:\n",
    "            anchors.append(a)\n",
    "    if not anchors:\n",
    "        anchors = ANCHORS_P[:]  # inject anchors if missing\n",
    "    # dedupe keeping order\n",
    "    P = list(dict.fromkeys(P)); I = list(dict.fromkeys(I)); anchors = list(dict.fromkeys(anchors))\n",
    "    return P, I, anchors\n",
    "\n",
    "def or_block(terms, field=\"tiab\"):\n",
    "    toks=[]; seen=set()\n",
    "    for t in terms:\n",
    "        t = t.strip()\n",
    "        if not t or t.lower() in seen: continue\n",
    "        seen.add(t.lower())\n",
    "        toks.append(f\"\\\"{t}\\\"[{field}]\" if (\" \" in t or \"-\" in t) else f\"{t}[{field}]\")\n",
    "    return \"(\" + \" OR \".join(toks) + \")\" if toks else \"\"\n",
    "\n",
    "def or_block_anchored(terms, anchors, field=\"tiab\"):\n",
    "    toks=[]; seen=set()\n",
    "    for t in anchors + terms:\n",
    "        t = t.strip()\n",
    "        if not t or t.lower() in seen: continue\n",
    "        seen.add(t.lower())\n",
    "        toks.append(f\"\\\"{t}\\\"[{field}]\" if (\" \" in t or \"-\" in t) else f\"{t}[{field}]\")\n",
    "    return \"(\" + \" OR \".join(toks) + \")\" if toks else \"\"\n",
    "\n",
    "def build_broad_from(P_terms, I_terms, anchors, extra=None, field=\"tiab\"):\n",
    "    P = or_block_anchored(P_terms, anchors, field)\n",
    "    I = or_block(I_terms, field)\n",
    "    X = (\" AND \" + or_block(extra, field)) if extra else \"\"\n",
    "    if not P or not I:\n",
    "        return None\n",
    "    return f\"{P} AND {I}{X}\"\n",
    "\n",
    "def build_focused(broad_core):\n",
    "    return f\"({broad_core}) AND {RCT_HEDGE_LEX}\"\n",
    "\n",
    "# ----------------------------\n",
    "# Metrics\n",
    "# ----------------------------\n",
    "PRIMARY_HINTS = {\"Randomized Controlled Trial\",\"Clinical Trial\",\"Controlled Clinical Trial\",\n",
    "                 \"Prospective Studies\",\"Cohort Studies\",\"Case-Control Studies\",\"Comparative Study\"}\n",
    "\n",
    "def lexical_stats(records, p_terms, i_terms, outcomes):\n",
    "    def hits(text, terms):\n",
    "        tl=(text or \"\").lower()\n",
    "        return sum(1 for t in terms if t and t.lower() in tl)\n",
    "    n = min(SAMPLE_N, len(records))\n",
    "    sample = records[:n]\n",
    "    pi_rate=out_rate=design_rate=0\n",
    "    scores=[]\n",
    "    for r in sample:\n",
    "        t = (r['title'] or \"\") + \"\\n\" + (r['abstract'] or \"\")\n",
    "        pi = (hits(t, p_terms)+hits(t, i_terms))>0\n",
    "        po = hits(t, outcomes)>0\n",
    "        de = len(set(r['publication_types']) & PRIMARY_HINTS)>0\n",
    "        pi_rate += 1 if pi else 0\n",
    "        out_rate+= 1 if po else 0\n",
    "        design_rate+= 1 if de else 0\n",
    "        s = (2.0*(1 if pi else 0) + 1.0*(1 if po else 0) + 0.5*(1 if de else 0))\n",
    "        scores.append(s)\n",
    "    if n==0:\n",
    "        return {\"n_sample\":0,\"pi_rate\":0,\"outcome_rate\":0,\"design_rate\":0,\"median_score\":0,\"mean_score\":0}\n",
    "    scores.sort()\n",
    "    med = scores[n//2]\n",
    "    mean = sum(scores)/n\n",
    "    return {\"n_sample\":n,\"pi_rate\":round(pi_rate/n,3),\"outcome_rate\":round(out_rate/n,3),\n",
    "            \"design_rate\":round(design_rate/n,3),\"median_score\":round(med,3),\"mean_score\":round(mean,3)}\n",
    "\n",
    "def rq_quality(stats):\n",
    "    return round(stats[\"median_score\"] + 0.25*stats[\"mean_score\"] + 0.5*stats[\"pi_rate\"] + 0.25*stats[\"outcome_rate\"], 3)\n",
    "\n",
    "def top_titles(records, p_terms, i_terms, outcomes, k=8):\n",
    "    rows=[]\n",
    "    def hits(text, terms):\n",
    "        tl=(text or \"\").lower()\n",
    "        return sum(1 for t in terms if t and t.lower() in tl)\n",
    "    for r in records:\n",
    "        t = (r['title'] or \"\") + \"\\n\" + (r['abstract'] or \"\")\n",
    "        pi = (hits(t, p_terms)+hits(t, i_terms))>0\n",
    "        po = hits(t, outcomes)>0\n",
    "        de = len(set(r['publication_types']) & PRIMARY_HINTS)>0\n",
    "        score = (2.0*(1 if pi else 0) + 1.0*(1 if po else 0) + 0.5*(1 if de else 0))\n",
    "        rows.append((score, r['pmid'], r['year'], r['title'], pi, po, de))\n",
    "    rows.sort(key=lambda x: (-x[0], -(x[2] or 0)))\n",
    "    return rows[:k]\n",
    "\n",
    "# ----------------------------\n",
    "# Main runner\n",
    "# ----------------------------\n",
    "def sniff_nlq(USER_NLQ: str, year_min: int = YEAR_MIN_DEFAULT):\n",
    "    # 1) Qwen: extract seed term lists\n",
    "    terms_js = ask_json(QWEN_MODEL, TERMS_SYSTEM, terms_user(USER_NLQ), TERMS_TEMPLATE, max_tokens=None)\n",
    "    P0 = terms_js.get(\"population\", []) or []\n",
    "    I0 = terms_js.get(\"intervention\", []) or []\n",
    "    O0 = terms_js.get(\"outcomes\", []) or []\n",
    "    MUST = terms_js.get(\"must_have\", []) or []\n",
    "\n",
    "    # 2) Initial broad (seed P/I)\n",
    "    candidates=[]\n",
    "    def try_query(name, q):\n",
    "        cnt, ids = esearch_count_and_ids(q, year_min)\n",
    "        xml = efetch_xml(ids[:SAMPLE_N])\n",
    "        recs = parse_pubmed_xml(xml)\n",
    "        stats = lexical_stats(recs, P0, I0, O0)\n",
    "        rq = rq_quality(stats)\n",
    "        candidates.append({\"name\":name,\"query\":q,\"total\":cnt,\"stats\":stats,\"rq\":rq,\"ids\":ids,\"sample\":recs})\n",
    "        print(f\"{time.strftime('%H:%M:%S')}  {name} hits={cnt} rq={rq} stats={stats}\")\n",
    "        return cnt, recs\n",
    "\n",
    "    # 2a) MeSH from initial sample\n",
    "    q_seed = build_broad_from(P0, I0, anchors=ANCHORS_P, extra=None)\n",
    "    if not q_seed:\n",
    "        rp = ask_json(QWEN_MODEL, REPROMPT_SYS, reprompt_user(\"Failed to construct core P and I term groups from your question.\"), {\"reprompt\":\"\"}, max_tokens=None)\n",
    "        raise SystemExit(\"REPROMPT: \" + (rp.get(\"reprompt\",\"need clarification\")))\n",
    "    _, recs_seed = try_query(\"B0_seed\", q_seed)\n",
    "\n",
    "    # 3) Mine MeSH from B0 sample and role-tag it\n",
    "    mesh_all = Counter()\n",
    "    for r in recs_seed:\n",
    "        for m in r.get(\"mesh\", []) or []:\n",
    "            mesh_all[m] += 1\n",
    "    top_mesh = [m for m,_ in mesh_all.most_common(40)]\n",
    "    mesh_tag_js = ask_json(QWEN_MODEL, MESH_TAG_SYSTEM, mesh_tag_user(P0, I0, top_mesh), MESH_TAG_TEMPLATE, max_tokens=None)\n",
    "\n",
    "    # 4) Distill terms (drop demographics/generic I; enforce anchors)\n",
    "    Pq, Iq, anchors = distilled_terms_for_query(terms_js, mesh_tag_js)\n",
    "\n",
    "    # 5) Build variants using distilled terms\n",
    "    q_broad  = build_broad_from(Pq, Iq, anchors, extra=MUST)\n",
    "    q_i_tight= build_broad_from(P0, Iq, anchors, extra=None)\n",
    "    q_p_core = build_broad_from(Pq, I0, anchors, extra=None)\n",
    "\n",
    "    _, _ = try_query(\"B1_broad\", q_broad)\n",
    "    if q_i_tight: try_query(\"B2_I_tight\", q_i_tight)\n",
    "    if q_p_core:  try_query(\"B3_P_core\",  q_p_core)\n",
    "\n",
    "    # choose broad: best rq within BROAD_TARGET else within BROAD_OK\n",
    "    def choose_broad():\n",
    "        cands = [c for c in candidates if c[\"name\"].startswith(\"B\")]\n",
    "        in_target = [c for c in cands if BROAD_TARGET[0] <= c[\"total\"] <= BROAD_TARGET[1]]\n",
    "        pool = in_target if in_target else [c for c in cands if BROAD_OK[0] <= c[\"total\"] <= BROAD_OK[1]]\n",
    "        return max((pool or cands), key=lambda x: x[\"rq\"]) if (pool or cands) else None\n",
    "\n",
    "    chosen_broad = choose_broad()\n",
    "    if not chosen_broad:\n",
    "        rp = ask_json(QWEN_MODEL, REPROMPT_SYS, reprompt_user(\"No broad query produced viable hit counts or relevance.\"), {\"reprompt\":\"\"}, max_tokens=None)\n",
    "        print(\"\\nREPROMPT:\", rp.get(\"reprompt\",\"need clarification\")); return\n",
    "\n",
    "    # 6) Focused\n",
    "    q_focused = build_focused(chosen_broad[\"query\"])\n",
    "    cntF, idsF = esearch_count_and_ids(q_focused, year_min)\n",
    "    xmlF = efetch_xml(idsF[:SAMPLE_N])\n",
    "    recsF = parse_pubmed_xml(xmlF)\n",
    "    statsF = lexical_stats(recsF, P0, I0, O0); rqF = rq_quality(statsF)\n",
    "    focused = {\"name\":\"F_focused\",\"query\":q_focused,\"total\":cntF,\"stats\":statsF,\"rq\":rqF,\"ids\":idsF,\"sample\":recsF}\n",
    "    candidates.append(focused)\n",
    "\n",
    "    # 7) Gemma triage for chosen sets\n",
    "    def sanity_screen(records):\n",
    "        if not records: return {\"include\":0,\"borderline\":0,\"exclude\":0,\"n\":0}\n",
    "        inc=bor=exc=0\n",
    "        for r in records[:min(SAMPLE_N,len(records))]:\n",
    "            js = ask_json(GEMMA_MODEL, PASSA_SYS, passa_user(P0, I0, O0, r), PASSA_TEMPLATE, max_tokens=None)\n",
    "            d = (js.get(\"decision\",\"\") or \"\").lower()\n",
    "            if d==\"include\": inc+=1\n",
    "            elif d==\"borderline\": bor+=1\n",
    "            else: exc+=1\n",
    "        n=inc+bor+exc\n",
    "        return {\"include\":inc,\"borderline\":bor,\"exclude\":exc,\"n\":n}\n",
    "\n",
    "    sanity_broad  = sanity_screen(chosen_broad[\"sample\"])\n",
    "    sanity_focused= sanity_screen(focused[\"sample\"])\n",
    "\n",
    "    # 8) Prepare report data\n",
    "    def why_not(c, chosen):\n",
    "        reasons=[]\n",
    "        if c is chosen: return \"chosen\"\n",
    "        if c[\"total\"] < BROAD_OK[0] or c[\"total\"] > BROAD_OK[1]: reasons.append(\"count_out_of_window\")\n",
    "        if c[\"rq\"] < chosen[\"rq\"]: reasons.append(\"lower_rq\")\n",
    "        return \"; \".join(reasons) or \"ok\"\n",
    "\n",
    "    kept_mesh = [x[\"mesh\"] for x in mesh_tag_js.get(\"labels\", []) if x.get(\"keep\")]\n",
    "    drop_mesh = [x[\"mesh\"] for x in mesh_tag_js.get(\"labels\", []) if not x.get(\"keep\")]\n",
    "\n",
    "    # 9) Print concise report\n",
    "    def yn(x): return \"yes\" if x else \"no\"\n",
    "    def ok_range(lo,hi,x): return lo<=x<=hi\n",
    "\n",
    "    print(\"\\n==================== SNIFF REPORT ====================\")\n",
    "    print(\"NLQ:\")\n",
    "    print(\"  \", USER_NLQ[:200].replace(\"\\n\",\" \") + (\"...\" if len(USER_NLQ)>200 else \"\"))\n",
    "    print(\"\\nSEED TERMS (Qwen):\")\n",
    "    print(\"  P:\", \", \".join(P0[:10]))\n",
    "    print(\"  I:\", \", \".join(I0[:10]))\n",
    "    if terms_js.get(\"comparators\"): print(\"  C:\", \", \".join((terms_js.get(\"comparators\") or [])[:10]))\n",
    "    print(\"  O:\", \", \".join(O0[:10]))\n",
    "    if terms_js.get(\"must_have\"):  print(\"  must_have:\", \", \".join((terms_js.get(\"must_have\") or [])[:8]))\n",
    "    if terms_js.get(\"avoid\"):      print(\"  avoid:\", \", \".join((terms_js.get(\"avoid\") or [])[:8]))\n",
    "\n",
    "    print(\"\\nDISTILLED FOR TIAB (anchors enforced):\")\n",
    "    print(\"  Pq:\", \", \".join(Pq[:10]))\n",
    "    print(\"  Iq:\", \", \".join(Iq[:10]))\n",
    "    print(\"  anchors:\", \", \".join(anchors))\n",
    "\n",
    "    print(\"\\nCANDIDATES:\")\n",
    "    for c in candidates:\n",
    "        if c[\"name\"].startswith(\"F_\"): continue\n",
    "        s=c[\"stats\"]\n",
    "        print(f\"  {c['name']}: hits={c['total']}, rq={c['rq']:.3f}, PI={s['pi_rate']:.2f}, OUT={s['outcome_rate']:.2f}, DESIGN={s['design_rate']:.2f}, reason={why_not(c, chosen_broad)}\")\n",
    "\n",
    "    print(\"\\nCHOSEN BROAD\")\n",
    "    s = chosen_broad[\"stats\"]\n",
    "    print(\"  Query:\", chosen_broad[\"query\"])\n",
    "    print(f\"  Hits={chosen_broad['total']}  in_window={ok_range(*BROAD_OK, chosen_broad['total'])}  target_window={ok_range(*BROAD_TARGET, chosen_broad['total'])}\")\n",
    "    print(f\"  Signals: PI={s['pi_rate']:.2f}  OUT={s['outcome_rate']:.2f}  DESIGN={s['design_rate']:.2f}  RQ={chosen_broad['rq']:.3f}\")\n",
    "    if sanity_broad[\"n\"]:\n",
    "        print(f\"  Quick triage (Gemma): include={sanity_broad['include']} borderline={sanity_broad['borderline']} exclude={sanity_broad['exclude']} (n={sanity_broad['n']})\")\n",
    "    tb = top_titles(chosen_broad[\"sample\"], P0, I0, O0, k=REPORT_TOP_K)\n",
    "    print(\"  Top titles:\")\n",
    "    for sc, pmid, yr, ttl, pi, po, de in tb:\n",
    "        flags = []\n",
    "        if pi: flags.append(\"PI\")\n",
    "        if po: flags.append(\"OUT\")\n",
    "        if de: flags.append(\"DES\")\n",
    "        print(f\"   • [{pmid}] ({yr})  {ttl[:95]}{'...' if len(ttl)>95 else ''}  | {' '.join(flags) or '-'}  | s={sc:.1f}\")\n",
    "\n",
    "    print(\"\\nFOCUSED (BROAD ∧ RCT hedge)\")\n",
    "    s = focused[\"stats\"]\n",
    "    print(\"  Query:\", focused[\"query\"])\n",
    "    print(f\"  Hits={focused['total']}  in_window={ok_range(*FOCUSED_OK, focused['total'])}  target_window={ok_range(*FOCUSED_TARGET, focused['total'])}\")\n",
    "    print(f\"  Signals: PI={s['pi_rate']:.2f}  OUT={s['outcome_rate']:.2f}  DESIGN={s['design_rate']:.2f}  RQ={focused['rq']:.3f}\")\n",
    "    if sanity_focused[\"n\"]:\n",
    "        print(f\"  Quick triage (Gemma): include={sanity_focused['include']} borderline={sanity_focused['borderline']} exclude={sanity_focused['exclude']} (n={sanity_focused['n']})\")\n",
    "    tf = top_titles(focused[\"sample\"], P0, I0, O0, k=REPORT_TOP_K)\n",
    "    print(\"  Top titles:\")\n",
    "    for sc, pmid, yr, ttl, pi, po, de in tf:\n",
    "        flags=[]\n",
    "        if pi: flags.append(\"PI\")\n",
    "        if po: flags.append(\"OUT\")\n",
    "        if de: flags.append(\"DES\")\n",
    "        print(f\"   • [{pmid}] ({yr})  {ttl[:95]}{'...' if len(ttl)>95 else ''}  | {' '.join(flags) or '-'}  | s={sc:.1f}\")\n",
    "\n",
    "    print(\"\\nMESH mined (top kept vs dropped):\")\n",
    "    print(\"  kept  :\", \", \".join(kept_mesh[:12]))\n",
    "    print(\"  dropped:\", \", \".join(drop_mesh[:12]))\n",
    "\n",
    "    print(\"\\n================== END OF REPORT =====================\")\n",
    "\n",
    "    # 10) Minimal artifacts (optional)\n",
    "    if WRITE_ARTIFACTS:\n",
    "        report = {\n",
    "            \"nlq\": USER_NLQ,\n",
    "            \"year_min\": year_min,\n",
    "            \"seed_terms\": terms_js,\n",
    "            \"distilled\": {\"Pq\": Pq, \"Iq\": Iq, \"anchors\": anchors},\n",
    "            \"mesh\": {\"top\": top_mesh, \"tagged\": mesh_tag_js},\n",
    "            \"candidates\": [\n",
    "                {k: v for k,v in c.items() if k not in (\"ids\",\"sample\")}\n",
    "                for c in candidates if not c[\"name\"].startswith(\"F_\")\n",
    "            ],\n",
    "            \"chosen\": {\n",
    "                \"broad\": {k: v for k,v in chosen_broad.items() if k not in (\"ids\",\"sample\")},\n",
    "                \"focused\": {k: v for k,v in focused.items() if k not in (\"ids\",\"sample\")},\n",
    "                \"sanity\": {\"broad\": sanity_broad, \"focused\": sanity_focused}\n",
    "            }\n",
    "        }\n",
    "        (OUT_DIR/\"broad.txt\").write_text(chosen_broad[\"query\"], encoding=\"utf-8\")\n",
    "        (OUT_DIR/\"focused.txt\").write_text(focused[\"query\"], encoding=\"utf-8\")\n",
    "        (OUT_DIR/\"sniff_report.json\").write_text(json.dumps(report, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "        print(f\"\\nArtifacts: broad.txt, focused.txt, sniff_report.json  -> {OUT_DIR.resolve()}\")\n",
    "\n",
    "# ----------------------------\n",
    "# RUN: put your NLQ here\n",
    "# ----------------------------\n",
    "USER_NLQ = \"\"\"Population = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE (the intervention of interest is INC, not the surgery). Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia. Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days. Study designs = RCTs preferred; if RCTs absent, include comparative cohort/case-control. Year_min = 2015. Languages = English, Portuguese, Spanish.\"\"\"\n",
    "sniff_nlq(USER_NLQ, year_min=YEAR_MIN_DEFAULT)\n",
    "\n",
    "# On notebook shutdown, stop the evictor\n",
    "_LM_EVICTOR_STOP = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d2444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:17:51  B0_seed probing...\n",
      "13:20:28  B1_broad hits=176 rq=3.1 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.2, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 2.2, 'years': {'median': 2025, 'min': 2025, 'max': 2025}, 'languages': {'eng': 5}, 'ptypes': {'Journal Article': 5, 'Review': 2}, 'top_mesh': ['Humans', 'Funnel Chest', 'Pain Management', 'Pain, Postoperative', 'Perioperative Care']}\n",
      "13:20:30  B2_I_tight hits=176 rq=3.1 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.2, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 2.2, 'years': {'median': 2025, 'min': 2025, 'max': 2025}, 'languages': {'eng': 5}, 'ptypes': {'Journal Article': 5, 'Review': 2}, 'top_mesh': ['Humans', 'Funnel Chest', 'Pain Management', 'Pain, Postoperative', 'Perioperative Care']}\n",
      "13:20:37  B3_P_core hits=118 rq=3.1 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.2, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 2.2, 'years': {'median': 2025, 'min': 2025, 'max': 2025}, 'languages': {'eng': 5}, 'ptypes': {'Journal Article': 5, 'Review': 1}, 'top_mesh': ['Humans', 'Funnel Chest', 'Pain Management', 'Pain, Postoperative', 'Perioperative Care']}\n",
      "13:20:45  B4_with_comparator hits=48 rq=3.15 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.2, 'design_rate': 0.4, 'median_score': 2.0, 'mean_score': 2.4, 'years': {'median': 2024, 'min': 2024, 'max': 2025}, 'languages': {'eng': 5}, 'ptypes': {'Journal Article': 5, 'Systematic Review': 1, 'Network Meta-Analysis': 1, 'Observational Study': 1, 'Comparative Study': 1, 'Randomized Controlled Trial': 1}, 'top_mesh': ['Humans', 'Pain, Postoperative', 'Nerve Block', 'Intercostal Nerves', 'Funnel Chest', 'Pain Management', 'Length of Stay', 'Cryosurgery', 'Minimally Invasive Surgical Procedures', 'Analgesia, Patient-Controlled']}\n",
      "13:20:49  B_lang_filtered hits=173 rq=3.1 stats={'n_sample': 5, 'pi_rate': 1.0, 'outcome_rate': 0.2, 'design_rate': 0.0, 'median_score': 2.0, 'mean_score': 2.2, 'years': {'median': 2025, 'min': 2025, 'max': 2025}, 'languages': {'eng': 5}, 'ptypes': {'Journal Article': 5, 'Review': 2}, 'top_mesh': ['Humans', 'Funnel Chest', 'Pain Management', 'Pain, Postoperative', 'Perioperative Care']}\n",
      "\n",
      "==================== SNIFF REPORT ====================\n",
      "NLQ:\n",
      "   Population = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). Intervention = intercostal nerve cryoablation (INC) used intraoperati...\n",
      "\n",
      "SEED TERMS (Qwen):\n",
      "  P: adults, pectus excavatum, Nuss, MIRPE, minimally invasive repair\n",
      "  I: intercostal nerve cryoablation, cryoanalgesia, intraoperative analgesia, Nuss surgery\n",
      "  C: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia\n",
      "  O: postoperative opioid consumption, pain scores (0-7 days), opioid use in-hospital, discharge opioid use, pain assessment\n",
      "  must_have: MIRPE, Nuss, cryoablation\n",
      "\n",
      "DISTILLED FOR TIAB (anchors enforced):\n",
      "  Pq: adults, pectus excavatum, Nuss, MIRPE, minimally invasive repair, Funnel Chest, Perioperative Care\n",
      "  Iq: intercostal nerve cryoablation, cryoanalgesia, intraoperative analgesia, Nuss surgery, Pain Management\n",
      "  anchors: MIRPE, Nuss, cryoablation\n",
      "\n",
      "CANDIDATES:\n",
      "  B0_seed: hits=118 PI=1.00 OUT=0.20 DES=0.00  RQ=3.100  reason=lower_rq\n",
      "  B1_broad: hits=176 PI=1.00 OUT=0.20 DES=0.00  RQ=3.100  reason=lower_rq\n",
      "  B2_I_tight: hits=176 PI=1.00 OUT=0.20 DES=0.00  RQ=3.100  reason=lower_rq\n",
      "  B3_P_core: hits=118 PI=1.00 OUT=0.20 DES=0.00  RQ=3.100  reason=lower_rq\n",
      "  B4_with_comparator: hits=48 PI=1.00 OUT=0.20 DES=0.40  RQ=3.150  reason=lower_rq\n",
      "  B_lang_filtered: hits=173 PI=1.00 OUT=0.20 DES=0.00  RQ=3.100  reason=chosen_broad\n",
      "\n",
      "CHOSEN BROAD\n",
      "  Query: (adults[tiab] OR \"pectus excavatum\"[tiab] OR Nuss[tiab] OR MIRPE[tiab] OR \"minimally invasive repair\"[tiab] OR \"Funnel Chest\"[tiab] OR \"Perioperative Care\"[tiab]) AND (\"intercostal nerve cryoablation\"[tiab] OR cryoanalgesia[tiab] OR \"intraoperative analgesia\"[tiab] OR \"Nuss surgery\"[tiab] OR \"Pain Management\"[tiab]) AND (MIRPE[tiab] OR Nuss[tiab] OR cryoablation[tiab]) AND (\"english\"[lang] OR \"portuguese\"[lang] OR \"spanish\"[lang])\n",
      "  Hits=173  in_window=True  target_window=True\n",
      "  Signals: PI=1.00 OUT=0.20 DES=0.00  RQ=3.100\n",
      "  Quick triage (Gemma): include=2 borderline=1 exclude=2 (n=5)\n",
      "  Top titles:\n",
      "   • [40918613] (2025)  Best Evidence Summary for Perioperative Pain Management in Patients With Pectus Excavatum.  | PI OUT\n",
      "   • [40902820] (2025)  Same Day Discharge Following Minimally Invasive Repair of Pectus Excavatum.  | PI\n",
      "   • [40853453] (2025)  The Nuss and reverse Nuss procedure paradox: divergent outcomes in scoliosis risk for pect...  | PI\n",
      "   • [40853261] (2025)  Integrative Medicine Modalities for Adolescent Patients Undergoing Minimally Invasive Repa...  | PI\n",
      "   • [40818798] (2025)  Effects of gabapentin following minimally invasive repair of pectus excavatum with interco...  | PI\n",
      "\n",
      "FOCUSED (BROAD ∧ trial filters)\n",
      "  Query: ((adults[tiab] OR \"pectus excavatum\"[tiab] OR Nuss[tiab] OR MIRPE[tiab] OR \"minimally invasive repair\"[tiab] OR \"Funnel Chest\"[tiab] OR \"Perioperative Care\"[tiab]) AND (\"intercostal nerve cryoablation\"[tiab] OR cryoanalgesia[tiab] OR \"intraoperative analgesia\"[tiab] OR \"Nuss surgery\"[tiab] OR \"Pain Management\"[tiab]) AND (MIRPE[tiab] OR Nuss[tiab] OR cryoablation[tiab]) AND (\"english\"[lang] OR \"portuguese\"[lang] OR \"spanish\"[lang])) AND (randomized[tiab] OR randomised[tiab] OR randomization[tiab] OR \"random allocation\"[tiab])\n",
      "  Hits=22  in_window=True  target_window=True\n",
      "  Signals: PI=1.00 OUT=0.60 DES=0.60  RQ=4.375\n",
      "  Quick triage (Gemma): include=2 borderline=0 exclude=3 (n=5)\n",
      "  Top titles:\n",
      "   • [40918613] (2025)  Best Evidence Summary for Perioperative Pain Management in Patients With Pectus Excavatum.  | PI OUT\n",
      "   • [40300759] (2025)  Cryoanalgesia Plus Nerve Block Strategies versus Cryoanalgesia Alone in Patients with Pect...  | PI OUT\n",
      "   • [39701138] (2024)  Intercostal Nerve Cryoablation as an Effective Pain Management Strategy in the Nuss Proced...  | PI OUT DES\n",
      "   • [39522714] (2024)  A randomized study of cryoablation of intercostal nerves in patients undergoing minimally ...  | PI DES\n",
      "   • [39353112] (2024)  Erector Spinae Plane Block Provided Comparable Analgesia as Thoracic Paravertebral Block P...  | PI DES\n",
      "\n",
      "MESH mined (top kept tokens):\n",
      "  kept  : adults, pectus excavatum, Nuss, MIRPE, minimally invasive repair, Funnel Chest, Perioperative Care, intercostal nerve cryoablation, cryoanalgesia, intraoperative analgesia\n",
      "\n",
      "SAMPLE DISTRIBUTIONS (broad sample):\n",
      "  years: median=2025 range=(2025,2025)\n",
      "  languages: eng:5\n",
      "  pub types: Journal Article:5, Review:2\n",
      "\n",
      "Protocol brief (used for Gemma triage):\n",
      "  FOCUS: adults with pectus excavatum undergoing MIRPE/Nuss.\n",
      "  POP (include): adults, pectus excavatum, Nuss, MIRPE, minimally invasive repair\n",
      "  INDEX INTERVENTION: intercostal nerve cryoablation, cryoanalgesia, intraoperative analgesia, Nuss surgery\n",
      "  COMPARATORS: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia\n",
      "  OUTCOMES (primary): postoperative opioid consumption, pain scores (0-7 days), opioid use in-hospital, discharge opioid use, pain assessment\n",
      "  DESIGNS: RCTs preferred; include comparative cohort/case-control if RCTs absent.\n",
      "  YEAR_MIN: 2015; LANGUAGES: english, portuguese, spanish\n",
      "  ANCHORS (must have): MIRPE, Nuss, cryoablation\n",
      "  HARD EXCLUDES: guidelines, editorials, technical notes without outcomes\n",
      "\n",
      "================== END OF REPORT =====================\n"
     ]
    }
   ],
   "source": [
    "# SNIFF STAGE — Overhauled, protocol-aware, with concise report and LM Studio auto-evict\n",
    "# - Qwen: seed extraction + MeSH role-tagging\n",
    "# - Gemma: protocol-based title/abstract triage (with gates)\n",
    "# - Deterministic PubMed probing with multiple query variants\n",
    "# - Rich, human-readable report + machine artifacts\n",
    "#\n",
    "# CONFIGURE:\n",
    "#   LM Studio server, model names, Entrez email/API, and optional env knobs below.\n",
    "#\n",
    "# ENV knobs (optional):\n",
    "#   LMSTUDIO_BASE=http://127.0.0.1:1234\n",
    "#   QWEN_MODEL=qwen/qwen3-4b\n",
    "#   GEMMA_MODEL=gemma-3n-e2b-it\n",
    "#   LM_KEEP_ALIVE_SEC=5\n",
    "#   HTTP_TIMEOUT=300\n",
    "#   SAMPLE_N=5\n",
    "#   YEAR_MIN_DEFAULT=2015\n",
    "#\n",
    "# Usage:\n",
    "#   Put your NLQ in USER_NLQ and run sniff_nlq(USER_NLQ).\n",
    "\n",
    "import os, json, time, re, statistics, pathlib\n",
    "from collections import Counter, defaultdict\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "LMSTUDIO_BASE = os.getenv(\"LMSTUDIO_BASE\", \"http://127.0.0.1:1234\")\n",
    "QWEN_MODEL    = os.getenv(\"QWEN_MODEL\", \"qwen/qwen3-4b\")\n",
    "GEMMA_MODEL   = os.getenv(\"GEMMA_MODEL\", \"gemma-3n-e2b-it\")\n",
    "LM_KEEP_ALIVE = str(os.getenv(\"LM_KEEP_ALIVE_SEC\", \"5\")) + \"s\"  # ask LM Studio to unload model ~5s after idle\n",
    "ENTREZ_EMAIL   = os.getenv(\"ENTREZ_EMAIL\", \"you@example.com\")\n",
    "ENTREZ_API_KEY = os.getenv(\"ENTREZ_API_KEY\", \"\")\n",
    "HTTP_TIMEOUT   = int(os.getenv(\"HTTP_TIMEOUT\", \"300\"))\n",
    "\n",
    "OUT_DIR = pathlib.Path(\"sniff_poc_out\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Windows / caps (soft)\n",
    "YEAR_MIN_DEFAULT = int(os.getenv(\"YEAR_MIN_DEFAULT\", \"2015\"))\n",
    "BROAD_TARGET   = (50, 5000)\n",
    "FOCUSED_TARGET = (3, 500)\n",
    "BROAD_OK       = (10, 10000)\n",
    "FOCUSED_OK     = (1, 2000)\n",
    "\n",
    "SAMPLE_N = int(os.getenv(\"SAMPLE_N\", \"5\"))\n",
    "RCT_HEDGE_LEX = '(randomized[tiab] OR randomised[tiab] OR randomization[tiab] OR \"random allocation\"[tiab])'\n",
    "\n",
    "LANG_NAME_TO_TAG = {\n",
    "    \"english\":\"english\", \"en\":\"english\",\n",
    "    \"portuguese\":\"portuguese\", \"pt\":\"portuguese\",\n",
    "    \"spanish\":\"spanish\", \"es\":\"spanish\"\n",
    "}\n",
    "\n",
    "PRIMARY_HINTS = {\n",
    "    \"Randomized Controlled Trial\",\"Clinical Trial\",\"Controlled Clinical Trial\",\n",
    "    \"Prospective Studies\",\"Cohort Studies\",\"Case-Control Studies\",\"Comparative Study\"\n",
    "}\n",
    "\n",
    "PTYPES_TRIAL_COMP = [\n",
    "    \"randomized controlled trial\",\"controlled clinical trial\",\"comparative study\",\n",
    "    \"clinical trial\",\"cohort studies\",\"case-control studies\",\"prospective studies\"\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# HTTP helpers — LM Studio chat (with keep_alive)\n",
    "# ----------------------------\n",
    "def lm_chat(model: str, system: str, user: str, temperature=0.0, max_tokens=8000, response_format=None, stop=None):\n",
    "    \"\"\"\n",
    "    LM Studio-compatible /v1/chat/completions call.\n",
    "    Adds keep_alive to request; server may unload model after ~LM_KEEP_ALIVE idle.\n",
    "    \"\"\"\n",
    "    url = f\"{LMSTUDIO_BASE.rstrip('/')}/v1/chat/completions\"\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "        \"temperature\": float(temperature),\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"stream\": False,\n",
    "        \"keep_alive\": LM_KEEP_ALIVE\n",
    "    }\n",
    "    if response_format is not None:\n",
    "        body[\"response_format\"] = response_format\n",
    "    if stop is not None:\n",
    "        body[\"stop\"] = stop\n",
    "    r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ----------------------------\n",
    "# JSON extraction/repair\n",
    "# ----------------------------\n",
    "_BEGIN = re.compile(r\"BEGIN_JSON\\s*\", re.I)\n",
    "_END   = re.compile(r\"\\s*END_JSON\", re.I)\n",
    "FENCE  = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)```\", re.I)\n",
    "\n",
    "def _sanitize_json_str(s: str) -> str:\n",
    "    s = s.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n",
    "    s = re.sub(r\",\\s*(\\}|\\])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_json_block_or_fence(txt: str) -> str:\n",
    "    blocks = []\n",
    "    pos = 0\n",
    "    while True:\n",
    "        m1 = _BEGIN.search(txt, pos)\n",
    "        if not m1: break\n",
    "        m2 = _END.search(txt, m1.end())\n",
    "        if not m2: break\n",
    "        blocks.append(txt[m1.end():m2.start()])\n",
    "        pos = m2.end()\n",
    "    if blocks:\n",
    "        return _sanitize_json_str(blocks[-1])\n",
    "    fences = FENCE.findall(txt)\n",
    "    if fences:\n",
    "        return _sanitize_json_str(fences[-1])\n",
    "    # last {...}\n",
    "    s = txt\n",
    "    last_obj = None\n",
    "    stack = 0; start = None\n",
    "    for i,ch in enumerate(s):\n",
    "        if ch == '{':\n",
    "            if stack == 0: start = i\n",
    "            stack += 1\n",
    "        elif ch == '}':\n",
    "            if stack > 0:\n",
    "                stack -= 1\n",
    "                if stack == 0 and start is not None:\n",
    "                    last_obj = s[start:i+1]\n",
    "    if last_obj:\n",
    "        return _sanitize_json_str(last_obj)\n",
    "    raise ValueError(\"No JSON-like content found\")\n",
    "\n",
    "REPAIR_SYSTEM = \"You repair malformed JSON to exactly match the given template keys. Return ONLY one JSON object between BEGIN_JSON/END_JSON.\"\n",
    "def repair_user(template_json: str, bad_output: str) -> str:\n",
    "    return f\"\"\"TEMPLATE_JSON:\n",
    "{template_json}\n",
    "\n",
    "BAD_OUTPUT:\n",
    "{bad_output}\n",
    "\n",
    "TASK: Output valid JSON matching TEMPLATE_JSON keys (fill missing with empty arrays/strings). No prose.\n",
    "\n",
    "BEGIN_JSON\n",
    "{{}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "TERMS_TEMPLATE    = {\"population\":[],\"intervention\":[],\"comparators\":[],\"outcomes\":[],\"must_have\":[],\"avoid\":[]}\n",
    "MESH_TAG_TEMPLATE = {\"labels\":[{\"mesh\":\"\",\"role\":\"G\",\"keep\":False,\"why\":\"\"}]}\n",
    "PASSA_TEMPLATE    = {\"pmid\":\"\",\"decision\":\"\",\"reason\":\"\",\"confidence\":0.0,\"population_quote\":\"\",\"intervention_quote\":\"\"}\n",
    "\n",
    "STRICT_JSON_RULES = (\n",
    "  \"Return ONLY one JSON object. No analysis, no preface, no notes. \"\n",
    "  \"Wrap it EXACTLY with:\\nBEGIN_JSON\\n{...}\\nEND_JSON\"\n",
    ")\n",
    "\n",
    "def ask_json_strict(model: str, system: str, user: str, template: dict, max_tokens=8000):\n",
    "    user_strict = f\"{user}\\n\\n{STRICT_JSON_RULES}\"\n",
    "    raw = lm_chat(model, system, user_strict, temperature=0.0, max_tokens=max_tokens, stop=[\"END_JSON\"])\n",
    "    try:\n",
    "        return json.loads(extract_json_block_or_fence(raw))\n",
    "    except Exception:\n",
    "        repaired = lm_chat(\n",
    "            model,\n",
    "            REPAIR_SYSTEM,\n",
    "            repair_user(json.dumps(template, ensure_ascii=False, indent=2), raw) + \"\\n\\n\" + STRICT_JSON_RULES,\n",
    "            temperature=0.0,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=[\"END_JSON\"]\n",
    "        )\n",
    "        return json.loads(extract_json_block_or_fence(repaired))\n",
    "\n",
    "def ask_json(model: str, system: str, user: str, template: dict, max_tokens=8000):\n",
    "    raw = lm_chat(model, system, user, temperature=0.0, max_tokens=max_tokens)\n",
    "    try:\n",
    "        return json.loads(extract_json_block_or_fence(raw))\n",
    "    except Exception:\n",
    "        repaired = lm_chat(\n",
    "            model,\n",
    "            REPAIR_SYSTEM,\n",
    "            repair_user(json.dumps(template, ensure_ascii=False, indent=2), raw),\n",
    "            temperature=0.0,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return json.loads(extract_json_block_or_fence(repaired))\n",
    "\n",
    "ask_json = ask_json_strict\n",
    "\n",
    "# ----------------------------\n",
    "# PubMed E-utilities\n",
    "# ----------------------------\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "HEADERS = {\"User-Agent\": \"sniff-poc/0.2 (+local)\", \"Accept\": \"application/json\"}\n",
    "\n",
    "def esearch_count_and_ids(term: str, mindate: int|None):\n",
    "    p = {\"db\":\"pubmed\",\"retmode\":\"json\",\"term\":term,\"retmax\":5000,\"email\":ENTREZ_EMAIL,\"usehistory\":\"y\"}\n",
    "    if ENTREZ_API_KEY: p[\"api_key\"]=ENTREZ_API_KEY\n",
    "    if mindate: p[\"mindate\"]=str(mindate)\n",
    "    r = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params=p, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    js = r.json().get(\"esearchresult\", {})\n",
    "    count = int(js.get(\"count\",\"0\"))\n",
    "    webenv = js.get(\"webenv\"); qk = js.get(\"querykey\")\n",
    "    if not count or not webenv or not qk:\n",
    "        return 0, []\n",
    "    r2 = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params={\n",
    "        \"db\":\"pubmed\",\"retmode\":\"json\",\"retmax\":5000,\"retstart\":0,\"email\":ENTREZ_EMAIL,\n",
    "        \"WebEnv\":webenv,\"query_key\":qk, **({\"api_key\":ENTREZ_API_KEY} if ENTREZ_API_KEY else {})\n",
    "    }, timeout=HTTP_TIMEOUT)\n",
    "    r2.raise_for_status()\n",
    "    ids = r2.json().get(\"esearchresult\",{}).get(\"idlist\",[])\n",
    "    return count, [str(x) for x in ids]\n",
    "\n",
    "def efetch_xml(pmids):\n",
    "    if not pmids: return \"\"\n",
    "    params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"id\":\",\".join(pmids),\"email\":ENTREZ_EMAIL}\n",
    "    if ENTREZ_API_KEY: params[\"api_key\"]=ENTREZ_API_KEY\n",
    "    r = requests.get(f\"{EUTILS}/efetch.fcgi\", headers={\"User-Agent\": \"sniff-poc/0.2\"}, params=params, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_pubmed_xml(xml_text: str):\n",
    "    out = []\n",
    "    if not xml_text.strip(): return out\n",
    "    root = ET.fromstring(xml_text)\n",
    "    def _join(node):\n",
    "        if node is None: return \"\"\n",
    "        try: return \"\".join(node.itertext())\n",
    "        except Exception: return node.text or \"\"\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = art.findtext(\".//PMID\") or \"\"\n",
    "        title = _join(art.find(\".//ArticleTitle\")).strip()\n",
    "        abs_nodes = art.findall(\".//Abstract/AbstractText\")\n",
    "        abstract = \" \".join(_join(n).strip() for n in abs_nodes) if abs_nodes else \"\"\n",
    "        year = None\n",
    "        for path in (\".//ArticleDate/Year\",\".//PubDate/Year\",\".//DateCreated/Year\",\".//PubDate/MedlineDate\"):\n",
    "            s = art.findtext(path)\n",
    "            if s:\n",
    "                m = re.search(r\"\\d{4}\", s)\n",
    "                if m: year = int(m.group(0)); break\n",
    "        lang = art.findtext(\".//Language\") or None\n",
    "        pubtypes = [pt.text for pt in art.findall(\".//PublicationTypeList/PublicationType\") if pt.text]\n",
    "        mesh = [mh.findtext(\"./DescriptorName\") for mh in art.findall(\".//MeshHeadingList/MeshHeading\") if mh.findtext(\"./DescriptorName\")]\n",
    "        out.append({\n",
    "            \"pmid\": pmid, \"title\": title, \"abstract\": abstract, \"year\": year, \"language\": lang,\n",
    "            \"publication_types\": pubtypes, \"mesh\": mesh\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# LLM prompts (semantic steps)\n",
    "# ----------------------------\n",
    "TERMS_SYSTEM = \"You extract controlled, compact term lists for biomedical retrieval. Return strict JSON only.\"\n",
    "def terms_user(nlq: str):\n",
    "    return f\"\"\"From the natural-language question below, produce compact term lists.\n",
    "\n",
    "NATURAL_LANGUAGE_QUESTION:\n",
    "<<<\n",
    "{nlq}\n",
    ">>>\n",
    "\n",
    "Rules:\n",
    "- Return JSON with arrays of P/I/C/O strings: {{ \"population\":[], \"intervention\":[], \"comparators\":[], \"outcomes\":[] }}\n",
    "- Strings must be concise phrases (no boolean, no field tags, no quotes/brackets).\n",
    "- Include common synonyms and acronyms (e.g., Nuss, MIRPE, cryoanalgesia).\n",
    "- Add 2–5 must_have tokens in \"must_have\" that anchor topicality (e.g., MIRPE, Nuss, cryoablation).\n",
    "- Add 2–5 avoid tokens in \"avoid\" if obvious confounders (e.g., pediatric oncology if off-topic).\n",
    "- Keep each list ≤ 12 items.\n",
    "\n",
    "Return ONLY:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{...}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "MESH_TAG_SYSTEM = \"You classify MeSH descriptors into roles relative to a PICOS.\"\n",
    "def mesh_tag_user(p_terms, i_terms, descriptors):\n",
    "    return f\"\"\"Classify each MeSH descriptor as one of: P (population/procedure context), I (intervention/analgesia), O (outcome), C (comparator/technique), G (generic context), X (irrelevant).\n",
    "Also provide a 'keep' boolean (true if useful for building search), and a 1-line rationale.\n",
    "\n",
    "P_TERMS = {p_terms}\n",
    "I_TERMS = {i_terms}\n",
    "\n",
    "DESCRIPTORS = {descriptors}\n",
    "\n",
    "Return ONLY:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{ \"labels\": [{{\"mesh\":\"...\", \"role\":\"P|I|O|C|G|X\", \"keep\": true|false, \"why\": \"...\"}}] }}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "# -------- Protocol brief builder (auto) --------\n",
    "def build_protocol_brief(nlq: str, terms, year_min: int, languages: list[str]):\n",
    "    langs_norm = [LANG_NAME_TO_TAG.get(x.lower(), x.lower()) for x in languages if x]\n",
    "    langs_norm = [x for x in langs_norm if x in LANG_NAME_TO_TAG.values()]\n",
    "    brief = {\n",
    "        \"focus\": \"Adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\",\n",
    "        \"population_include\": terms.get(\"population\", []),\n",
    "        \"intervention_index\": terms.get(\"intervention\", []),\n",
    "        \"comparators_eligible\": terms.get(\"comparators\", []),\n",
    "        \"primary_outcomes\": terms.get(\"outcomes\", []),\n",
    "        \"designs\": \"RCTs preferred; include comparative cohort/case-control if RCTs absent.\",\n",
    "        \"time_window_min_year\": year_min,\n",
    "        \"languages\": langs_norm or [\"english\",\"portuguese\",\"spanish\"],\n",
    "        \"must_have_anchors\": terms.get(\"must_have\", []),\n",
    "        \"hard_excludes\": [\"guidelines\",\"editorials\",\"technical notes without outcomes\"]\n",
    "    }\n",
    "    # textual compact version Gemma sees\n",
    "    txt = (\n",
    "        f\"FOCUS: adults with pectus excavatum undergoing MIRPE/Nuss.\\n\"\n",
    "        f\"POP (include): {', '.join(brief['population_include'])}\\n\"\n",
    "        f\"INDEX INTERVENTION: {', '.join(brief['intervention_index'])}\\n\"\n",
    "        f\"COMPARATORS: {', '.join(brief['comparators_eligible'])}\\n\"\n",
    "        f\"OUTCOMES (primary): {', '.join(brief['primary_outcomes'])}\\n\"\n",
    "        f\"DESIGNS: {brief['designs']}\\n\"\n",
    "        f\"YEAR_MIN: {year_min}; LANGUAGES: {', '.join(brief['languages'])}\\n\"\n",
    "        f\"ANCHORS (must have): {', '.join(brief['must_have_anchors']) or 'MIRPE, Nuss, cryoablation'}\\n\"\n",
    "        f\"HARD EXCLUDES: {', '.join(brief['hard_excludes'])}\"\n",
    "    )\n",
    "    return brief, txt\n",
    "\n",
    "PASSA_SYS = \"You are a strict PRISMA title/abstract screener for effects triage. Return JSON only.\"\n",
    "def passa_user(protocol_txt: str, record, enforce_adult=True):\n",
    "    return f\"\"\"Systematic-review protocol (brief):\n",
    "{protocol_txt}\n",
    "\n",
    "Rules for this triage:\n",
    "- Include only if population aligns with MIRPE/Nuss for pectus excavatum (prefer adults; pediatric-only is out unless adults are clearly included/analyzed).\n",
    "- The index intervention is intercostal nerve cryoablation (INC) / cryoanalgesia used intraoperatively for MIRPE/Nuss analgesia.\n",
    "- Eligible comparators: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia.\n",
    "- Outcomes required for inclusion signal: postoperative opioid consumption (in-hospital or discharge) and/or pain scores within 0–7 days.\n",
    "- Designs: RCTs preferred; if no RCTs, allow comparative cohort/case-control. Exclude guidelines, letters, editorials, technical notes lacking outcomes.\n",
    "- If the abstract lacks clear nexus of BOTH (MIRPE|Nuss|pectus excavatum) AND (cryoablation|cryoanalgesia|INC) -> mark borderline or exclude.\n",
    "\n",
    "Record:\n",
    "PMID: {record['pmid']}\n",
    "Title: {record['title']}\n",
    "Abstract: {record['abstract']}\n",
    "PubTypes: {record['publication_types']}\n",
    "Year: {record['year']}\n",
    "Lang: {record['language']}\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{record['pmid']}\",\n",
    "  \"decision\":\"include|borderline|exclude\",\n",
    "  \"reason\":\"population_mismatch|intervention_mismatch|design_ineligible|off_topic|language|year|insufficient_info\",\n",
    "  \"confidence\": 0.0,\n",
    "  \"population_quote\":\"\", \"intervention_quote\":\"\"\n",
    "}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Deterministic query builders\n",
    "# ----------------------------\n",
    "def or_block(terms, field=\"tiab\"):\n",
    "    toks=[]\n",
    "    for t in terms:\n",
    "        if not t: continue\n",
    "        t=t.strip()\n",
    "        if not t: continue\n",
    "        if \" \" in t or \"-\" in t or \"(\" in t:\n",
    "            toks.append(f\"\\\"{t}\\\"[{field}]\")\n",
    "        else:\n",
    "            toks.append(f\"{t}[{field}]\")\n",
    "    if not toks: return \"\"\n",
    "    return \"(\" + \" OR \".join(toks) + \")\"\n",
    "\n",
    "def lang_filter_block(langs):\n",
    "    tags = [LANG_NAME_TO_TAG.get(x.lower(), x.lower()) for x in langs if x]\n",
    "    tags = [t for t in tags if t in LANG_NAME_TO_TAG.values()]\n",
    "    if not tags: return \"\"\n",
    "    return \"(\" + \" OR \".join(f\"\\\"{t}\\\"[lang]\" for t in tags) + \")\"\n",
    "\n",
    "def pubtype_block_pt():\n",
    "    return \"(\" + \" OR \".join(f\"\\\"{pt}\\\"[pt]\" for pt in PTYPES_TRIAL_COMP) + \")\"\n",
    "\n",
    "def build_broad(p_syn, i_syn, extra=None, field=\"tiab\"):\n",
    "    P = or_block(p_syn, field)\n",
    "    I = or_block(i_syn, field)\n",
    "    X = (\" AND \" + or_block(extra, field)) if extra else \"\"\n",
    "    if not P or not I: return None\n",
    "    return f\"{P} AND {I}{X}\"\n",
    "\n",
    "def build_with_comparator(p_syn, i_syn, comps, anchors=None, field=\"tiab\"):\n",
    "    base = build_broad(p_syn, i_syn, field=field)\n",
    "    C = or_block(comps, field)\n",
    "    A = or_block(anchors, field) if anchors else \"\"\n",
    "    if not base: return None\n",
    "    out = base\n",
    "    if C: out += \" AND \" + C\n",
    "    if A: out += \" AND \" + A\n",
    "    return out\n",
    "\n",
    "def add_lang_filter(q, langs):\n",
    "    L = lang_filter_block(langs)\n",
    "    return f\"{q} AND {L}\" if L else q\n",
    "\n",
    "def build_focused_lex(broad_core):\n",
    "    return f\"({broad_core}) AND {RCT_HEDGE_LEX}\"\n",
    "\n",
    "def build_focused_ptype(broad_core):\n",
    "    return f\"({broad_core}) AND {pubtype_block_pt()}\"\n",
    "\n",
    "# ----------------------------\n",
    "# Metrics & gates\n",
    "# ----------------------------\n",
    "def lexical_stats(records, p_terms, i_terms, outcomes):\n",
    "    def hits(text, terms):\n",
    "        tl=(text or \"\").lower()\n",
    "        return sum(1 for t in terms if t and t.lower() in tl)\n",
    "    n = min(SAMPLE_N, len(records))\n",
    "    sample = records[:n]\n",
    "    pi_rate=0; out_rate=0; design_rate=0; scores=[]\n",
    "    years=[]; langs=[]; ptypes=[]; mesh=Counter()\n",
    "    for r in sample:\n",
    "        t = (r['title'] or \"\") + \"\\n\" + (r['abstract'] or \"\")\n",
    "        pi = (hits(t, p_terms)+hits(t, i_terms))>0\n",
    "        po = hits(t, outcomes)>0\n",
    "        de = len(set(r['publication_types']) & PRIMARY_HINTS)>0\n",
    "        pi_rate += 1 if pi else 0\n",
    "        out_rate+= 1 if po else 0\n",
    "        design_rate+= 1 if de else 0\n",
    "        s = (2.0*(1 if pi else 0) + 1.0*(1 if po else 0) + 0.5*(1 if de else 0))\n",
    "        scores.append(s)\n",
    "        if r[\"year\"]: years.append(r[\"year\"])\n",
    "        if r[\"language\"]: langs.append(r[\"language\"])\n",
    "        ptypes.extend(r[\"publication_types\"])\n",
    "        mesh.update(r.get(\"mesh\") or [])\n",
    "    if n==0:\n",
    "        return {\"n_sample\":0,\"pi_rate\":0,\"outcome_rate\":0,\"design_rate\":0,\"median_score\":0,\"mean_score\":0,\n",
    "                \"years\":{},\"languages\":{},\"ptypes\":{},\"top_mesh\":[]}\n",
    "    scores.sort()\n",
    "    med = scores[n//2]\n",
    "    mean = sum(scores)/n\n",
    "    summary = {\n",
    "        \"n_sample\": n,\n",
    "        \"pi_rate\": round(pi_rate/n,3),\n",
    "        \"outcome_rate\": round(out_rate/n,3),\n",
    "        \"design_rate\": round(design_rate/n,3),\n",
    "        \"median_score\": round(med,3),\n",
    "        \"mean_score\": round(mean,3),\n",
    "        \"years\": {\"median\": (statistics.median(years) if years else None), \"min\": (min(years) if years else None), \"max\": (max(years) if years else None)},\n",
    "        \"languages\": dict(Counter(langs).most_common(5)),\n",
    "        \"ptypes\": dict(Counter(ptypes).most_common(8)),\n",
    "        \"top_mesh\": [m for m,_ in mesh.most_common(10)]\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "def rq_quality(stats):\n",
    "    return round( stats[\"median_score\"] + 0.25*stats[\"mean_score\"] + 0.5*stats[\"pi_rate\"] + 0.25*stats[\"outcome_rate\"], 3)\n",
    "\n",
    "# Gemma gating: require anchors; for focused, require trial/comparator cue\n",
    "ANCHOR_P  = re.compile(r\"\\b(nuss|mirpe|pectus\\s+excavatum)\\b\", re.I)\n",
    "ANCHOR_I  = re.compile(r\"\\b(cryoablation|cryoanalgesia|\\binc\\b)\\b\", re.I)\n",
    "TRIAL_COMPARATOR_CUE = re.compile(r\"\\b(randomi[sz]ed?|trial|epidural|paravertebral|intercostal nerve block|erector spinae|comparative|cohort|case[- ]control)\\b\", re.I)\n",
    "\n",
    "def gemma_gate(record_text: str, is_focused: bool):\n",
    "    ok_anchor = bool(ANCHOR_P.search(record_text) and ANCHOR_I.search(record_text))\n",
    "    if not ok_anchor:\n",
    "        return False\n",
    "    if is_focused:\n",
    "        return bool(TRIAL_COMPARATOR_CUE.search(record_text))\n",
    "    return True\n",
    "\n",
    "# ----------------------------\n",
    "# Main SNiff runner\n",
    "# ----------------------------\n",
    "def sniff_nlq(USER_NLQ: str, year_min: int = YEAR_MIN_DEFAULT, languages: list[str] = [\"English\",\"Portuguese\",\"Spanish\"]):\n",
    "    # 1) Qwen: extract seed term lists\n",
    "    terms_js = ask_json(QWEN_MODEL, TERMS_SYSTEM, terms_user(USER_NLQ), TERMS_TEMPLATE, max_tokens=8000)\n",
    "    (OUT_DIR/\"seed_terms.json\").write_text(json.dumps(terms_js, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    P0 = terms_js.get(\"population\", []) or []\n",
    "    I0 = terms_js.get(\"intervention\", []) or []\n",
    "    C0 = terms_js.get(\"comparators\", []) or []\n",
    "    O0 = terms_js.get(\"outcomes\", []) or []\n",
    "    MUST = terms_js.get(\"must_have\", []) or []\n",
    "\n",
    "    # 2) Seed probe (B0) to mine MeSH\n",
    "    def try_query(name, q):\n",
    "        cnt, ids = esearch_count_and_ids(q, year_min)\n",
    "        xml = efetch_xml(ids[:SAMPLE_N])\n",
    "        recs = parse_pubmed_xml(xml)\n",
    "        stats = lexical_stats(recs, P0, I0, O0)\n",
    "        rq = rq_quality(stats)\n",
    "        return {\"name\":name,\"query\":q,\"total\":cnt,\"stats\":stats,\"rq\":rq,\"ids\":ids,\"sample\":recs}\n",
    "\n",
    "    anchors_for_P = [x for x in MUST if x.lower() in (\"mirpe\",\"nuss\",\"pectus excavatum\",\"cryoablation\")]\n",
    "    if not anchors_for_P:\n",
    "        anchors_for_P = [\"MIRPE\",\"Nuss\",\"cryoablation\"]\n",
    "\n",
    "    q_b0 = build_broad(P0, I0, extra=anchors_for_P)\n",
    "    print(f\"{time.strftime('%H:%M:%S')}  B0_seed probing...\")\n",
    "    B0 = try_query(\"B0_seed\", q_b0)\n",
    "\n",
    "    # 3) MeSH mining and role tagging\n",
    "    mesh_all = Counter()\n",
    "    for r in B0[\"sample\"]:\n",
    "        for m in r.get(\"mesh\", []) or []:\n",
    "            mesh_all[m] += 1\n",
    "    top_mesh = [m for m,_ in mesh_all.most_common(40)]\n",
    "    (OUT_DIR/\"mesh_raw.json\").write_text(json.dumps({\"top_mesh\":top_mesh, \"counts\":mesh_all.most_common(100)}, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    mesh_tag_js = ask_json(QWEN_MODEL, MESH_TAG_SYSTEM, mesh_tag_user(P0, I0, top_mesh), MESH_TAG_TEMPLATE, max_tokens=8000)\n",
    "    (OUT_DIR/\"mesh_tagged.json\").write_text(json.dumps(mesh_tag_js, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    keepP = [x[\"mesh\"] for x in mesh_tag_js.get(\"labels\", []) if x.get(\"keep\") and x.get(\"role\")==\"P\"]\n",
    "    keepI = [x[\"mesh\"] for x in mesh_tag_js.get(\"labels\", []) if x.get(\"keep\") and x.get(\"role\")==\"I\"]\n",
    "\n",
    "    def expand_terms(base, extra, limit=12):\n",
    "        seen=set([b.lower() for b in base])\n",
    "        out=list(base)\n",
    "        for e in extra:\n",
    "            w=e.strip()\n",
    "            if not w: continue\n",
    "            if w.lower() not in seen:\n",
    "                out.append(w)\n",
    "                seen.add(w.lower())\n",
    "        return out[:limit]\n",
    "\n",
    "    P_core = expand_terms(P0, keepP[:6])\n",
    "    I_core = expand_terms(I0, keepI[:8])\n",
    "\n",
    "    # 4) Protocol brief (shown & used by Gemma)\n",
    "    protocol, protocol_txt = build_protocol_brief(USER_NLQ, terms_js, year_min, languages)\n",
    "\n",
    "    # 5) Build candidates\n",
    "    candidates = []\n",
    "    def add_variant(name, q):\n",
    "        if not q: return\n",
    "        res = try_query(name, q)\n",
    "        print(f\"{time.strftime('%H:%M:%S')}  {name} hits={res['total']} rq={res['rq']} stats={res['stats']}\")\n",
    "        candidates.append(res)\n",
    "\n",
    "    add_variant(\"B1_broad\", build_broad(P_core, I_core, extra=anchors_for_P))\n",
    "    add_variant(\"B2_I_tight\", build_broad(P0, I_core, extra=anchors_for_P))\n",
    "    add_variant(\"B3_P_core\", build_broad(P_core, I0, extra=anchors_for_P))\n",
    "    add_variant(\"B4_with_comparator\", build_with_comparator(P_core, I_core, C0, anchors=anchors_for_P))\n",
    "    # Language filtered variant on the best of the above later (to avoid redundant E-queries)\n",
    "\n",
    "    # Choose BROAD by window then RQ\n",
    "    def choose_broad():\n",
    "        pool = [c for c in candidates if c[\"name\"].startswith(\"B\")]\n",
    "        in_target = [c for c in pool if BROAD_TARGET[0] <= c[\"total\"] <= BROAD_TARGET[1]]\n",
    "        if in_target:\n",
    "            return max(in_target, key=lambda x: x[\"rq\"])\n",
    "        pool2 = [c for c in pool if BROAD_OK[0] <= c[\"total\"] <= BROAD_OK[1]]\n",
    "        if pool2:\n",
    "            return max(pool2, key=lambda x: x[\"rq\"])\n",
    "        return max(pool, key=lambda x: x[\"rq\"]) if pool else B0\n",
    "\n",
    "    chosen_broad = choose_broad()\n",
    "\n",
    "    # Add explicit language filter variant on chosen broad (if not already present)\n",
    "    lang_filtered_query = add_lang_filter(chosen_broad[\"query\"], protocol[\"languages\"])\n",
    "    B_lang = try_query(\"B_lang_filtered\", lang_filtered_query)\n",
    "    candidates.append(B_lang)\n",
    "    print(f\"{time.strftime('%H:%M:%S')}  B_lang_filtered hits={B_lang['total']} rq={B_lang['rq']} stats={B_lang['stats']}\")\n",
    "\n",
    "    # Re-choose between chosen_broad and language-filtered if both valid\n",
    "    if BROAD_OK[0] <= B_lang[\"total\"] <= BROAD_OK[1] and B_lang[\"rq\"] >= chosen_broad[\"rq\"]*0.98:\n",
    "        chosen_broad = B_lang\n",
    "\n",
    "    # 6) Focused queries (lexical hedge + ptype)\n",
    "    qF_lex = build_focused_lex(chosen_broad[\"query\"])\n",
    "    F_lex = try_query(\"F_lexical\", qF_lex)\n",
    "    qF_pt  = build_focused_ptype(chosen_broad[\"query\"])\n",
    "    F_pt   = try_query(\"F_ptype\", qF_pt)\n",
    "\n",
    "    focused_cands = [F_lex, F_pt]\n",
    "    def choose_focused():\n",
    "        pool = [c for c in focused_cands]\n",
    "        in_target = [c for c in pool if FOCUSED_TARGET[0] <= c[\"total\"] <= FOCUSED_TARGET[1]]\n",
    "        if in_target:\n",
    "            return max(in_target, key=lambda x: x[\"rq\"])\n",
    "        pool2 = [c for c in pool if FOCUSED_OK[0] <= c[\"total\"] <= FOCUSED_OK[1]]\n",
    "        if pool2:\n",
    "            return max(pool2, key=lambda x: x[\"rq\"])\n",
    "        return max(pool, key=lambda x: x[\"rq\"])\n",
    "    chosen_focused = choose_focused()\n",
    "\n",
    "    # 7) Gemma: protocol-based sanity screen with gates\n",
    "    def sanity_screen(records, is_focused=False):\n",
    "        if not records: return {\"include\":0,\"borderline\":0,\"exclude\":0,\"n\":0}\n",
    "        inc=bor=exc=0\n",
    "        examples=[]\n",
    "        for r in records[:min(SAMPLE_N,len(records))]:\n",
    "            js = ask_json(GEMMA_MODEL, PASSA_SYS, passa_user(protocol_txt, r), PASSA_TEMPLATE, max_tokens=8000)\n",
    "            decision = (js.get(\"decision\",\"\") or \"\").lower()\n",
    "            # Gates\n",
    "            rt = ((r['title'] or \"\") + \" \" + (r['abstract'] or \"\"))\n",
    "            if not gemma_gate(rt, is_focused=is_focused):\n",
    "                if decision == \"include\":\n",
    "                    decision = \"borderline\" if not is_focused else \"exclude\"\n",
    "            if decision==\"include\": inc+=1\n",
    "            elif decision==\"borderline\": bor+=1\n",
    "            else: exc+=1\n",
    "            examples.append({\"pmid\":r[\"pmid\"],\"title\":r[\"title\"][:140],\n",
    "                             \"flags\":\" \".join([\n",
    "                                 \"PI\" if any(t.lower() in rt.lower() for t in (P0+I0)) else \"\",\n",
    "                                 \"OUT\" if any(t.lower() in rt.lower() for t in O0) else \"\",\n",
    "                                 \"DES\" if (set(r[\"publication_types\"]) & PRIMARY_HINTS) else \"\"\n",
    "                             ]).strip()})\n",
    "        n=inc+bor+exc\n",
    "        return {\"include\":inc,\"borderline\":bor,\"exclude\":exc,\"n\":n,\"examples\":examples}\n",
    "\n",
    "    sample_broad = chosen_broad[\"sample\"]\n",
    "    sample_focused = chosen_focused[\"sample\"]\n",
    "    sanity_broad = sanity_screen(sample_broad, is_focused=False)\n",
    "    sanity_focused = sanity_screen(sample_focused, is_focused=True)\n",
    "\n",
    "    # 8) Persist artifacts\n",
    "    artifacts = {\n",
    "        \"nlq\": USER_NLQ,\n",
    "        \"year_min\": year_min,\n",
    "        \"languages\": protocol[\"languages\"],\n",
    "        \"seed_terms\": terms_js,\n",
    "        \"protocol_brief\": protocol,\n",
    "        \"mesh_top\": top_mesh,\n",
    "        \"mesh_tagged\": mesh_tag_js,\n",
    "        \"candidates\": [\n",
    "            {k:(v if k!=\"sample\" else None) for k,v in c.items()} for c in [B0]+candidates+[F_lex,F_pt]\n",
    "        ],\n",
    "        \"chosen\": {\n",
    "            \"broad\": {\"name\": chosen_broad[\"name\"], \"query\": chosen_broad[\"query\"], \"total\": chosen_broad[\"total\"], \"rq\": chosen_broad[\"rq\"], \"stats\": chosen_broad[\"stats\"], \"sanity\": sanity_broad},\n",
    "            \"focused\": {\"name\": chosen_focused[\"name\"], \"query\": chosen_focused[\"query\"], \"total\": chosen_focused[\"total\"], \"rq\": chosen_focused[\"rq\"], \"stats\": chosen_focused[\"stats\"], \"sanity\": sanity_focused},\n",
    "        }\n",
    "    }\n",
    "    (OUT_DIR/\"broad.txt\").write_text(chosen_broad[\"query\"], encoding=\"utf-8\")\n",
    "    (OUT_DIR/\"focused.txt\").write_text(chosen_focused[\"query\"], encoding=\"utf-8\")\n",
    "    (OUT_DIR/\"sniff_artifacts.json\").write_text(json.dumps(artifacts, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # 9) Human-readable report (concise)\n",
    "    def fmt_stats(s):\n",
    "        return f\"PI={s['pi_rate']:.2f} OUT={s['outcome_rate']:.2f} DES={s['design_rate']:.2f}  RQ={rq_quality(s):.3f}\"\n",
    "\n",
    "    def top_titles(sample):\n",
    "        out=[]\n",
    "        for r in sample[:min(SAMPLE_N,len(sample))]:\n",
    "            flags=[]\n",
    "            tt=(r['title'] or \"\")\n",
    "            txt=(r['title'] or \"\")+\" \"+(r['abstract'] or \"\")\n",
    "            if any(t.lower() in txt.lower() for t in (P0+I0)): flags.append(\"PI\")\n",
    "            if any(t.lower() in txt.lower() for t in O0): flags.append(\"OUT\")\n",
    "            if set(r[\"publication_types\"]) & PRIMARY_HINTS: flags.append(\"DES\")\n",
    "            out.append(f\"   • [{r['pmid']}] ({r['year']})  {tt[:90]}{'...' if len(tt)>90 else ''}  | {' '.join(flags)}\")\n",
    "        return \"\\n\".join(out) if out else \"   (no sample)\"\n",
    "\n",
    "    # Candidate reasoning labels\n",
    "    lines=[]\n",
    "    lines.append(\"\\n==================== SNIFF REPORT ====================\")\n",
    "    lines.append(\"NLQ:\")\n",
    "    nlq_line = (USER_NLQ or \"\").strip().replace(\"\\n\",\" \")\n",
    "    lines.append(\"   \" + (nlq_line[:160] + (\"...\" if len(nlq_line)>160 else \"\")))\n",
    "    lines.append(\"\\nSEED TERMS (Qwen):\")\n",
    "    lines.append(f\"  P: {', '.join(P0)}\")\n",
    "    lines.append(f\"  I: {', '.join(I0)}\")\n",
    "    lines.append(f\"  C: {', '.join(C0)}\")\n",
    "    lines.append(f\"  O: {', '.join(O0)}\")\n",
    "    lines.append(f\"  must_have: {', '.join(MUST) if MUST else '(none)'}\")\n",
    "\n",
    "    # Distilled tokens used (show what actually hit queries)\n",
    "    lines.append(\"\\nDISTILLED FOR TIAB (anchors enforced):\")\n",
    "    lines.append(f\"  Pq: {', '.join(P_core) if P_core else '(none)'}\")\n",
    "    lines.append(f\"  Iq: {', '.join(I_core) if I_core else '(none)'}\")\n",
    "    lines.append(f\"  anchors: {', '.join(anchors_for_P)}\")\n",
    "\n",
    "    # Candidate set summary\n",
    "    lines.append(\"\\nCANDIDATES:\")\n",
    "    all_cands = [B0]+candidates\n",
    "    for c in all_cands:\n",
    "        reason = []\n",
    "        if c is chosen_broad:\n",
    "            reason.append(\"chosen_broad\")\n",
    "        elif c[\"name\"].startswith(\"B\"):\n",
    "            if not (BROAD_OK[0] <= c[\"total\"] <= BROAD_OK[1]): reason.append(\"count_out_of_window\")\n",
    "            if c is not chosen_broad: reason.append(\"lower_rq\")\n",
    "        else:\n",
    "            reason.append(\"-\")\n",
    "        lines.append(f\"  {c['name']}: hits={c['total']} {fmt_stats(c['stats'])}  reason={','.join(reason)}\")\n",
    "\n",
    "    # Chosen BROAD\n",
    "    lines.append(\"\\nCHOSEN BROAD\")\n",
    "    lines.append(f\"  Query: {chosen_broad['query']}\")\n",
    "    lines.append(f\"  Hits={chosen_broad['total']}  in_window={BROAD_OK[0] <= chosen_broad['total'] <= BROAD_OK[1]}  target_window={BROAD_TARGET[0] <= chosen_broad['total'] <= BROAD_TARGET[1]}\")\n",
    "    lines.append(f\"  Signals: {fmt_stats(chosen_broad['stats'])}\")\n",
    "    lines.append(f\"  Quick triage (Gemma): include={sanity_broad['include']} borderline={sanity_broad['borderline']} exclude={sanity_broad['exclude']} (n={sanity_broad['n']})\")\n",
    "    lines.append(\"  Top titles:\")\n",
    "    lines.append(top_titles(chosen_broad[\"sample\"]))\n",
    "\n",
    "    # Chosen FOCUSED\n",
    "    lines.append(\"\\nFOCUSED (BROAD ∧ trial filters)\")\n",
    "    lines.append(f\"  Query: {chosen_focused['query']}\")\n",
    "    lines.append(f\"  Hits={chosen_focused['total']}  in_window={FOCUSED_OK[0] <= chosen_focused['total'] <= FOCUSED_OK[1]}  target_window={FOCUSED_TARGET[0] <= chosen_focused['total'] <= FOCUSED_TARGET[1]}\")\n",
    "    lines.append(f\"  Signals: {fmt_stats(chosen_focused['stats'])}\")\n",
    "    lines.append(f\"  Quick triage (Gemma): include={sanity_focused['include']} borderline={sanity_focused['borderline']} exclude={sanity_focused['exclude']} (n={sanity_focused['n']})\")\n",
    "    lines.append(\"  Top titles:\")\n",
    "    lines.append(top_titles(chosen_focused[\"sample\"]))\n",
    "\n",
    "    # MeSH mined highlights\n",
    "    kept_tokens = list(dict.fromkeys(P_core + I_core))[:10]\n",
    "    lines.append(\"\\nMESH mined (top kept tokens):\")\n",
    "    lines.append(\"  kept  : \" + \", \".join(kept_tokens))\n",
    "    # extras\n",
    "    y = chosen_broad[\"stats\"][\"years\"]; langs = chosen_broad[\"stats\"][\"languages\"]; pts = chosen_broad[\"stats\"][\"ptypes\"]\n",
    "    lines.append(\"\\nSAMPLE DISTRIBUTIONS (broad sample):\")\n",
    "    lines.append(f\"  years: median={y.get('median')} range=({y.get('min')},{y.get('max')})\")\n",
    "    lines.append(f\"  languages: {', '.join(f'{k}:{v}' for k,v in langs.items()) or '(none)'}\")\n",
    "    lines.append(f\"  pub types: {', '.join(f'{k}:{v}' for k,v in pts.items()) or '(none)'}\")\n",
    "\n",
    "    lines.append(\"\\nProtocol brief (used for Gemma triage):\")\n",
    "    lines.append(\"  \" + protocol_txt.replace(\"\\n\", \"\\n  \"))\n",
    "\n",
    "    lines.append(\"\\n================== END OF REPORT =====================\")\n",
    "    report = \"\\n\".join(lines)\n",
    "    print(report)\n",
    "\n",
    "    (OUT_DIR/\"sniff_report.json\").write_text(json.dumps({\n",
    "        \"report_text\": report,\n",
    "        \"chosen_broad\": {\"query\": chosen_broad[\"query\"], \"hits\": chosen_broad[\"total\"]},\n",
    "        \"chosen_focused\": {\"query\": chosen_focused[\"query\"], \"hits\": chosen_focused[\"total\"]},\n",
    "    }, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "# ----------------------------\n",
    "# RUN: put your NLQ here\n",
    "# ----------------------------\n",
    "USER_NLQ = \"\"\"Population = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE (the intervention of interest is INC, not the surgery). Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia. Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days. Study designs = RCTs preferred; if RCTs absent, include comparative cohort/case-control. Year_min = 2015. Languages = English, Portuguese, Spanish.\"\"\"\n",
    "sniff_nlq(USER_NLQ, year_min=YEAR_MIN_DEFAULT, languages=[\"English\",\"Portuguese\",\"Spanish\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a928765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:15:49  [S1] Protocol lockdown...\n",
      "16:17:01  [S2] Universe definition & sizing...\n",
      "16:17:02   [Universe] try=0 count=7 window=(50, 10000) query=((adults[tiab] OR \"minimally invasive repair of pectus excavatum\"[tiab] OR Nuss[tiab])) AND (\"intercostal nerve cryoablation\"[tiab]) AND ((adult[tiab] OR adults[tiab]))\n",
      "16:17:21   [Universe] try=1 count=18 window=(50, 10000) query=((adults[tiab] OR \"minimally invasive repair of pectus excavatum\"[tiab] OR Nuss[tiab] OR \"chest wall surgery\"[tiab])) AND ((\"intercostal nerve cryoablation\"[tiab] OR \"nerve ablation\"[tiab])) AND ((adult[tiab] OR adults[tiab]))\n",
      "16:17:35   [Universe] try=2 count=21 window=(50, 10000) query=((adults[tiab] OR \"minimally invasive repair of pectus excavatum\"[tiab] OR Nuss[tiab] OR \"chest wall surgery\"[tiab] OR \"thoracic surgery\"[tiab])) AND ((\"intercostal nerve cryoablation\"[tiab] OR \"nerve ablation\"[tiab] OR neuroablation[tiab])) AND ((adult[tiab] OR adults[tiab]))\n",
      "16:17:55  [S3] Ground-truth discovery & protocol validation...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: http://127.0.0.1:1234/v1/chat/completions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 808\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# RUN\u001b[39;00m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    807\u001b[39m USER_NLQ = \u001b[33m\"\"\"\u001b[39m\u001b[33mPopulation = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE (the intervention of interest is INC, not the surgery). Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia. Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days. Study designs = RCTs preferred; if RCTs absent, include comparative cohort/case-control. Year_min = 2015. Languages = English, Portuguese, Spanish.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m \u001b[43msniff_validate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUSER_NLQ\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 769\u001b[39m, in \u001b[36msniff_validate_engine\u001b[39m\u001b[34m(USER_NLQ)\u001b[39m\n\u001b[32m    766\u001b[39m warnings.extend(w2)\n\u001b[32m    768\u001b[39m \u001b[38;5;28mprint\u001b[39m(time.strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m [S3] Ground-truth discovery & protocol validation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m gt_pmids, mesh_roles, dist, w3 = \u001b[43mstate3_ground_truth\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniverse_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    770\u001b[39m warnings.extend(w3)\n\u001b[32m    772\u001b[39m \u001b[38;5;28mprint\u001b[39m(time.strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m [S4] Search-strategy validation & refinement...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 554\u001b[39m, in \u001b[36mstate3_ground_truth\u001b[39m\u001b[34m(universe_query, protocol)\u001b[39m\n\u001b[32m    552\u001b[39m includes=[]; mesh_roles={\u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:[],\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m:[],\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m:[],\u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m:[]}\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m recs:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     js = \u001b[43mask_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSCREENER_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCREENER_SYSTEM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscreener_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCREENER_TEMPLATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (js.get(\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mN\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mY\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    556\u001b[39m         includes.append(r[\u001b[33m\"\u001b[39m\u001b[33mpmid\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 191\u001b[39m, in \u001b[36mask_json\u001b[39m\u001b[34m(model, system, user, template, stop_at_end)\u001b[39m\n\u001b[32m    189\u001b[39m rules = \u001b[33m\"\u001b[39m\u001b[33mReturn ONLY one JSON object. No analysis, no notes. Wrap EXACTLY with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBEGIN_JSON\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m{\u001b[39m\u001b[33m...}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEND_JSON\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    190\u001b[39m user_full = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrules\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m raw = \u001b[43mLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEND_JSON\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop_at_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(extract_json_block_or_fence(raw))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mLMStudioClient.chat\u001b[39m\u001b[34m(self, model, system, user, temperature, stop, response_format)\u001b[39m\n\u001b[32m    115\u001b[39m     body[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m] = response_format\n\u001b[32m    116\u001b[39m r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m out = r.json()[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# schedule idle unload after this use\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 400 Client Error: Bad Request for url: http://127.0.0.1:1234/v1/chat/completions"
     ]
    }
   ],
   "source": [
    "# SNIPPET: Sniff Validation Engine (state-machine refactor)\n",
    "# Goal: rigorous, resilient validation of evidence + search strategy feasibility\n",
    "# - Single \"Universe Query\" + validated Recommended Filters (topic/design/lang)\n",
    "# - State machine with remediation loops (no brittle hard-fails)\n",
    "# - Qwen: protocol lockdown, scope remediation, strategy remediation\n",
    "# - Gemma/Qwen-small: strict checklist screener for ground-truth discovery\n",
    "# - Idle model eviction for LM Studio (best-effort) after configurable idle secs\n",
    "#\n",
    "# CONFIGURE via env:\n",
    "#   LMSTUDIO_BASE = http://127.0.0.1:1234\n",
    "#   QWEN_MODEL    = qwen/qwen3-4b\n",
    "#   SCREENER_MODEL= gemma-3n-e2b-it  (or any small instruction-tuned model)\n",
    "#   ENTREZ_EMAIL  = you@example.com\n",
    "#   ENTREZ_API_KEY= ... (optional)\n",
    "#   HTTP_TIMEOUT  = 300   (seconds)\n",
    "#   LM_EVICT_ENABLED     = 1\n",
    "#   LM_IDLE_EVICT_SECS   = 5\n",
    "#   SYSTEM_KB_PATH       = system_knowledge_base.json (or /mnt/data/...)\n",
    "#\n",
    "# USAGE:\n",
    "#   1) Put your NLQ into USER_NLQ at bottom. Optionally place a knowledge-base JSON.\n",
    "#   2) Run this cell. Check sniff_poc_out/sniff_report.txt + sniff_artifacts.json.\n",
    "#\n",
    "# IMPORTANT:\n",
    "#   - We DO NOT set 'max_tokens' anywhere (no truncation).\n",
    "#   - We DO enforce strict JSON fences (BEGIN_JSON/END_JSON) for robustness.\n",
    "\n",
    "import os, json, time, re, pathlib, threading, textwrap, random\n",
    "from collections import Counter, defaultdict\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# ----------------------------\n",
    "# Config & Paths\n",
    "# ----------------------------\n",
    "LMSTUDIO_BASE = os.getenv(\"LMSTUDIO_BASE\", \"http://127.0.0.1:1234\")\n",
    "QWEN_MODEL    = os.getenv(\"QWEN_MODEL\", \"unsloth/qwen3-4b\")\n",
    "SCREENER_MODEL= os.getenv(\"SCREENER_MODEL\", \"gemma-3n-e2b-it@q8_0\")\n",
    "\n",
    "ENTREZ_EMAIL   = os.getenv(\"ENTREZ_EMAIL\", \"you@example.com\")\n",
    "ENTREZ_API_KEY = os.getenv(\"ENTREZ_API_KEY\", \"\")\n",
    "HTTP_TIMEOUT   = int(os.getenv(\"HTTP_TIMEOUT\", \"3000\"))\n",
    "\n",
    "LM_EVICT_ENABLED   = os.getenv(\"LM_EVICT_ENABLED\", \"1\") == \"1\"\n",
    "LM_IDLE_EVICT_SECS = int(os.getenv(\"LM_IDLE_EVICT_SECS\", \"5\"))\n",
    "\n",
    "SYSTEM_KB_PATH = os.getenv(\"SYSTEM_KB_PATH\", \"system_knowledge_base.json\")\n",
    "\n",
    "OUT_DIR = pathlib.Path(\"sniff_poc_out\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Universe windows (adjustable)\n",
    "UNIVERSE_MIN = int(os.getenv(\"UNIVERSE_MIN\", \"50\"))\n",
    "UNIVERSE_MAX = int(os.getenv(\"UNIVERSE_MAX\", \"10000\"))\n",
    "\n",
    "# Validation windows for strategy\n",
    "STRAT_MIN = int(os.getenv(\"STRAT_MIN\", \"10\"))\n",
    "STRAT_MAX = int(os.getenv(\"STRAT_MAX\", \"2000\"))\n",
    "\n",
    "# Numbers for GT discovery\n",
    "GT_FETCH_N   = int(os.getenv(\"GT_FETCH_N\", \"30\"))\n",
    "GT_REQUIRE_N = int(os.getenv(\"GT_REQUIRE_N\", \"3\"))\n",
    "\n",
    "# ----------------------------\n",
    "# LM Studio Client with Idle Eviction\n",
    "# ----------------------------\n",
    "class LMStudioClient:\n",
    "    def __init__(self, base):\n",
    "        self.base = base.rstrip(\"/\")\n",
    "        self._timers = {}  # model -> Timer\n",
    "\n",
    "    def _schedule_unload(self, model: str):\n",
    "        if not LM_EVICT_ENABLED:\n",
    "            return\n",
    "        # Cancel prior timer (if any)\n",
    "        t = self._timers.get(model)\n",
    "        if t and t.is_alive():\n",
    "            t.cancel()\n",
    "        timer = threading.Timer(LM_IDLE_EVICT_SECS, self._try_unload, args=(model,))\n",
    "        self._timers[model] = timer\n",
    "        timer.daemon = True\n",
    "        timer.start()\n",
    "\n",
    "    def _try_unload(self, model: str):\n",
    "        # Best-effort. Try a few plausible endpoints. Ignore failures.\n",
    "        endpoints = [\n",
    "            f\"{self.base}/v1/models/unload\",\n",
    "            f\"{self.base}/v1/unload\",\n",
    "            f\"{self.base}/v1/models/{model}/unload\"\n",
    "        ]\n",
    "        payloads = [\n",
    "            {\"model\": model},\n",
    "            {\"model\": model},\n",
    "            {}\n",
    "        ]\n",
    "        for url, payload in zip(endpoints, payloads):\n",
    "            try:\n",
    "                r = requests.post(url, json=payload, timeout=10)\n",
    "                if r.status_code in (200, 204):\n",
    "                    break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def chat(self, model: str, system: str, user: str, temperature=0.0, stop=None, response_format=None):\n",
    "        url = f\"{self.base}/v1/chat/completions\"\n",
    "        body = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}],\n",
    "            \"temperature\": float(temperature),\n",
    "            \"stream\": False\n",
    "        }\n",
    "        if stop is not None:\n",
    "            body[\"stop\"] = stop\n",
    "        if response_format is not None:\n",
    "            body[\"response_format\"] = response_format\n",
    "        r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        out = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # schedule idle unload after this use\n",
    "        self._schedule_unload(model)\n",
    "        return out\n",
    "\n",
    "LM = LMStudioClient(LMSTUDIO_BASE)\n",
    "\n",
    "# ----------------------------\n",
    "# Strict JSON extraction helpers\n",
    "# ----------------------------\n",
    "_BEGIN = re.compile(r\"BEGIN_JSON\\s*\", re.I)\n",
    "_END   = re.compile(r\"\\s*END_JSON\", re.I)\n",
    "FENCE  = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)```\", re.I)\n",
    "\n",
    "def _sanitize_json_str(s: str) -> str:\n",
    "    s = s.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n",
    "    s = re.sub(r\",\\s*(\\}|\\])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_json_block_or_fence(txt: str) -> str:\n",
    "    blocks = []\n",
    "    pos=0\n",
    "    while True:\n",
    "        m1 = _BEGIN.search(txt, pos)\n",
    "        if not m1: break\n",
    "        m2 = _END.search(txt, m1.end())\n",
    "        if not m2: break\n",
    "        blocks.append(txt[m1.end():m2.start()])\n",
    "        pos = m2.end()\n",
    "    if blocks:\n",
    "        return _sanitize_json_str(blocks[-1])\n",
    "\n",
    "    fences = FENCE.findall(txt)\n",
    "    if fences:\n",
    "        return _sanitize_json_str(fences[-1])\n",
    "\n",
    "    # last {...} object if present\n",
    "    s = txt\n",
    "    last_obj = None\n",
    "    stack = 0; start = None\n",
    "    for i,ch in enumerate(s):\n",
    "        if ch == '{':\n",
    "            if stack == 0: start = i\n",
    "            stack += 1\n",
    "        elif ch == '}':\n",
    "            if stack > 0:\n",
    "                stack -= 1\n",
    "                if stack == 0 and start is not None:\n",
    "                    last_obj = s[start:i+1]\n",
    "    if last_obj:\n",
    "        return _sanitize_json_str(last_obj)\n",
    "    raise ValueError(\"No JSON-like content found\")\n",
    "\n",
    "# Robust \"ask for JSON\" with repair fallback\n",
    "REPAIR_SYSTEM = \"You repair malformed JSON to exactly match the given template keys. Return ONLY one JSON object between BEGIN_JSON/END_JSON.\"\n",
    "\n",
    "def repair_user(template_json: str, bad_output: str) -> str:\n",
    "    return f\"\"\"TEMPLATE_JSON:\n",
    "{template_json}\n",
    "\n",
    "BAD_OUTPUT:\n",
    "{bad_output}\n",
    "\n",
    "TASK: Output valid JSON matching TEMPLATE_JSON keys (fill missing with empty arrays/strings). No prose.\n",
    "\n",
    "BEGIN_JSON\n",
    "{{}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "def ask_json(model: str, system: str, user: str, template: dict, stop_at_end=True):\n",
    "    rules = \"Return ONLY one JSON object. No analysis, no notes. Wrap EXACTLY with:\\nBEGIN_JSON\\n{...}\\nEND_JSON\"\n",
    "    user_full = f\"{user}\\n\\n{rules}\"\n",
    "    raw = LM.chat(model, system, user_full, temperature=0.0, stop=[\"END_JSON\"] if stop_at_end else None)\n",
    "    try:\n",
    "        return json.loads(extract_json_block_or_fence(raw))\n",
    "    except Exception:\n",
    "        repaired = LM.chat(\n",
    "            model,\n",
    "            REPAIR_SYSTEM,\n",
    "            repair_user(json.dumps(template, ensure_ascii=False, indent=2), raw) + \"\\n\\n\" + rules,\n",
    "            temperature=0.0,\n",
    "            stop=[\"END_JSON\"] if stop_at_end else None\n",
    "        )\n",
    "        return json.loads(extract_json_block_or_fence(repaired))\n",
    "\n",
    "# ----------------------------\n",
    "# PubMed E-utilities (esearch/efetch)\n",
    "# ----------------------------\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "HEADERS = {\"User-Agent\": \"sniff-validation-engine/0.2 (+local)\", \"Accept\": \"application/json\"}\n",
    "\n",
    "def esearch_ids(term: str, mindate: int|None, retmax=100, retstart=0, usehistory=True):\n",
    "    params = {\n",
    "        \"db\":\"pubmed\",\"retmode\":\"json\",\"term\":term,\"retmax\":retmax,\"retstart\":retstart,\n",
    "        \"email\":ENTREZ_EMAIL\n",
    "    }\n",
    "    if usehistory:\n",
    "        params[\"usehistory\"]=\"y\"\n",
    "    if ENTREZ_API_KEY:\n",
    "        params[\"api_key\"]=ENTREZ_API_KEY\n",
    "    if mindate:\n",
    "        params[\"mindate\"]=str(mindate)\n",
    "    r = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params=params, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    js = r.json().get(\"esearchresult\", {})\n",
    "    count = int(js.get(\"count\",\"0\"))\n",
    "    ids = js.get(\"idlist\", []) or []\n",
    "    webenv = js.get(\"webenv\")\n",
    "    qk = js.get(\"querykey\")\n",
    "    return count, [str(x) for x in ids], webenv, qk\n",
    "\n",
    "def esearch_all_ids(term: str, mindate: int|None, limit=5000):\n",
    "    # Fetch up to 'limit' ids using WebEnv\n",
    "    count, _, webenv, qk = esearch_ids(term, mindate, retmax=0, retstart=0, usehistory=True)\n",
    "    if not count or not webenv or not qk:\n",
    "        return 0, []\n",
    "    out=[]\n",
    "    start=0\n",
    "    while start < min(count, limit):\n",
    "        r2 = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params={\n",
    "            \"db\":\"pubmed\",\"retmode\":\"json\",\"retmax\":min(500, limit-start),\"retstart\":start,\"email\":ENTREZ_EMAIL,\n",
    "            \"WebEnv\":webenv,\"query_key\":qk, **({\"api_key\":ENTREZ_API_KEY} if ENTREZ_API_KEY else {})\n",
    "        }, timeout=HTTP_TIMEOUT)\n",
    "        r2.raise_for_status()\n",
    "        ids = r2.json().get(\"esearchresult\",{}).get(\"idlist\",[])\n",
    "        if not ids: break\n",
    "        out.extend([str(x) for x in ids])\n",
    "        start += len(ids)\n",
    "    return count, out\n",
    "\n",
    "def efetch_xml(pmids):\n",
    "    if not pmids: return \"\"\n",
    "    params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"id\":\",\".join(pmids),\"email\":ENTREZ_EMAIL}\n",
    "    if ENTREZ_API_KEY: params[\"api_key\"]=ENTREZ_API_KEY\n",
    "    r = requests.get(f\"{EUTILS}/efetch.fcgi\", headers={\"User-Agent\": \"sniff-validation-engine/0.2\"}, params=params, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_pubmed_xml(xml_text: str):\n",
    "    out = []\n",
    "    if not xml_text.strip(): return out\n",
    "    root = ET.fromstring(xml_text)\n",
    "    def _join(node):\n",
    "        if node is None: return \"\"\n",
    "        try: return \"\".join(node.itertext())\n",
    "        except Exception: return node.text or \"\"\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = art.findtext(\".//PMID\") or \"\"\n",
    "        title = _join(art.find(\".//ArticleTitle\")).strip()\n",
    "        abs_nodes = art.findall(\".//Abstract/AbstractText\")\n",
    "        abstract = \" \".join(_join(n).strip() for n in abs_nodes) if abs_nodes else \"\"\n",
    "        year = None\n",
    "        for path in (\".//ArticleDate/Year\",\".//PubDate/Year\",\".//DateCreated/Year\",\".//PubDate/MedlineDate\"):\n",
    "            s = art.findtext(path)\n",
    "            if s:\n",
    "                m = re.search(r\"\\d{4}\", s)\n",
    "                if m: year = int(m.group(0)); break\n",
    "        lang = art.findtext(\".//Language\") or None\n",
    "        pubtypes = [pt.text for pt in art.findall(\".//PublicationTypeList/PublicationType\") if pt.text]\n",
    "        mesh = [mh.findtext(\"./DescriptorName\") for mh in art.findall(\".//MeshHeadingList/MeshHeading\") if mh.findtext(\"./DescriptorName\")]\n",
    "        out.append({\n",
    "            \"pmid\": pmid, \"title\": title, \"abstract\": abstract, \"year\": year, \"language\": lang,\n",
    "            \"publication_types\": pubtypes, \"mesh\": mesh\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge Base (KB) loading\n",
    "# ----------------------------\n",
    "DEFAULT_KB = {\n",
    "    \"publication_types\": [\n",
    "        \"Randomized Controlled Trial\",\"Clinical Trial\",\"Controlled Clinical Trial\",\n",
    "        \"Comparative Study\",\"Cohort Studies\",\"Case-Control Studies\",\"Observational Study\",\n",
    "        \"Systematic Review\",\"Meta-Analysis\",\"Network Meta-Analysis\"\n",
    "    ],\n",
    "    \"languages\": [\"english\",\"portuguese\",\"spanish\",\"french\",\"german\",\"italian\",\"chinese\",\"japanese\",\"korean\"],\n",
    "    \"design_precedence\": [\"Randomized Controlled Trial\",\"Controlled Clinical Trial\",\"Clinical Trial\",\"Comparative Study\",\"Cohort Studies\",\"Case-Control Studies\"],\n",
    "    \"mesh_topic_whitelist\": [],  # optional allow-list\n",
    "}\n",
    "\n",
    "def load_system_kb():\n",
    "    # Try explicit path, else /mnt/data, else defaults\n",
    "    p = pathlib.Path(SYSTEM_KB_PATH)\n",
    "    if not p.exists():\n",
    "        alt = pathlib.Path(\"/mnt/data/system_knowledge_base.json\")\n",
    "        if alt.exists():\n",
    "            p = alt\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return DEFAULT_KB\n",
    "\n",
    "SYSTEM_KB = load_system_kb()\n",
    "\n",
    "# ----------------------------\n",
    "# Utility: query builders\n",
    "# ----------------------------\n",
    "def or_block(terms, field=\"tiab\"):\n",
    "    toks=[]\n",
    "    for t in terms or []:\n",
    "        t=t.strip()\n",
    "        if not t: continue\n",
    "        if \" \" in t or \"-\" in t:\n",
    "            toks.append(f\"\\\"{t}\\\"[{field}]\")\n",
    "        else:\n",
    "            toks.append(f\"{t}[{field}]\")\n",
    "    if not toks: return \"\"\n",
    "    return \"(\" + \" OR \".join(toks) + \")\"\n",
    "\n",
    "def or_mesh(terms):\n",
    "    toks=[]\n",
    "    for t in terms or []:\n",
    "        t=t.strip()\n",
    "        if not t: continue\n",
    "        toks.append(f\"\\\"{t}\\\"[MeSH Terms]\")\n",
    "    if not toks: return \"\"\n",
    "    return \"(\" + \" OR \".join(toks) + \")\"\n",
    "\n",
    "def and_join(parts):\n",
    "    parts=[p for p in parts if p and p.strip()]\n",
    "    if not parts: return \"\"\n",
    "    return \" AND \".join(f\"({p})\" if \" OR \" in p or \" AND \" in p else p for p in parts)\n",
    "\n",
    "def lang_filter(langs):\n",
    "    if not langs: return \"\"\n",
    "    # PubMed supports lang filter via language field tags like english[lang]\n",
    "    return \"(\" + \" OR \".join(f\"\\\"{l}\\\"[lang]\" for l in langs) + \")\"\n",
    "\n",
    "# ----------------------------\n",
    "# STATE 1: Protocol Lockdown\n",
    "# ----------------------------\n",
    "PROTO_SYSTEM = \"You convert NLQs into a strict, compact protocol using only allowed values for controlled fields. Return strict JSON only.\"\n",
    "\n",
    "PROTO_TEMPLATE = {\n",
    "    \"research_question\": \"\",\n",
    "    \"population_terms\": [],\n",
    "    \"intervention_terms\": [],\n",
    "    \"comparator_terms\": [],\n",
    "    \"outcome_terms\": [],\n",
    "    \"anchors\": [],\n",
    "    \"avoid\": [],\n",
    "    \"languages\": [],\n",
    "    \"year_min\": 2015,\n",
    "    \"designs_preference\": [],   # subset of KB.design_precedence or KB.publication_types\n",
    "    \"adult_only\": False\n",
    "}\n",
    "\n",
    "def proto_user(nlq: str, kb: dict):\n",
    "    return f\"\"\"Natural-language question:\n",
    "<<<\n",
    "{nlq}\n",
    ">>>\n",
    "\n",
    "Your job:\n",
    "1) Parse into a compact protocol object.\n",
    "2) For 'designs_preference' choose ONLY from these allowed values (preserve order where applicable):\n",
    "   {kb.get(\"design_precedence\", [])}\n",
    "3) For 'languages' choose ONLY from:\n",
    "   {kb.get(\"languages\", [])}\n",
    "\n",
    "Rules:\n",
    "- Keep each term string simple (no boolean operators, quotes, or field tags).\n",
    "- 'anchors' should be 2–6 tokens that must appear to ensure topicality (e.g., MIRPE, Nuss, cryoablation).\n",
    "- If adult-only is implied, set adult_only=true.\n",
    "- If year_min is stated, use it; else leave default.\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\n",
    "  \"research_question\": \"\",\n",
    "  \"population_terms\": [],\n",
    "  \"intervention_terms\": [],\n",
    "  \"comparator_terms\": [],\n",
    "  \"outcome_terms\": [],\n",
    "  \"anchors\": [],\n",
    "  \"avoid\": [],\n",
    "  \"languages\": [],\n",
    "  \"year_min\": 2015,\n",
    "  \"designs_preference\": [],\n",
    "  \"adult_only\": false\n",
    "}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "def state1_protocol_lockdown(nlq: str):\n",
    "    js = ask_json(QWEN_MODEL, PROTO_SYSTEM, proto_user(nlq, SYSTEM_KB), PROTO_TEMPLATE)\n",
    "    # Clip controlled fields to KB just in case\n",
    "    js[\"languages\"] = [l for l in js.get(\"languages\", []) if l in SYSTEM_KB.get(\"languages\", [])]\n",
    "    pref = js.get(\"designs_preference\", [])\n",
    "    allowed = SYSTEM_KB.get(\"design_precedence\", SYSTEM_KB.get(\"publication_types\", []))\n",
    "    js[\"designs_preference\"] = [d for d in pref if d in allowed]\n",
    "    return js\n",
    "\n",
    "# ----------------------------\n",
    "# STATE 2: Universe Definition & Sizing\n",
    "# ----------------------------\n",
    "def build_universe_query(protocol: dict):\n",
    "    P = or_block(protocol.get(\"population_terms\", []), \"tiab\")\n",
    "    I = or_block(protocol.get(\"intervention_terms\", []), \"tiab\")\n",
    "    parts = [P, I]\n",
    "    # Adult bias (soft) as lexical cue only if adult_only flagged\n",
    "    if protocol.get(\"adult_only\"):\n",
    "        parts.append(or_block([\"adult\",\"adults\"], \"tiab\"))\n",
    "    return and_join(parts)\n",
    "\n",
    "REMEDIATION_SCOPE_TEMPLATE = {\"action\":\"KEEP|WIDEN|NARROW\",\"add_population_terms\":[],\"add_intervention_terms\":[],\"enforce_anchors\":[]}\n",
    "\n",
    "REMEDIATION_SCOPE_SYSTEM = \"You propose surgical lexical scope fixes for a PubMed TIAB-only query. Return strict JSON only.\"\n",
    "\n",
    "def remediate_scope_user(query: str, count: int, protocol: dict, window: tuple[int,int]):\n",
    "    low, high = window\n",
    "    status = \"too_narrow\" if count < low else \"too_broad\"\n",
    "    return f\"\"\"The current universe TIAB-only query (P AND I) is {status}.\n",
    "Target window: {low}..{high}. Current count={count}.\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Protocol snapshot:\n",
    "population_terms = {protocol.get(\"population_terms\", [])}\n",
    "intervention_terms = {protocol.get(\"intervention_terms\", [])}\n",
    "anchors = {protocol.get(\"anchors\", [])}\n",
    "\n",
    "Propose ONE fix:\n",
    "- If too_narrow: suggest 'WIDEN' with 1-4 broader/popular synonyms to ADD to P and/or I (no quotes, no field tags).\n",
    "- If too_broad: suggest 'NARROW' with 1-3 'enforce_anchors' tokens (from anchors or near equivalents) to AND onto the query.\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"action\":\"KEEP|WIDEN|NARROW\",\"add_population_terms\":[],\"add_intervention_terms\":[],\"enforce_anchors\":[]}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "def state2_universe(protocol: dict, attempts=2):\n",
    "    window = (UNIVERSE_MIN, UNIVERSE_MAX)\n",
    "    uq = build_universe_query(protocol)\n",
    "    if not uq:\n",
    "        raise SystemExit(\"FATAL: cannot build Universe Query (missing P or I terms).\")\n",
    "    for i in range(attempts+1):\n",
    "        cnt, _ = esearch_all_ids(uq, protocol.get(\"year_min\"), limit=0)\n",
    "        print(time.strftime('%H:%M:%S'), f\"  [Universe] try={i} count={cnt} window={window} query={uq}\")\n",
    "        if window[0] <= cnt <= window[1]:\n",
    "            return uq, cnt, []\n",
    "        # Ask LLM to remediate scope\n",
    "        remed = ask_json(QWEN_MODEL, REMEDIATION_SCOPE_SYSTEM,\n",
    "                         remediate_scope_user(uq, cnt, protocol, window),\n",
    "                         REMEDIATION_SCOPE_TEMPLATE)\n",
    "        if remed.get(\"action\") == \"WIDEN\" and cnt < window[0]:\n",
    "            protocol[\"population_terms\"] = list(dict.fromkeys(protocol.get(\"population_terms\", []) + remed.get(\"add_population_terms\", [])))\n",
    "            protocol[\"intervention_terms\"] = list(dict.fromkeys(protocol.get(\"intervention_terms\", []) + remed.get(\"add_intervention_terms\", [])))\n",
    "            uq = build_universe_query(protocol)\n",
    "        elif remed.get(\"action\") == \"NARROW\" and cnt > window[1]:\n",
    "            anchors = remed.get(\"enforce_anchors\", []) or protocol.get(\"anchors\", [])\n",
    "            if anchors:\n",
    "                uq = and_join([uq, or_block(anchors, \"tiab\")])\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    # last resort: return whatever we have with warning\n",
    "    return uq, cnt, [f\"WARNING: Universe count {cnt} outside target window {window}. Proceeding anyway.\"]\n",
    "\n",
    "# ----------------------------\n",
    "# STATE 3: Ground Truth Discovery & Protocol Validation\n",
    "# ----------------------------\n",
    "SCREENER_TEMPLATE = {\n",
    "  \"pmid\":\"\", \"checklist\":{\"population\":\"N\",\"intervention\":\"N\",\"outcome\":\"N\",\"design\":\"N\"},\n",
    "  \"include\":\"N\",\"why\":\"\", \"mesh_roles\":{\"P\":[],\"I\":[],\"C\":[],\"O\":[]}\n",
    "}\n",
    "\n",
    "SCREENER_SYSTEM = \"You are a strict protocol checklist screener. Return JSON only. No explanations unless asked.\"\n",
    "\n",
    "def screener_user(protocol: dict, record: dict):\n",
    "    # Concrete, minimal checklist. INCLUDE only if ALL are 'Y'.\n",
    "    return f\"\"\"Protocol (locked):\n",
    "- Population scope (lexical cues): {protocol.get(\"population_terms\", [])}\n",
    "- Index intervention (lexical cues): {protocol.get(\"intervention_terms\", [])}\n",
    "- Outcomes of interest (lexical cues): {protocol.get(\"outcome_terms\", [])}\n",
    "- Designs preference (for 'design' check): {protocol.get(\"designs_preference\", [])}\n",
    "- Adults only: {protocol.get(\"adult_only\", False)}\n",
    "\n",
    "Article:\n",
    "PMID: {record.get('pmid')}\n",
    "Year: {record.get('year')}\n",
    "Lang: {record.get('language')}\n",
    "PubTypes: {record.get('publication_types')}\n",
    "Title: {record.get('title')}\n",
    "Abstract: {record.get('abstract')}\n",
    "\n",
    "Checklist (answer with 'Y' or 'N'):\n",
    "- Does the title/abstract clearly indicate the POPULATION/procedure context matches? (population)\n",
    "- Does it clearly include the INDEX INTERVENTION? (intervention)\n",
    "- Does it clearly report or promise our target OUTCOMES (pain scores, opioid consumption/requirements, etc.)? (outcome)\n",
    "- Does it clearly meet a preferred DESIGN category (e.g., randomized/controlled/comparative/clinical trial)? (design)\n",
    "\n",
    "INCLUDE if and only if ALL four are 'Y'. If INCLUDE, classify any useful MeSH descriptors (from the article text or common MeSH for this topic) into the buckets P/I/C/O.\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\n",
    "  \"pmid\": \"{record.get('pmid')}\",\n",
    "  \"checklist\": {{\"population\":\"N\",\"intervention\":\"N\",\"outcome\":\"N\",\"design\":\"N\"}},\n",
    "  \"include\": \"N\",\n",
    "  \"why\": \"\",\n",
    "  \"mesh_roles\": {{\"P\":[],\"I\":[],\"C\":[],\"O\":[]}}\n",
    "}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "def accumulate_distributions(recs):\n",
    "    years=[r.get(\"year\") for r in recs if r.get(\"year\")]\n",
    "    langs=[r.get(\"language\") for r in recs if r.get(\"language\")]\n",
    "    ptypes=[pt for r in recs for pt in (r.get(\"publication_types\") or [])]\n",
    "    dist = {\n",
    "        \"years\": {\"median\": int(sorted(years)[len(years)//2]) if years else None,\n",
    "                  \"min\": min(years) if years else None,\n",
    "                  \"max\": max(years) if years else None},\n",
    "        \"languages\": dict(Counter(langs)),\n",
    "        \"ptypes\": dict(Counter(ptypes))\n",
    "    }\n",
    "    return dist\n",
    "\n",
    "def state3_ground_truth(universe_query: str, protocol: dict):\n",
    "    # Get first GT_FETCH_N results\n",
    "    cnt, ids, webenv, qk = esearch_ids(universe_query, protocol.get(\"year_min\"), retmax=GT_FETCH_N, retstart=0, usehistory=True)\n",
    "    xml = efetch_xml(ids)\n",
    "    recs = parse_pubmed_xml(xml)\n",
    "    # Strict screener\n",
    "    includes=[]; mesh_roles={\"P\":[],\"I\":[],\"C\":[],\"O\":[]}\n",
    "    for r in recs:\n",
    "        js = ask_json(SCREENER_MODEL, SCREENER_SYSTEM, screener_user(protocol, r), SCREENER_TEMPLATE)\n",
    "        if (js.get(\"include\",\"N\") == \"Y\"):\n",
    "            includes.append(r[\"pmid\"])\n",
    "            for k in (\"P\",\"I\",\"C\",\"O\"):\n",
    "                mesh_roles[k].extend([t for t in js.get(\"mesh_roles\",{}).get(k,[]) if t])\n",
    "    # Dedup & small cleanup\n",
    "    mesh_roles = {k: list(dict.fromkeys(v)) for k,v in mesh_roles.items()}\n",
    "    warnings=[]\n",
    "    if len(includes) < GT_REQUIRE_N:\n",
    "        raise SystemExit(f\"FATAL: insufficient ground-truth includes ({len(includes)}/{GT_REQUIRE_N}). Universe likely off-topic or too weak.\")\n",
    "    # Feasibility check on MeSH vernaculum\n",
    "    if not mesh_roles.get(\"P\"):\n",
    "        warnings.append(\"CRITICAL_WARNING: No P MeSH discovered; will fallback to lexical population terms.\")\n",
    "    if not mesh_roles.get(\"I\"):\n",
    "        warnings.append(\"CRITICAL_WARNING: No I MeSH discovered; will fallback to lexical intervention terms.\")\n",
    "    if not mesh_roles.get(\"C\"):\n",
    "        warnings.append(\"STANDARD_WARNING: No comparator MeSH discovered; lexical fallback.\")\n",
    "    if not mesh_roles.get(\"O\"):\n",
    "        warnings.append(\"STANDARD_WARNING: No outcome MeSH discovered; lexical fallback.\")\n",
    "    dist = accumulate_distributions(recs)\n",
    "    return includes, mesh_roles, dist, warnings\n",
    "\n",
    "# ----------------------------\n",
    "# STATE 4: Strategy Validation & Refinement\n",
    "# ----------------------------\n",
    "def best_design_filter(protocol: dict, kb: dict):\n",
    "    # Deterministic: choose first available from designs_preference; map to [pt] terms\n",
    "    precedence = protocol.get(\"designs_preference\") or kb.get(\"design_precedence\", [])\n",
    "    if not precedence:\n",
    "        return \"\"\n",
    "    # Build OR of acceptable [pt] tags from top preference down to 2 levels\n",
    "    chosen = precedence[:2] if len(precedence)>=2 else precedence\n",
    "    return \"(\" + \" OR \".join(f\"\\\"{pt}\\\"[Publication Type]\" for pt in chosen) + \")\"\n",
    "\n",
    "def build_topic_filter(mesh_roles: dict, protocol: dict):\n",
    "    # Prefer MeSH P & I; fallback to lexical if missing\n",
    "    Pm = mesh_roles.get(\"P\", [])\n",
    "    Im = mesh_roles.get(\"I\", [])\n",
    "    P = or_mesh(Pm) if Pm else or_block(protocol.get(\"population_terms\", []), \"tiab\")\n",
    "    I = or_mesh(Im) if Im else or_block(protocol.get(\"intervention_terms\", []), \"tiab\")\n",
    "    if not P or not I:\n",
    "        return \"\"\n",
    "    return and_join([P, I])\n",
    "\n",
    "REMEDIATION_STRAT_TEMPLATE = {\"op\":\"DROP_TERM|ADD_ANCHOR|BROADEN_DESIGN_FILTER\",\"term\":\"\",\"where\":\"P|I|ANCHOR|DESIGN\"}\n",
    "\n",
    "REMEDIATION_STRAT_SYSTEM = \"You propose ONE small surgical fix to a PubMed strategy that must pass recall & precision gates. Return strict JSON only.\"\n",
    "\n",
    "def remediate_strategy_user(universe_query: str, topic_filter: str, design_filter: str, anchors: list, failed: dict):\n",
    "    return f\"\"\"A validation just failed for the strategy below.\n",
    "\n",
    "UNIVERSE_QUERY:\n",
    "{universe_query}\n",
    "\n",
    "TOPIC_FILTER:\n",
    "{topic_filter}\n",
    "\n",
    "DESIGN_FILTER:\n",
    "{design_filter}\n",
    "\n",
    "ANCHORS:\n",
    "{anchors}\n",
    "\n",
    "Failure snapshot:\n",
    "{failed}\n",
    "\n",
    "You must propose exactly ONE of:\n",
    "- DROP_TERM (remove a single overly-specific token from topic filter; indicate 'term' and 'where': 'P' or 'I')\n",
    "- ADD_ANCHOR (add one topical anchor term to be AND-ed to the universe query; 'where'='ANCHOR')\n",
    "- BROADEN_DESIGN_FILTER (loosen the design filter; 'where'='DESIGN'; 'term' can be an additional publication type)\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"op\":\"DROP_TERM|ADD_ANCHOR|BROADEN_DESIGN_FILTER\",\"term\":\"\",\"where\":\"P|I|ANCHOR|DESIGN\"}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "def state4_validate_strategy(universe_query: str, ground_truth_pmids: list, protocol: dict, mesh_roles: dict, attempts=3):\n",
    "    topic = build_topic_filter(mesh_roles, protocol)\n",
    "    design = best_design_filter(protocol, SYSTEM_KB)\n",
    "    langs = lang_filter(protocol.get(\"languages\", []))\n",
    "    def combine(uq, topic, design, langs):\n",
    "        parts=[uq, topic, design, langs]\n",
    "        parts=[p for p in parts if p]\n",
    "        return and_join(parts)\n",
    "    strategy = combine(universe_query, topic, design, langs)\n",
    "\n",
    "    for i in range(attempts+1):\n",
    "        total, ids = esearch_all_ids(strategy, protocol.get(\"year_min\"), limit=5000)\n",
    "        # Check recall: all GT must be in results\n",
    "        recall_ok = set(ground_truth_pmids).issubset(set(ids))\n",
    "        precision_ok = STRAT_MIN <= total <= STRAT_MAX\n",
    "        print(time.strftime('%H:%M:%S'), f\"  [Strategy] try={i} total={total} recall_ok={recall_ok} precision_ok={precision_ok}\")\n",
    "        if recall_ok and precision_ok:\n",
    "            return {\"topic_filter\": topic, \"design_filter\": design, \"language_filter\": langs, \"final_query\": strategy, \"total\": total}\n",
    "        failed = {\"total\": total, \"recall_ok\": recall_ok, \"precision_ok\": precision_ok}\n",
    "        remed = ask_json(QWEN_MODEL, REMEDIATION_STRAT_SYSTEM,\n",
    "                         remediate_strategy_user(universe_query, topic, design, protocol.get(\"anchors\", []), failed),\n",
    "                         REMEDIATION_STRAT_TEMPLATE)\n",
    "        op = remed.get(\"op\",\"\")\n",
    "        term = (remed.get(\"term\") or \"\").strip()\n",
    "        where = remed.get(\"where\",\"\")\n",
    "\n",
    "        # Execute remediation\n",
    "        if op == \"DROP_TERM\" and term and where in (\"P\",\"I\"):\n",
    "            # Remove a token from topic filter by lexical fallback: rebuild topic with lexicals, drop term\n",
    "            if where==\"P\":\n",
    "                if term in mesh_roles.get(\"P\", []):\n",
    "                    mesh_roles[\"P\"] = [t for t in mesh_roles[\"P\"] if t != term]\n",
    "                # also drop from lexical P if present\n",
    "                protocol[\"population_terms\"] = [t for t in protocol.get(\"population_terms\", []) if t != term]\n",
    "            else:\n",
    "                if term in mesh_roles.get(\"I\", []):\n",
    "                    mesh_roles[\"I\"] = [t for t in mesh_roles[\"I\"] if t != term]\n",
    "                protocol[\"intervention_terms\"] = [t for t in protocol.get(\"intervention_terms\", []) if t != term]\n",
    "            topic = build_topic_filter(mesh_roles, protocol)\n",
    "        elif op == \"ADD_ANCHOR\" and term:\n",
    "            # AND anchor into universe query\n",
    "            uq_anchor = or_block([term], \"tiab\")\n",
    "            universe_query = and_join([universe_query, uq_anchor])\n",
    "        elif op == \"BROADEN_DESIGN_FILTER\":\n",
    "            # Add another ptype OR fallback to comparative set\n",
    "            add = term if term else \"Comparative Study\"\n",
    "            design = and_join([design, f\"\\\"{add}\\\"[Publication Type]\"]) if design else f\"\\\"{add}\\\"[Publication Type]\"\n",
    "        else:\n",
    "            # if remediation invalid, try a simple deterministic fallback: drop design filter entirely\n",
    "            design = \"\"\n",
    "        strategy = combine(universe_query, topic, design, langs)\n",
    "\n",
    "    raise SystemExit(\"FATAL: Strategy validation failed after remediation attempts.\")\n",
    "\n",
    "# ----------------------------\n",
    "# STATE 5: Finalization & Handoff\n",
    "# ----------------------------\n",
    "EMBED_SYSTEM = \"You write a single-sentence research question string optimized for embedding search. Return JSON only.\"\n",
    "EMBED_TEMPLATE = {\"embedding_query\": \"\"}\n",
    "\n",
    "def embed_user(protocol: dict, mesh_roles: dict):\n",
    "    return f\"\"\"Generate one compact sentence (<= 30 words) suitable for vector embeddings that captures the finalized protocol and key vocabulary.\n",
    "\n",
    "Protocol brief:\n",
    "- Population: {protocol.get(\"population_terms\", [])}\n",
    "- Intervention: {protocol.get(\"intervention_terms\", [])}\n",
    "- Outcomes: {protocol.get(\"outcome_terms\", [])}\n",
    "- Anchors: {protocol.get(\"anchors\", [])}\n",
    "- Languages: {protocol.get(\"languages\", [])}\n",
    "- Year_min: {protocol.get(\"year_min\")}\n",
    "- Designs_preference: {protocol.get(\"designs_preference\", [])}\n",
    "\n",
    "MeSH vernaculum (kept):\n",
    "- P: {mesh_roles.get(\"P\", [])}\n",
    "- I: {mesh_roles.get(\"I\", [])}\n",
    "- C: {mesh_roles.get(\"C\", [])}\n",
    "- O: {mesh_roles.get(\"O\", [])}\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"embedding_query\": \"\"}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "def write_report(out_path: pathlib.Path, nlq: str, protocol: dict, universe_query: str, universe_count: int,\n",
    "                 gt_pmids: list, mesh_roles: dict, dist: dict, warnings: list, strategy: dict):\n",
    "    lines=[]\n",
    "    lines.append(\"==================== SNIFF VALIDATION REPORT ====================\")\n",
    "    lines.append(\"NLQ:\")\n",
    "    lines.append(\"  \" + (nlq.strip().replace(\"\\n\",\" \")[:160] + (\"...\" if len(nlq.strip())>160 else \"\")))\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"LOCKED PROTOCOL\")\n",
    "    lines.append(f\"  languages: {protocol.get('languages', [])}   year_min: {protocol.get('year_min')}\")\n",
    "    lines.append(f\"  designs_preference: {protocol.get('designs_preference', [])}   adult_only: {protocol.get('adult_only', False)}\")\n",
    "    lines.append(f\"  P terms: {protocol.get('population_terms', [])}\")\n",
    "    lines.append(f\"  I terms: {protocol.get('intervention_terms', [])}\")\n",
    "    lines.append(f\"  C terms: {protocol.get('comparator_terms', [])}\")\n",
    "    lines.append(f\"  O terms: {protocol.get('outcome_terms', [])}\")\n",
    "    lines.append(f\"  anchors: {protocol.get('anchors', [])}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"UNIVERSE\")\n",
    "    lines.append(f\"  query: {universe_query}\")\n",
    "    lines.append(f\"  count: {universe_count}  window_ok: {UNIVERSE_MIN <= universe_count <= UNIVERSE_MAX}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"GROUND TRUTH (strict checklist includes)\")\n",
    "    lines.append(f\"  n_includes: {len(gt_pmids)}  pmids: {gt_pmids}\")\n",
    "    lines.append(f\"  MeSH vernaculum: P={mesh_roles.get('P', [])}  I={mesh_roles.get('I', [])}  C={mesh_roles.get('C', [])}  O={mesh_roles.get('O', [])}\")\n",
    "    lines.append(f\"  Sample distributions: years={dist.get('years')}  languages={dist.get('languages')}  pubtypes={dist.get('ptypes')}\")\n",
    "    if warnings:\n",
    "        lines.append(\"  WARNINGS:\")\n",
    "        for w in warnings:\n",
    "            lines.append(f\"   - {w}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"VALIDATED STRATEGY & FILTERS\")\n",
    "    lines.append(f\"  topic_filter: {strategy.get('topic_filter')}\")\n",
    "    lines.append(f\"  design_filter: {strategy.get('design_filter')}\")\n",
    "    lines.append(f\"  language_filter: {strategy.get('language_filter')}\")\n",
    "    lines.append(f\"  final_query: {strategy.get('final_query')}\")\n",
    "    lines.append(f\"  total_hits: {strategy.get('total')}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"================== END OF REPORT =====================\")\n",
    "    out_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "# ----------------------------\n",
    "# Top-level: Sniff Validation Engine (state machine)\n",
    "# ----------------------------\n",
    "def sniff_validate_engine(USER_NLQ: str):\n",
    "    warnings=[]\n",
    "    print(time.strftime('%H:%M:%S'), \" [S1] Protocol lockdown...\")\n",
    "    protocol = state1_protocol_lockdown(USER_NLQ)\n",
    "\n",
    "    print(time.strftime('%H:%M:%S'), \" [S2] Universe definition & sizing...\")\n",
    "    universe_query, universe_count, w2 = state2_universe(protocol, attempts=2)\n",
    "    warnings.extend(w2)\n",
    "\n",
    "    print(time.strftime('%H:%M:%S'), \" [S3] Ground-truth discovery & protocol validation...\")\n",
    "    gt_pmids, mesh_roles, dist, w3 = state3_ground_truth(universe_query, protocol)\n",
    "    warnings.extend(w3)\n",
    "\n",
    "    print(time.strftime('%H:%M:%S'), \" [S4] Search-strategy validation & refinement...\")\n",
    "    strategy = state4_validate_strategy(universe_query, gt_pmids, protocol, mesh_roles, attempts=3)\n",
    "\n",
    "    print(time.strftime('%H:%M:%S'), \" [S5] Finalization & handoff...\")\n",
    "    emb = ask_json(QWEN_MODEL, EMBED_SYSTEM, embed_user(protocol, mesh_roles), EMBED_TEMPLATE)\n",
    "    embedding_q = emb.get(\"embedding_query\",\"\")\n",
    "\n",
    "    artifacts = {\n",
    "        \"locked_protocol\": protocol,\n",
    "        \"universe_query\": universe_query,\n",
    "        \"universe_count\": universe_count,\n",
    "        \"recommended_filters\": {\n",
    "            \"topic\": strategy.get(\"topic_filter\"),\n",
    "            \"design\": strategy.get(\"design_filter\"),\n",
    "            \"language\": strategy.get(\"language_filter\")\n",
    "        },\n",
    "        \"final_query\": strategy.get(\"final_query\"),\n",
    "        \"final_query_total\": strategy.get(\"total\"),\n",
    "        \"ground_truth_pmids\": gt_pmids,\n",
    "        \"mesh_vernaculum\": mesh_roles,\n",
    "        \"warnings\": warnings,\n",
    "        \"research_question_string_for_embedding\": embedding_q,\n",
    "        \"system_kb_snapshot\": SYSTEM_KB\n",
    "    }\n",
    "\n",
    "    # Persist\n",
    "    (OUT_DIR/\"sniff_artifacts.json\").write_text(json.dumps(artifacts, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    write_report(OUT_DIR/\"sniff_report.txt\", USER_NLQ, protocol, universe_query, universe_count, gt_pmids, mesh_roles, dist, warnings, strategy)\n",
    "    print(\"Artifacts written:\", OUT_DIR)\n",
    "    print(\" - sniff_report.txt\")\n",
    "    print(\" - sniff_artifacts.json\")\n",
    "\n",
    "# ----------------------------\n",
    "# RUN\n",
    "# ----------------------------\n",
    "USER_NLQ = \"\"\"Population = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE (the intervention of interest is INC, not the surgery). Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia. Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days. Study designs = RCTs preferred; if RCTs absent, include comparative cohort/case-control. Year_min = 2015. Languages = English, Portuguese, Spanish.\"\"\"\n",
    "sniff_validate_engine(USER_NLQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a7d6135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:30:08  [S1] Protocol lockdown...\n",
      "  [S1] Locked protocol:\n",
      "    {\"population_terms\": [\"adults\", \"minimally invasive repair of pectus excavatum\", \"Nuss procedure\", \"pectus excavatum surgery\"], \"intervention_terms\": [\"intercostal nerve cryoablation\", \"INC\", \"intraoperative analgesia\"], \"comparators_terms\": [\"thoracic epidural\", \"paravertebral block\", \"intercostal nerve block\", \"erector spinae plane block\", \"systemic multimodal analgesia\"], \"outcomes_terms\": [\"postoperative opioid consumption\", \"in-hospital opioid use\", \"discharge opioid use\", \"pain scores\", \"0-7 day pain assessment\"], \"must_have\": [\"pectus excavatum surgery\", \"minimally invasive repair\", \"postoperative pain management\", \"analgesia techniques\"], \"avoid\": [\"cardiac surgery\", \"chest wall deformity\"], \"designs_preference\": \"Randomized Controlled Trial\", \"languages\": [\"english\", \"portuguese\", \"spanish\"], \"year_min\": 2015}\n",
      "16:30:35  [S2] Universe definition & sizing...\n",
      "   [Universe] try=0 count=27 window=(50, 10000) query=(adults[tiab] OR \"minimally invasive repair of pectus excavatum\"[tiab] OR \"Nuss procedure\"[tiab] OR \"pectus excavatum surgery\"[tiab]) AND (\"intercostal nerve cryoablation\"[tiab] OR INC[tiab] OR \"intraoperative analgesia\"[tiab]) AND (\"pectus excavatum surgery\"[tiab] OR \"minimally invasive repair\"[tiab] OR \"postoperative pain management\"[tiab] OR \"analgesia techniques\"[tiab])\n",
      "   [Universe] try=1 count=4059 window=(50, 10000) query=(adults[tiab] OR \"minimally invasive repair of pectus excavatum\"[tiab] OR \"Nuss procedure\"[tiab] OR \"pectus excavatum surgery\"[tiab] OR pediatric[tiab]) AND (\"intercostal nerve cryoablation\"[tiab] OR INC[tiab] OR \"intraoperative analgesia\"[tiab] OR \"postoperative analgesia\"[tiab])\n",
      "16:31:01  [S3] Ground-truth discovery & protocol validation...\n",
      "16:31:03  [LM] Switching models: waiting 2.0s for TTL auto-evict...\n",
      "     [Screen] PMID 40926926 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'N', 'D': 'N'} why=The record does not mention the specified population (adults, minimally invasive repair of pectus excavatum, Nuss procedure, pectus excavatum surgery), intervention (intercostal nerve cryoablation, INC, intraoperative analgesia), or outcomes (postoperative opioid consumption, in-hospital opioid use, discharge opioid use, pain scores, 0-7 day pain assessment). It focuses on infection prevention in pediatric and neonatal populations using UV-C technology.\n",
      "       Title: Advances in Infection Prevention for Pediatric and Neonatal Populations: Classification of Methods and the Emerging Superiority of Ultraviolet-C (UV-C) Technologies.\n",
      "       Abstract: Healthcare-associated infections (HAIs) continue to pose major risks to pediatric and neonatal patients, whose immature immune systems and unique vulnerabilities demand tailored infection prevention strategies. Traditional methods, including chemical disinfectants, procedural protocols, and physical hygiene measures, have contributed to reductions in HAIs but remain limited by human error, environmental toxicity, and the rise of antimicrobial resistance. Advances in disinfection technologies, pa...\n",
      "     [Screen] PMID 40924552 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'Y', 'D': 'N'} why=The population does not clearly mention pectus excavatum or Nuss procedure. The intervention does not mention intercostal nerve cryoablation (INC). The design is an observational study, not a randomized controlled trial.\n",
      "       Title: Comparison of Unilateral Spinal Anesthesia and Popliteal Nerve Block for Postoperative Rebound Pain in Foot Surgeries - An Observational Study.\n",
      "       Abstract: Regional anesthesia techniques, such as unilateral spinal anesthesia and peripheral nerve blocks, are essential components of multimodal analgesia. Nonetheless, \"rebound pain,\" an abrupt increase in nociceptive intensity following the cessation of the block, is inadequately defined and may compromise patient satisfaction and functional recovery. This study aimed to compare postoperative pain profiles, the incidence of rebound pain, and patient satisfaction following popliteal sciatic nerve block...\n",
      "     [Screen] PMID 40920126 -> decision=exclude checklist={'P': False, 'I': False, 'O': True, 'D': False} why=The population is not adults, but children. The intervention (costoclavicular brachial plexus block) is not intercostal nerve cryoablation (INC). The design is observational, not randomized controlled trial.\n",
      "       Title: Incidence of Phrenic Nerve Palsy in Pediatric Costoclavicular Brachial Plexus Block.\n",
      "       Abstract: The costoclavicular brachial plexus block has gained relevance as a safe and effective regional anesthesia technique for upper limb orthopedic surgery in adults, but data in pediatric populations remain limited. This study aimed to evaluate the incidence of phrenic nerve palsy associated with CBPB in pediatric patients. We conducted a descriptive observational study in 30 children undergoing upper limb orthopedic surgery. Diaphragmatic excursion was assessed before and after the block using M-mo...\n",
      "     [Screen] PMID 40918613 -> decision=exclude checklist={'P': False, 'I': False, 'O': False, 'D': False} why=The abstract mentions minimally invasive repair of pectus excavatum and Nuss procedure but does not explicitly state that the study is a Randomized Controlled Trial (RCT).\n",
      "       Title: Best Evidence Summary for Perioperative Pain Management in Patients With Pectus Excavatum.\n",
      "       Abstract: Background: Pectus excavatum is a common congenital chest wall deformity that can lead to significant cardiopulmonary compression and psychological distress. The minimally invasive Nuss procedure is the standard treatment, but it often results in severe postoperative pain. Effective perioperative pain management is essential to enhance recovery and improve patient outcomes. Objectives: This study aimed to synthesize the most effective evidence on perioperative pain management in patients with pe...\n",
      "     [Screen] PMID 40916707 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'Y', 'D': 'N'} why=The population is not specified as adults, minimally invasive repair of pectus excavatum, Nuss procedure, or pectus excavatum surgery. The intervention (intercostal nerve cryoablation) is not studied. The design is a systematic review and meta-analysis, not a randomized controlled trial.\n",
      "       Title: Safety and Efficacy of Dexmedetomidine in Spinal Fusion: A Systematic Review and Meta-Analysis Dexmedetomidine in Spinal Fusion Surgery.\n",
      "       Abstract: Dexmedetomidine (DEX) has been proposed as an opioid-sparing adjunct after spinal fusion, but its efficacy across age groups is unclear. We conducted a systematic review and meta-analysis following PRISMA and registered in International Prospective Register of Systematic Reviews (PROSPERO) (CRD42024531252). Twelve studies (RCTs and cohorts; n=1,644) were included. Outcomes were postoperative pain (VAS), opioid consumption, and adverse events. DEX significantly reduced postoperative nausea and vo...\n",
      "     [Screen] PMID 40915105 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'Y', 'D': 'N'} why=The record does not clearly indicate the specified Population (adults, minimally invasive repair of pectus excavatum, Nuss procedure, pectus excavatum surgery) or Intervention (intercostal nerve cryoablation, INC, intraoperative analgesia). It focuses on epilepsy and Responsive Neurostimulation.\n",
      "       Title: Electrographic seizures on responsive neurostimulation: An early and objective measure of response to cenobamate.\n",
      "       Abstract: Responsive neurostimulation (RNS) electrocorticographic (ECoG) data may have a role in objectively assessing the efficacy of add-on antiseizure medications (ASMs). This retrospective, multicenter, observational, 24-week study is the first to report the effects of cenobamate on RNS-detected events (RDE). Patients included adults (≥18 years) with a history of recurrent focal seizures and implanted RNS who initiated adjunctive cenobamate ≥ 3 months after RNS implant between 4/1/20-12/15/23 and who ...\n",
      "     [Screen] PMID 40908130 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'N', 'D': 'N'} why=The record does not mention adults, minimally invasive repair of pectus excavatum, Nuss procedure, or pectus excavatum surgery. It focuses on intracranial aneurysms.\n",
      "       Title: Intracranial aneurysm embolization using Penumbra fill and finish coils: 1-year results of a prospective, real-world, multicenter SURF study.\n",
      "       Abstract: SURF was a prospective, multicenter, single-arm, observational study with core lab adjudication of radiographic data, assessing embolization of intracranial aneurysms (IAs) using WAVE Extra Soft Coils as part of SMART Coil System. Adults undergoing IA embolization with the SMART Coil System (Penumbra, Inc.) comprising >75% of implanted coils and WAVE as the final finishing coil were enrolled at 43 global centers. Primary outcomes were adequate occlusion (Raymond-Roy Occlusion Classification, RRO...\n",
      "     [Screen] PMID 40905294 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'Y', 'D': 'N'} why=The record does not clearly indicate the specified Population (adults, minimally invasive repair of pectus excavatum, Nuss procedure, pectus excavatum surgery) or Intervention (intercostal nerve cryoablation, INC, intraoperative analgesia).\n",
      "       Title: Postoperative Ketorolac Administration and Pseudoarthrosis Following Multilevel Posterior Cervical Decompression and Fusion: A Retrospective Cohort Study.\n",
      "       Abstract: The effect of perioperative ketorolac use after posterior cervical decompression and fusion (PCDF) remains unclear with ongoing concern regarding NSAID-induced pseudoarthrosis. This study investigates the association between postoperative ketorolac use and pseudoarthrosis after multilevel PCDF. This retrospective cohort study analyzed adults undergoing multilevel PCDF (2002-2024) using TriNetX. Patients were grouped by postoperative ketorolac within 48 hours versus acetaminophen only. Propensity...\n",
      "     [Screen] PMID 40904256 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'N', 'D': 'N'} why=The record does not mention adults, minimally invasive repair of pectus excavatum, Nuss procedure, or pectus excavatum surgery. It focuses on pulp therapy in pediatric dentistry.\n",
      "       Title: Comparison of YouTube, TikTok, and Instagram as digital sources for obtaining information about pulp therapy in primary and permanent teeth.\n",
      "       Abstract: This study aimed to compare the content, educational quality, and dependability of videos on Instagram, TikTok, and YouTube about pulp therapy (PT) in pediatric dentistry and endodontics. Three popular video sites, Instagram (Meta Platforms, Inc.,), TikTok (ByteDance Ltd.), and YouTube (Google LLC), were searched for PT content to analyze for compliance with the American Association of Endodontists and American Academy of Pediatric Dentistry guidelines for clinical endodontists and pediatric den...\n",
      "     [Screen] PMID 40896022 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'N', 'D': 'N'} why=The record does not mention the required population (adults, pectus excavatum surgery), intervention (intercostal nerve cryoablation, INC), or outcomes (postoperative opioid consumption, pain scores). It focuses on pressure ulcers in older adults and body composition.\n",
      "       Title: Effect of Body Composition on Supine Pressure Distribution in Bed: A Comparison Between Young and Older Adults.\n",
      "       Abstract: Pressure ulcers are a serious concern in older adults and are often caused by uneven pressure distribution in the supine position. While body mass index is commonly used in risk assessments, it may not reflect localized body pressure. Exploring the relationship between body composition and pressure distribution may help guide individualized prevention strategies. To investigate how body composition influences supine pressure distribution across different body regions and to clarify whether these...\n",
      "     [Screen] PMID 40895961 -> decision=exclude checklist={'P': False, 'I': False, 'O': False, 'D': False} why=The record does not mention minimally invasive repair of pectus excavatum, Nuss procedure, or pectus excavatum surgery. It also does not study intercostal nerve cryoablation (INC) or intraoperative analgesia.\n",
      "       Title: Epidemiological Study of Dengue Fever in a Tertiary Care Hospital in Dera Ismail Khan, Pakistan.\n",
      "       Abstract: Background Dengue fever significantly burdens healthcare systems, particularly in resource-limited settings such as Dera Ismail Khan, Khyber Pakhtunkhwa, Pakistan. Mufti Mehmood Memorial Teaching Hospital, the designated Dengue Isolation Unit in the region, continues to receive a steady influx of patients. This study analyzed the epidemiological profile of dengue cases admitted to the hospital to support public health planning and guide resource allocation. Methods This retrospective study was c...\n",
      "     [Screen] PMID 40882220 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'N', 'D': 'N'} why=The record does not clearly indicate the specified Population, Intervention, Outcome, or Design. It focuses on vaccine sentiment analysis on social media and surveys, not on medical procedures like pectus excavatum repair.\n",
      "       Title: Mapping Vaccine Sentiment by Analyzing Spanish-Language Social Media Posts and Survey-Based Public Opinion: Dual Methods Study.\n",
      "       Abstract: The internet and social media have been considered useful platforms for obtaining health information. However, critical and erroneous content about vaccines on social media has been associated with vaccination delays and refusal. This study aimed to examine how social networks influence access to and perceptions of vaccine-related information. We sought to (1) quantify the proportion of individuals engaging with vaccine-related content on social media and to characterize their demographic and be...\n",
      "     [Screen] PMID 40888903 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'Y', 'D': 'N'} why=The record does not clearly indicate the specified Population (adults, minimally invasive repair of pectus excavatum, Nuss procedure, pectus excavatum surgery) or Intervention (intercostal nerve cryoablation, INC, intraoperative analgesia). It focuses on pediatric patients undergoing tethered cord release surgery and a Quadroiliac Plane Block.\n",
      "       Title: Quadroiliac plane block for postoperative pain management in a pediatric patient undergoing tethered cord release surgery: a case report.\n",
      "       Abstract: Opioid-free analgesia is critical in pediatric patients with complex comorbidities to avoid adverse effects, such as respiratory depression. Patients with tethered cord syndrome (TCS), often presenting with conditions like spina bifida, renal impairment, and musculoskeletal deformities, pose unique perioperative challenges that demand alternative pain management strategies. We present the case of an 11-year-old male with spina bifida and chronic kidney disease (CKD) undergoing tethered cord rele...\n",
      "     [Screen] PMID 40882805 -> decision=exclude checklist={'P': 'N', 'I': 'N', 'O': 'Y', 'D': 'Y'} why=The population does not mention pectus excavatum or Nuss procedure. The intervention does not mention intercostal nerve cryoablation (INC).\n",
      "       Title: Effect of intravenous dexmedetomidine on sensory block duration in spinal anesthesia for lower limb surgery: A randomized controlled trial.\n",
      "       Abstract: To study the effect of Intravenous (IV) dexmedetomidine during spinal anesthesia on duration of sensory block and postoperative analgesia in patients undergoing lower limb orthopedic surgery. Prospective randomized double blind controlled trial. Patients in intervention (DX) group received 0.5 mcg.kg-1 IV dexmedetomidine over 10 min. Spinal anesthesia was administered and an infusion of dexmedetomidine 0.5 mcg.kg-1.h-1 was given throughout surgery. Onset time of sensory and motor block, maximum ...\n",
      "     [Screen] PMID 40866940 -> decision=exclude checklist={'P': 'Y', 'I': 'N', 'O': 'N', 'D': 'N'} why=The record does not clearly indicate the specified Intervention (intercostal nerve cryoablation, INC, intraoperative analgesia) or Outcomes (postoperative opioid consumption, in-hospital opioid use, discharge opioid use, pain scores, 0-7 day pain assessment). It focuses on bioelectrical impedance analysis and ultrasound for body composition assessment.\n",
      "       Title: Association between bioelectrical impedance analysis parameters and muscle-related measurements obtained using a non-diagnostic ultrasound-based technique.\n",
      "       Abstract: An in-depth analysis of body composition (BC) is fundamental for the assessment of nutritional status. A multifactorial approach is important for this assessment, integrating parameters and measurements obtained through different techniques, most notably bioelectrical impedance analysis (BIA) and ultrasound. In this study, we aimed to evaluate associations between BIA parameters and ultrasound measurements. The BC of 993 adults (539 women and 454 men) was evaluated using BIA (BIA 101 RJL, Akern ...\n",
      "     [Screen] PMID 40860211 -> decision=exclude checklist={'P': False, 'I': False, 'O': True, 'D': True} why=The population is total knee arthroplasty patients, not pectus excavatum patients. The intervention is local anesthesia, not intercostal nerve cryoablation. While outcomes are mentioned (pain scores, opioid consumption), the study design is not a randomized controlled trial for pectus excavatum surgery.\n",
      "       Title: Analgesic effect of local anesthesia in total knee arthroplasty: protocol of a randomized controlled clinical trial.\n",
      "       Abstract: Postoperative pain management remains a critical determinant of functional recovery following total knee arthroplasty (TKA). While local infiltration analgesia (LIA) is commonly employed, its clinical utility is limited by inconsistent analgesic duration (median duration of 8-12 hours), technical variability among surgeons, and systemic toxicity risks associated with high-volume injections. This phase II randomized controlled trial evaluates a dual-optimization strategy combining anatomic mappin...\n",
      "     [Screen] PMID 40850780 -> decision=exclude checklist={'P': False, 'I': False, 'O': False, 'D': False} why=The record does not mention minimally invasive repair of pectus excavatum, Nuss procedure, or pectus excavatum surgery. It also does not mention intercostal nerve cryoablation (INC) or intraoperative analgesia.\n",
      "       Title: [Characteristics of respondents to Internet survey collecting screenshots of step-count on iPhone: A cross-sectional study].\n",
      "       Abstract: Objectives　Recently, step-count, as an objective measure of physical activity, has become available on smartphones and is recorded by default on an iPhone (Apple Inc.). Using screenshot images of the step-count graph of the iPhone healthcare application, an image analysis tool was developed to enable a large-scale and retrospective physical activity assessment. However, the characteristics of the respondents involved in the collection of such step-count images remain unclear. We aimed to investi...\n",
      "16:33:37  [LM] Switching models: waiting 3.1s for TTL auto-evict...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://127.0.0.1:1234/v1/chat/completions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mask_json\u001b[39m\u001b[34m(model, system, user, template, stop_on_end)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     content = \u001b[43m_lm_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTRICT_JSON_RULES\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEND_JSON\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop_on_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(extract_json_like(content))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36m_lm_call\u001b[39m\u001b[34m(model, messages, stop)\u001b[39m\n\u001b[32m     95\u001b[39m r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m _last_model_used[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 400 Client Error: Bad Request for url: http://127.0.0.1:1234/v1/chat/completions",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 798\u001b[39m\n\u001b[32m    787\u001b[39m USER_NLQ = \u001b[33m\"\"\"\u001b[39m\u001b[33mPopulation = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[33mIntervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[33m(the intervention of interest is INC, not the surgery).\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    794\u001b[39m \u001b[33mYear_min = 2015. Languages = English, Portuguese, Spanish.\u001b[39m\n\u001b[32m    795\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m798\u001b[39m     \u001b[43msniff_validate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUSER_NLQ\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 774\u001b[39m, in \u001b[36msniff_validate_engine\u001b[39m\u001b[34m(USER_NLQ, kb_path)\u001b[39m\n\u001b[32m    772\u001b[39m state1_protocol_lockdown(st, USER_NLQ)\n\u001b[32m    773\u001b[39m state2_universe(st)\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[43mstate3_ground_truth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m state35_plausibility(st)\n\u001b[32m    776\u001b[39m state4_strategy(st)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 535\u001b[39m, in \u001b[36mstate3_ground_truth\u001b[39m\u001b[34m(st)\u001b[39m\n\u001b[32m    533\u001b[39m includes = []\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m recs[:SCREEN_SAMPLE_MAX]:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     js = \u001b[43mask_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSCREENER_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscreener_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscreener_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlocked_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCREENER_TEMPLATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m     \u001b[38;5;66;03m# Print each screened record title + abstract (compact)\u001b[39;00m\n\u001b[32m    537\u001b[39m     title = (r[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).strip()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mask_json\u001b[39m\u001b[34m(model, system, user, template, stop_on_end)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m         \u001b[38;5;66;03m# Try a repair pass\u001b[39;00m\n\u001b[32m    153\u001b[39m         rep_user = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mTEMPLATE_JSON:\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mjson.dumps(template,\u001b[38;5;250m \u001b[39mensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m,\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    155\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m \u001b[33mEND_JSON\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         repaired = \u001b[43m_lm_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREPAIR_SYSTEM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mREPAIR_SYSTEM\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mrep_user\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTRICT_JSON_RULES\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEND_JSON\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop_on_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(extract_json_like(repaired))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36m_lm_call\u001b[39m\u001b[34m(model, messages, stop)\u001b[39m\n\u001b[32m     94\u001b[39m     body[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m] = stop\n\u001b[32m     95\u001b[39m r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m _last_model_used[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] = model\n\u001b[32m     98\u001b[39m _last_model_used[\u001b[33m\"\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m] = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: http://127.0.0.1:1234/v1/chat/completions"
     ]
    }
   ],
   "source": [
    "# SNIPPET: Sniff Validation Engine (state-machine PoC, resilient, single cell)\n",
    "# - Replaces linear \"broad/focused\" approach with a Universe Query + Validated Filters\n",
    "# - Adds remediation loops and a senior \"plausibility\" guardrail to stop GIGO cascades\n",
    "# - Strict LM Studio model TTL to auto-evict idle models and avoid CPU fallback\n",
    "# - Verbose, human-readable report printed inline + compact artifacts on disk\n",
    "#\n",
    "# CONFIGURE before running:\n",
    "#   - LM Studio server: http://127.0.0.1:1234 (or set LMSTUDIO_BASE)\n",
    "#   - QWEN_MODEL (reasoning/senior)  e.g., \"qwen/qwen3-4b\"\n",
    "#   - SCREENER_MODEL (fast screener) e.g., \"gemma-3n-e2b-it\" or a small Qwen\n",
    "#   - Ensure LM Studio Settings > Developer > \"JIT load models\" ON and \"Auto-evict\" ON\n",
    "#   - This script sets per-request \"ttl\" (seconds) so models unload ~immediately after use\n",
    "#\n",
    "# ENV VARS (optional):\n",
    "#   LMSTUDIO_BASE, QWEN_MODEL, SCREENER_MODEL\n",
    "#   ENTREZ_EMAIL, ENTREZ_API_KEY, HTTP_TIMEOUT\n",
    "#   LM_TTL_SECONDS  (default 5)  <-- per-request idle TTL for model auto-eviction\n",
    "#\n",
    "# INPUTS:\n",
    "#   - USER_NLQ (your natural-language question)\n",
    "#   - Optional: system_knowledge_base.json (KB of valid values). If missing, a safe default is used.\n",
    "#\n",
    "# OUTPUTS (folder: sniff_out/):\n",
    "#   - sniff_report.txt   : human-readable full log + final validated strategy\n",
    "#   - sniff_artifacts.json : machine-readable state, protocol, filters, ground-truth, MeSH vernaculum, warnings, embed string\n",
    "#   - ground_truth.tsv   : quick table of INCLUDEs (pmid, title, decisions)\n",
    "#\n",
    "# NOTE:\n",
    "#   - We DO NOT set max_tokens anywhere (to avoid premature truncation).\n",
    "#   - We DO set per-request {\"ttl\": LM_TTL_SECONDS} so LM Studio unloads models after ~idle.\n",
    "#   - When switching models, we wait (ttl + 0.5s) to let auto-evict complete before loading the next.\n",
    "#   - PubMed E-utilities are used for counts/records (esearch/efetch); no scraping.\n",
    "\n",
    "import os, json, time, re, pathlib, textwrap, random\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# ----------------------------\n",
    "# Config & I/O\n",
    "# ----------------------------\n",
    "LMSTUDIO_BASE   = os.getenv(\"LMSTUDIO_BASE\", \"http://127.0.0.1:1234\").rstrip(\"/\")\n",
    "QWEN_MODEL      = os.getenv(\"QWEN_MODEL\", \"qwen3-4b@q6_k\")\n",
    "SCREENER_MODEL  = os.getenv(\"SCREENER_MODEL\", \"gemma-3n-e4b-it@q5_k_m\")  # can also be a small Qwen\n",
    "HTTP_TIMEOUT    = int(os.getenv(\"HTTP_TIMEOUT\", \"300\"))\n",
    "LM_TTL_SECONDS  = int(os.getenv(\"LM_TTL_SECONDS\", \"5\"))  # enforce fast auto-evict (5s idle)\n",
    "\n",
    "ENTREZ_EMAIL    = os.getenv(\"ENTREZ_EMAIL\", \"you@example.com\")\n",
    "ENTREZ_API_KEY  = os.getenv(\"ENTREZ_API_KEY\", \"\")\n",
    "\n",
    "OUT_DIR = pathlib.Path(\"sniff_out\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Universe sizing windows (can be relaxed via KB)\n",
    "UNIVERSE_TARGET = (50, 10000)\n",
    "UNIVERSE_HARD_MIN = 25      # below this -> terminate (to avoid polluted tiny \"universe\")\n",
    "GROUND_TRUTH_MIN = 3        # minimum unequivocal INCLUDEs required\n",
    "TOP_FETCH = 30              # how many top hits to fetch for ground-truth discovery\n",
    "SCREEN_SAMPLE_MAX = 30      # upper bound, we iterate deterministically over TOP_FETCH\n",
    "\n",
    "# PubMed API\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "HEADERS = {\"User-Agent\": \"sniff-validation-engine/0.3 (+local)\", \"Accept\": \"application/json\"}\n",
    "\n",
    "# ----------------------------\n",
    "# LM Studio helpers (OpenAI-compatible)\n",
    "# ----------------------------\n",
    "_last_model_used = {\"name\": None, \"t\": 0.0}\n",
    "\n",
    "def _lm_call(model: str, messages: List[Dict[str, str]], stop: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Raw LM Studio call to /v1/chat/completions with per-request TTL to auto-evict.\n",
    "    We intentionally DO NOT pass max_tokens (let the model decide).\n",
    "    \"\"\"\n",
    "    # If switching models, give TTL time for auto-evict of the previous one to avoid CPU fallback.\n",
    "    now = time.time()\n",
    "    if _last_model_used[\"name\"] and _last_model_used[\"name\"] != model:\n",
    "        delta = now - _last_model_used[\"t\"]\n",
    "        wait_need = LM_TTL_SECONDS + 0.5 - delta\n",
    "        if wait_need > 0:\n",
    "            print(f\"{time.strftime('%H:%M:%S')}  [LM] Switching models: waiting {wait_need:.1f}s for TTL auto-evict...\")\n",
    "            time.sleep(wait_need)\n",
    "\n",
    "    url = f\"{LMSTUDIO_BASE}/v1/chat/completions\"\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": False,\n",
    "        \"ttl\": LM_TTL_SECONDS\n",
    "    }\n",
    "    if stop:\n",
    "        body[\"stop\"] = stop\n",
    "    r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    _last_model_used[\"name\"] = model\n",
    "    _last_model_used[\"t\"] = time.time()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "_BEGIN = re.compile(r\"BEGIN_JSON\\s*\", re.I)\n",
    "_END   = re.compile(r\"\\s*END_JSON\", re.I)\n",
    "FENCE  = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)```\", re.I)\n",
    "\n",
    "def _sanitize_json_str(s: str) -> str:\n",
    "    s = s.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n",
    "    s = re.sub(r\",\\s*(\\}|\\])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_json_like(txt: str) -> str:\n",
    "    blocks = []\n",
    "    pos=0\n",
    "    while True:\n",
    "        m1 = _BEGIN.search(txt, pos)\n",
    "        if not m1: break\n",
    "        m2 = _END.search(txt, m1.end())\n",
    "        if not m2: break\n",
    "        blocks.append(txt[m1.end():m2.start()])\n",
    "        pos = m2.end()\n",
    "    if blocks:\n",
    "        return _sanitize_json_str(blocks[-1])\n",
    "    fences = FENCE.findall(txt)\n",
    "    if fences:\n",
    "        return _sanitize_json_str(fences[-1])\n",
    "\n",
    "    s = txt\n",
    "    last_obj=None; stack=0; start=None\n",
    "    for i,ch in enumerate(s):\n",
    "        if ch=='{':\n",
    "            if stack==0: start=i\n",
    "            stack+=1\n",
    "        elif ch=='}':\n",
    "            if stack>0:\n",
    "                stack-=1\n",
    "                if stack==0 and start is not None:\n",
    "                    last_obj=s[start:i+1]\n",
    "    if last_obj:\n",
    "        return _sanitize_json_str(last_obj)\n",
    "    raise ValueError(\"No JSON-like content found\")\n",
    "\n",
    "REPAIR_SYSTEM = \"You repair malformed JSON to exactly match template keys. Return ONE JSON object only, wrapped between BEGIN_JSON/END_JSON.\"\n",
    "STRICT_JSON_RULES = (\n",
    "  \"Return ONLY one JSON object. No analysis, no preface. \"\n",
    "  \"Wrap exactly with:\\nBEGIN_JSON\\n{...}\\nEND_JSON\"\n",
    ")\n",
    "\n",
    "def ask_json(model: str, system: str, user: str, template: Dict[str,Any], stop_on_end=True) -> Dict[str,Any]:\n",
    "    try:\n",
    "        content = _lm_call(model, [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\": user + \"\\n\\n\" + STRICT_JSON_RULES}], stop=[\"END_JSON\"] if stop_on_end else None)\n",
    "        return json.loads(extract_json_like(content))\n",
    "    except Exception as e:\n",
    "        # Try a repair pass\n",
    "        rep_user = f\"\"\"TEMPLATE_JSON:\n",
    "{json.dumps(template, ensure_ascii=False, indent=2)}\n",
    "\n",
    "BAD_OUTPUT:\n",
    "{content if 'content' in locals() else str(e)}\n",
    "\n",
    "TASK: Output valid JSON matching TEMPLATE_JSON keys (fill missing with empty arrays/strings). No prose.\n",
    "\n",
    "BEGIN_JSON\n",
    "{{}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "        repaired = _lm_call(REPAIR_SYSTEM, [{\"role\":\"system\",\"content\":REPAIR_SYSTEM},{\"role\":\"user\",\"content\":rep_user + \"\\n\\n\" + STRICT_JSON_RULES}], stop=[\"END_JSON\"] if stop_on_end else None)\n",
    "        return json.loads(extract_json_like(repaired))\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge Base (KB) load\n",
    "# ----------------------------\n",
    "DEFAULT_KB = {\n",
    "    \"publication_types\": [\n",
    "        \"Randomized Controlled Trial\",\"Clinical Trial\",\"Controlled Clinical Trial\",\n",
    "        \"Comparative Study\",\"Cohort Studies\",\"Case-Control Studies\",\"Observational Study\",\n",
    "        \"Systematic Review\",\"Meta-Analysis\",\"Network Meta-Analysis\"\n",
    "    ],\n",
    "    \"languages\": [\"english\",\"portuguese\",\"spanish\"],\n",
    "    \"universe_window\": {\"min\": 50, \"max\": 10000},\n",
    "    \"precision_window\": {\"min\": 5, \"max\": 2000},\n",
    "    \"design_filters\": {\n",
    "        \"RCT\": '(\"Randomized Controlled Trial\"[Publication Type])',\n",
    "        \"Comparative\": '(\"Comparative Study\"[Publication Type] OR \"Case-Control Studies\"[Publication Type] OR \"Cohort Studies\"[Publication Type])',\n",
    "        \"AnyTrial\": '(\"Randomized Controlled Trial\"[Publication Type] OR \"Clinical Trial\"[Publication Type] OR \"Controlled Clinical Trial\"[Publication Type])'\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_kb(path=\"system_knowledge_base.json\") -> Dict[str,Any]:\n",
    "    p = pathlib.Path(path)\n",
    "    if p.exists():\n",
    "        try:\n",
    "            js = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "            # merge defaults for missing keys\n",
    "            merged = json.loads(json.dumps(DEFAULT_KB))\n",
    "            for k,v in js.items():\n",
    "                merged[k]=v\n",
    "            return merged\n",
    "        except Exception:\n",
    "            return DEFAULT_KB\n",
    "    return DEFAULT_KB\n",
    "\n",
    "# ----------------------------\n",
    "# PubMed helpers\n",
    "# ----------------------------\n",
    "def esearch_ids(term: str, retmax=5000) -> Tuple[int, List[str]]:\n",
    "    p = {\n",
    "        \"db\":\"pubmed\",\"retmode\":\"json\",\"term\":term,\n",
    "        \"retmax\":min(retmax, 5000),\n",
    "        \"email\":ENTREZ_EMAIL,\"usehistory\":\"y\"\n",
    "    }\n",
    "    if ENTREZ_API_KEY: p[\"api_key\"]=ENTREZ_API_KEY\n",
    "    r = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params=p, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    js = r.json().get(\"esearchresult\", {})\n",
    "    count = int(js.get(\"count\",\"0\"))\n",
    "    webenv = js.get(\"webenv\"); qk = js.get(\"querykey\")\n",
    "    ids=[]\n",
    "    if count and webenv and qk:\n",
    "        r2 = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params={\n",
    "            \"db\":\"pubmed\",\"retmode\":\"json\",\"retmax\":min(count, retmax),\n",
    "            \"retstart\":0,\"email\":ENTREZ_EMAIL,\"WebEnv\":webenv,\"query_key\":qk,\n",
    "            **({\"api_key\":ENTREZ_API_KEY} if ENTREZ_API_KEY else {})\n",
    "        }, timeout=HTTP_TIMEOUT)\n",
    "        r2.raise_for_status()\n",
    "        ids = r2.json().get(\"esearchresult\",{}).get(\"idlist\",[])\n",
    "    return count, [str(x) for x in ids]\n",
    "\n",
    "def efetch_xml(pmids: List[str]) -> str:\n",
    "    if not pmids: return \"\"\n",
    "    params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"id\":\",\".join(pmids),\"email\":ENTREZ_EMAIL}\n",
    "    if ENTREZ_API_KEY: params[\"api_key\"]=ENTREZ_API_KEY\n",
    "    r = requests.get(f\"{EUTILS}/efetch.fcgi\", headers={\"User-Agent\":\"sniff-validation-engine/0.3\"}, params=params, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_pubmed_xml(xml_text: str) -> List[Dict[str,Any]]:\n",
    "    out=[]\n",
    "    if not xml_text.strip(): return out\n",
    "    root = ET.fromstring(xml_text)\n",
    "    def _join(node):\n",
    "        if node is None: return \"\"\n",
    "        try: return \"\".join(node.itertext())\n",
    "        except Exception: return node.text or \"\"\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = art.findtext(\".//PMID\") or \"\"\n",
    "        title = _join(art.find(\".//ArticleTitle\")).strip()\n",
    "        abs_nodes = art.findall(\".//Abstract/AbstractText\")\n",
    "        abstract = \" \".join(_join(n).strip() for n in abs_nodes) if abs_nodes else \"\"\n",
    "        year = None\n",
    "        for path in (\".//ArticleDate/Year\",\".//PubDate/Year\",\".//DateCreated/Year\",\".//PubDate/MedlineDate\"):\n",
    "            s = art.findtext(path)\n",
    "            if s:\n",
    "                m = re.search(r\"\\d{4}\", s)\n",
    "                if m: year = int(m.group(0)); break\n",
    "        lang = art.findtext(\".//Language\") or None\n",
    "        pubtypes = [pt.text for pt in art.findall(\".//PublicationTypeList/PublicationType\") if pt.text]\n",
    "        mesh = [mh.findtext(\"./DescriptorName\") for mh in art.findall(\".//MeshHeadingList/MeshHeading\") if mh.findtext(\"./DescriptorName\")]\n",
    "        out.append({\"pmid\":pmid,\"title\":title,\"abstract\":abstract,\"year\":year,\"language\":lang,\n",
    "                    \"publication_types\":pubtypes,\"mesh\":mesh})\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Term utilities\n",
    "# ----------------------------\n",
    "def or_block(terms: List[str], field=\"tiab\") -> str:\n",
    "    toks=[]\n",
    "    for t in terms:\n",
    "        t=t.strip()\n",
    "        if not t: continue\n",
    "        if \" \" in t or \"-\" in t:\n",
    "            toks.append(f\"\\\"{t}\\\"[{field}]\")\n",
    "        else:\n",
    "            toks.append(f\"{t}[{field}]\")\n",
    "    if not toks: return \"\"\n",
    "    return \"(\" + \" OR \".join(toks) + \")\"\n",
    "\n",
    "def build_universe_query(P_terms: List[str], I_terms: List[str], anchors: List[str]) -> str:\n",
    "    P = or_block(P_terms, \"tiab\")\n",
    "    I = or_block(I_terms, \"tiab\")\n",
    "    A = or_block(anchors, \"tiab\") if anchors else \"\"\n",
    "    if not P or not I: return \"\"\n",
    "    q = f\"{P} AND {I}\"\n",
    "    if A: q = f\"{q} AND {A}\"\n",
    "    return q\n",
    "\n",
    "# ----------------------------\n",
    "# LM prompts\n",
    "# ----------------------------\n",
    "PROTOCOL_TEMPLATE = {\n",
    "    \"population_terms\": [], \"intervention_terms\": [], \"comparators_terms\": [], \"outcomes_terms\": [],\n",
    "    \"must_have\": [], \"avoid\": [], \"designs_preference\": \"\", \"languages\": [], \"year_min\": 2015\n",
    "}\n",
    "\n",
    "def protocol_system(kb: Dict[str,Any]) -> str:\n",
    "    return \"You convert an NLQ into a strict, small protocol using ONLY allowed values from the provided knowledge base. Return JSON only.\"\n",
    "\n",
    "def protocol_user(nlq: str, kb: Dict[str,Any]) -> str:\n",
    "    return f\"\"\"NATURAL LANGUAGE QUESTION (NLQ):\n",
    "<<<\n",
    "{nlq}\n",
    ">>>\n",
    "\n",
    "KNOWLEDGE BASE (allowed values):\n",
    "publication_types = {kb[\"publication_types\"]}\n",
    "languages = {kb[\"languages\"]}\n",
    "universe_window = {kb.get(\"universe_window\",{})}\n",
    "\n",
    "TASK:\n",
    "- Parse the NLQ into a compact protocol with arrays of short phrases for population_terms and intervention_terms (include standard synonyms/acronyms), comparators_terms, outcomes_terms.\n",
    "- must_have: 2–6 anchors that enforce topicality.\n",
    "- avoid: 0–6 obvious off-topic terms to avoid.\n",
    "- designs_preference: Choose ONE value that best matches the NLQ intent, but it MUST be one of publication_types above (e.g., \"Randomized Controlled Trial\" or \"Comparative Study\").\n",
    "- languages: Choose ONLY from languages above (subset).\n",
    "- year_min: integer, from NLQ if stated else 2015.\n",
    "\n",
    "IMPORTANT:\n",
    "- You MUST select designs_preference from KB publication_types (no new strings).\n",
    "- Keep all lists between 2 and 10 items when possible.\n",
    "- Return only:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\n",
    "  \"population_terms\": [],\n",
    "  \"intervention_terms\": [],\n",
    "  \"comparators_terms\": [],\n",
    "  \"outcomes_terms\": [],\n",
    "  \"must_have\": [],\n",
    "  \"avoid\": [],\n",
    "  \"designs_preference\": \"\",\n",
    "  \"languages\": [],\n",
    "  \"year_min\": 2015\n",
    "}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "SCREENER_TEMPLATE = {\n",
    "  \"pmid\":\"\", \"checklist\":{\"P\":False,\"I\":False,\"O\":False,\"D\":False},\n",
    "  \"decision\":\"include|exclude\", \"why\":\"\", \"mesh_roles\":[]\n",
    "}\n",
    "\n",
    "def screener_system() -> str:\n",
    "    return (\"You are a STRICT effects screener. Decide ONLY on definitive INCLUDEs using a checklist. \"\n",
    "            \"If ANY checklist item is N=false, decision MUST be 'exclude'. Return JSON only.\")\n",
    "\n",
    "def screener_user(protocol: Dict[str,Any], record: Dict[str,Any]) -> str:\n",
    "    pop = \", \".join(protocol.get(\"population_terms\",[]))\n",
    "    itv = \", \".join(protocol.get(\"intervention_terms\",[]))\n",
    "    outs= \", \".join(protocol.get(\"outcomes_terms\",[]))\n",
    "    design = protocol.get(\"designs_preference\",\"\")\n",
    "    return f\"\"\"Protocol (STRICT):\n",
    "- Population (must mention): {pop}\n",
    "- Intervention (must mention): {itv}\n",
    "- Outcomes (must mention at least one): {outs}\n",
    "- Design (must match): {design}\n",
    "\n",
    "Record:\n",
    "PMID: {record['pmid']}\n",
    "Title: {record['title']}\n",
    "Abstract: {record['abstract']}\n",
    "Year: {record['year']}\n",
    "Language: {record['language']}\n",
    "PubTypes: {record['publication_types']}\n",
    "MeSH: {record['mesh']}\n",
    "\n",
    "CHECKLIST:\n",
    "- P: Does Title/Abstract clearly indicate the specified Population/setting? (Y/N)\n",
    "- I: Does it clearly study the specified Intervention (not just mention)? (Y/N)\n",
    "- O: Is at least one specified Outcome measured/reported? (Y/N)\n",
    "- D: Is the Design compatible with the required design above (same or stricter)? (Y/N)\n",
    "\n",
    "DECISION RULE: Only if P=Y and I=Y and O=Y and D=Y -> decision = \"include\". Else \"exclude\".\n",
    "If INCLUDE, also categorize each MeSH term as P/I/C/O/G/X briefly.\n",
    "\n",
    "Return ONLY:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\n",
    "  \"pmid\":\"{record['pmid']}\",\n",
    "  \"checklist\":{{\"P\":false,\"I\":false,\"O\":false,\"D\":false}},\n",
    "  \"decision\":\"include|exclude\",\n",
    "  \"why\":\"\",\n",
    "  \"mesh_roles\":[{{\"mesh\":\"\", \"role\":\"P|I|C|O|G|X\"}}]\n",
    "}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "PLAUS_SYS = \"You are a senior validator. You spot-check plausibility against the core topic. Return JSON only.\"\n",
    "def plaus_user(protocol: Dict[str,Any], record: Dict[str,Any]) -> str:\n",
    "    core = f\"Pectus excavatum MIRPE/Nuss + intercostal nerve cryoablation.\"\n",
    "    return f\"\"\"A junior screener marked this as INCLUDE. Perform a quick sanity check.\n",
    "\n",
    "Core topic required: {core}\n",
    "\n",
    "Record:\n",
    "PMID: {record['pmid']}\n",
    "Title: {record['title']}\n",
    "Abstract: {record['abstract']}\n",
    "\n",
    "Answer with PASS if the core topic obviously fits, else FAIL.\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{record['pmid']}\", \"plausibility\":\"PASS|FAIL\", \"note\":\"\"}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "REMEDIATION_SCOPE_SYS = \"You are a retrieval surgeon. Fix scope issues concisely.\"\n",
    "def remediation_scope_user(query: str, count: int, protocol: Dict[str,Any], window: Tuple[int,int]) -> str:\n",
    "    return f\"\"\"Universe query is out-of-window {window} with count={count}.\n",
    "Protocol anchors: must_have={protocol.get('must_have',[])}\n",
    "Population terms: {protocol.get('population_terms',[])}\n",
    "Intervention terms: {protocol.get('intervention_terms',[])}\n",
    "\n",
    "TASK:\n",
    "- If too NARROW: suggest broader synonyms to ADD (P_add[], I_add[]) and up to 2 anchors to KEEP.\n",
    "- If too BROAD: suggest up to 3 anchors to ENFORCE (A_enforce[]).\n",
    "Keep total suggestions <= 8 terms. Use terse surface forms.\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"P_add\":[], \"I_add\":[], \"A_enforce\":[]}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "REMEDIATION_STRAT_SYS = \"You are a strategy fixer. Choose ONE edit operation. Return JSON only.\"\n",
    "def remediation_strat_user(filters: Dict[str,str], stats: Dict[str,Any]) -> str:\n",
    "    return f\"\"\"Strategy failure snapshot:\n",
    "filters = {filters}\n",
    "stats = {stats}\n",
    "\n",
    "Choose one fix:\n",
    "- DROP_TERM: remove one noisy topic term (provide exact term)\n",
    "- ADD_ANCHOR: add one anchor term (P or I term) as TIAB anchor\n",
    "- BROADEN_DESIGN_FILTER: switch to a less restrictive design filter (e.g., AnyTrial or Comparative)\n",
    "\n",
    "Return:\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"op\":\"DROP_TERM|ADD_ANCHOR|BROADEN_DESIGN_FILTER\",\"term\":\"\"}}\n",
    "END_JSON\n",
    "\"\"\"\n",
    "\n",
    "EMBED_SYS = \"You distill a validated protocol into a single, ~200-char research question string suitable for dense embedding.\"\n",
    "def embed_user(protocol: Dict[str,Any], vernac: Dict[str,Any]) -> str:\n",
    "    return f\"\"\"Protocol:\n",
    "{json.dumps(protocol, ensure_ascii=False)}\n",
    "\n",
    "Key vocabulary (MeSH/lexical):\n",
    "{json.dumps(vernac, ensure_ascii=False)}\n",
    "\n",
    "Return ONLY one compact sentence (~200 chars) capturing the question with the most critical tokens (but readable).\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# State Machine\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class EngineState:\n",
    "    kb: Dict[str,Any]\n",
    "    report_lines: List[str] = field(default_factory=list)\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "    locked_protocol: Dict[str,Any] = field(default_factory=dict)\n",
    "    universe_query: str = \"\"\n",
    "    universe_count: int = 0\n",
    "    ground_truth: List[Dict[str,Any]] = field(default_factory=list) # records of INCLUDE (after plausibility)\n",
    "    mesh_vernac: Dict[str,List[str]] = field(default_factory=dict)\n",
    "    recommended_filters: Dict[str,str] = field(default_factory=dict)\n",
    "    final_strategy_count: int = 0\n",
    "    include_pmids: List[str] = field(default_factory=list)\n",
    "    plaus_fails: List[str] = field(default_factory=list)\n",
    "\n",
    "    def log(self, msg:str):\n",
    "        print(msg)\n",
    "        self.report_lines.append(msg)\n",
    "\n",
    "# ---- State 1: Protocol Lockdown\n",
    "def state1_protocol_lockdown(st: EngineState, nlq: str):\n",
    "    st.log(f\"{time.strftime('%H:%M:%S')}  [S1] Protocol lockdown...\")\n",
    "    template = PROTOCOL_TEMPLATE\n",
    "    js = ask_json(QWEN_MODEL, protocol_system(st.kb), protocol_user(nlq, st.kb), template)\n",
    "    # Sanitize: enforce designs_preference is in KB\n",
    "    if js.get(\"designs_preference\") not in st.kb[\"publication_types\"]:\n",
    "        # fallback: choose RCT if present else Comparative\n",
    "        js[\"designs_preference\"] = \"Randomized Controlled Trial\" if \"Randomized Controlled Trial\" in st.kb[\"publication_types\"] else st.kb[\"publication_types\"][0]\n",
    "    # Enforce languages subset\n",
    "    js[\"languages\"] = [l for l in js.get(\"languages\",[]) if l in st.kb[\"languages\"]] or st.kb[\"languages\"]\n",
    "    st.locked_protocol = js\n",
    "    st.log(\"  [S1] Locked protocol:\")\n",
    "    st.log(\"    \" + json.dumps(st.locked_protocol, ensure_ascii=False))\n",
    "\n",
    "# ---- State 2: Universe Definition & Sizing\n",
    "def apply_remediation_to_universe(protocol: Dict[str,Any], query: str, sugg: Dict[str,Any]) -> str:\n",
    "    P = list(protocol.get(\"population_terms\",[]))\n",
    "    I = list(protocol.get(\"intervention_terms\",[]))\n",
    "    if sugg.get(\"P_add\"):\n",
    "        P = (P + [t for t in sugg[\"P_add\"] if t])[:12]\n",
    "    if sugg.get(\"I_add\"):\n",
    "        I = (I + [t for t in sugg[\"I_add\"] if t])[:12]\n",
    "    anchors = sugg.get(\"A_enforce\", protocol.get(\"must_have\", []))\n",
    "    return build_universe_query(P, I, anchors)\n",
    "\n",
    "def state2_universe(st: EngineState):\n",
    "    st.log(f\"{time.strftime('%H:%M:%S')}  [S2] Universe definition & sizing...\")\n",
    "    P = st.locked_protocol.get(\"population_terms\",[])\n",
    "    I = st.locked_protocol.get(\"intervention_terms\",[])\n",
    "    A = st.locked_protocol.get(\"must_have\",[])\n",
    "    q = build_universe_query(P, I, A)\n",
    "    wmin, wmax = st.kb.get(\"universe_window\",{}).get(\"min\", UNIVERSE_TARGET[0]), st.kb.get(\"universe_window\",{}).get(\"max\", UNIVERSE_TARGET[1])\n",
    "\n",
    "    tries=0; max_tries=2\n",
    "    while True:\n",
    "        cnt, _ids = esearch_ids(q, retmax=TOP_FETCH)\n",
    "        st.log(f\"   [Universe] try={tries} count={cnt} window=({wmin}, {wmax}) query={q}\")\n",
    "        if wmin <= cnt <= wmax:\n",
    "            st.universe_query = q; st.universe_count = cnt\n",
    "            break\n",
    "        if tries >= max_tries:\n",
    "            st.warnings.append(f\"Universe out of window after {tries+1} tries (count={cnt}). Proceeding cautiously.\")\n",
    "            st.universe_query = q; st.universe_count = cnt\n",
    "            break\n",
    "        # Remediate via LLM\n",
    "        sugg = ask_json(QWEN_MODEL, REMEDIATION_SCOPE_SYS, remediation_scope_user(q, cnt, st.locked_protocol, (wmin,wmax)), {\"P_add\":[],\"I_add\":[],\"A_enforce\":[]})\n",
    "        q = apply_remediation_to_universe(st.locked_protocol, q, sugg)\n",
    "        tries += 1\n",
    "\n",
    "    if st.universe_count < UNIVERSE_HARD_MIN:\n",
    "        st.log(\"   [Universe] HARD FAIL: universe too small -> terminate.\")\n",
    "        raise SystemExit(\"Universe too small; refine NLQ or protocol.\")\n",
    "\n",
    "# ---- State 3: Ground Truth Discovery (strict screener)\n",
    "def state3_ground_truth(st: EngineState):\n",
    "    st.log(f\"{time.strftime('%H:%M:%S')}  [S3] Ground-truth discovery & protocol validation...\")\n",
    "    cnt, ids = esearch_ids(st.universe_query, retmax=TOP_FETCH)\n",
    "    recs = parse_pubmed_xml(efetch_xml(ids))\n",
    "    includes = []\n",
    "    for r in recs[:SCREEN_SAMPLE_MAX]:\n",
    "        js = ask_json(SCREENER_MODEL, screener_system(), screener_user(st.locked_protocol, r), SCREENER_TEMPLATE)\n",
    "        # Print each screened record title + abstract (compact)\n",
    "        title = (r['title'] or \"\").strip()\n",
    "        abstract = (r['abstract'] or \"\").strip()\n",
    "        st.log(f\"     [Screen] PMID {r['pmid']} -> decision={js.get('decision')} checklist={js.get('checklist')} why={js.get('why','')}\")\n",
    "        st.log(\"       Title: \" + title[:240])\n",
    "        if abstract:\n",
    "            st.log(\"       Abstract: \" + abstract[:500].replace(\"\\n\",\" \") + (\"...\" if len(abstract) > 500 else \"\"))\n",
    "        if js.get(\"decision\",\"\") == \"include\":\n",
    "            includes.append(r)\n",
    "\n",
    "    if len(includes) < GROUND_TRUTH_MIN:\n",
    "        st.log(f\"   [S3] FAIL: Found {len(includes)} INCLUDE(s) (< {GROUND_TRUTH_MIN}). Terminate.\")\n",
    "        raise SystemExit(\"Insufficient ground truth; refine NLQ or universe scope.\")\n",
    "\n",
    "    st.ground_truth = includes\n",
    "\n",
    "# ---- State 3.5: Senior plausibility guard\n",
    "def state35_plausibility(st: EngineState):\n",
    "    st.log(f\"{time.strftime('%H:%M:%S')}  [S3.5] Senior plausibility check...\")\n",
    "    vetted=[]\n",
    "    fails=[]\n",
    "    for r in st.ground_truth:\n",
    "        js = ask_json(QWEN_MODEL, PLAUS_SYS, plaus_user(st.locked_protocol, r), {\"pmid\":\"\",\"plausibility\":\"\",\"note\":\"\"})\n",
    "        ok = (js.get(\"plausibility\",\"\").upper()==\"PASS\")\n",
    "        st.log(f\"     [Plausibility] PMID {r['pmid']} -> {js.get('plausibility')} note={js.get('note','')}\")\n",
    "        if ok: vetted.append(r)\n",
    "        else:  fails.append(r['pmid'])\n",
    "    if len(vetted) < GROUND_TRUTH_MIN:\n",
    "        st.log(f\"   [S3.5] FAIL: Only {len(vetted)} plausible INCLUDE(s) after senior check (< {GROUND_TRUTH_MIN}). Terminate.\")\n",
    "        raise SystemExit(\"Ground truth implausible; refine NLQ or scope.\")\n",
    "    st.ground_truth = vetted\n",
    "    st.include_pmids = [r['pmid'] for r in vetted]\n",
    "    st.plaus_fails = fails\n",
    "\n",
    "    # Build MeSH vernaculum from vetted records\n",
    "    mr = defaultdict(int)\n",
    "    for r in vetted:\n",
    "        for m in (r.get(\"mesh\") or []):\n",
    "            mr[m] += 1\n",
    "    # Heuristic split using surface heuristics + protocol anchors\n",
    "    P_lex = st.locked_protocol.get(\"population_terms\",[])\n",
    "    I_lex = st.locked_protocol.get(\"intervention_terms\",[])\n",
    "    vernac = {\n",
    "        \"P_mesh\": [],\n",
    "        \"I_mesh\": [],\n",
    "        \"C_mesh\": [],\n",
    "        \"O_mesh\": [],\n",
    "        \"fallback_P\": P_lex,\n",
    "        \"fallback_I\": I_lex\n",
    "    }\n",
    "    # Assign roughly by keyword cues (lightweight; screener already has roles if needed via its mesh_roles)\n",
    "    for m,_n in sorted(mr.items(), key=lambda kv: -kv[1])[:30]:\n",
    "        ml=m.lower()\n",
    "        if \"funnel chest\" in ml or \"pectus\" in ml or \"minimally invasive\" in ml:\n",
    "            vernac[\"P_mesh\"].append(m)\n",
    "        elif \"cryosurg\" in ml or \"cryotherapy\" in ml or \"nerve block\" in ml or \"analgesia\" in ml:\n",
    "            vernac[\"I_mesh\"].append(m)\n",
    "        elif \"length of stay\" in ml or \"pain\" in ml or \"quality of life\" in ml or \"respiratory\" in ml:\n",
    "            vernac[\"O_mesh\"].append(m)\n",
    "        elif \"comparative\" in ml or \"randomized\" in ml or \"prospective\" in ml:\n",
    "            vernac[\"C_mesh\"].append(m)\n",
    "        elif \"intercostal\" in ml:\n",
    "            vernac[\"I_mesh\"].append(m)\n",
    "        elif \"thoracic\" in ml:\n",
    "            vernac[\"P_mesh\"].append(m)\n",
    "    st.mesh_vernac = vernac\n",
    "\n",
    "# ---- State 4: Strategy validation & refinement\n",
    "def make_topic_filter(vernac: Dict[str,List[str]]) -> str:\n",
    "    def mesh_block(arr):\n",
    "        toks=[]\n",
    "        for m in arr:\n",
    "            m=m.strip()\n",
    "            if not m: continue\n",
    "            toks.append(f\"\\\"{m}\\\"[MeSH Terms]\")\n",
    "        return \"(\" + \" OR \".join(toks) + \")\" if toks else \"\"\n",
    "    blocks=[]\n",
    "    for key in (\"P_mesh\",\"I_mesh\",\"O_mesh\"):\n",
    "        b = mesh_block(vernac.get(key,[]))\n",
    "        if b: blocks.append(b)\n",
    "    if not blocks:\n",
    "        return \"\"\n",
    "    return \" AND \".join(blocks)\n",
    "\n",
    "def pick_design_filter(st: EngineState) -> str:\n",
    "    pref = st.locked_protocol.get(\"designs_preference\", \"Randomized Controlled Trial\")\n",
    "    # choose the tightest matching filter from KB design_filters\n",
    "    dfs = st.kb.get(\"design_filters\", DEFAULT_KB[\"design_filters\"])\n",
    "    if \"Randomized Controlled Trial\" in pref and \"RCT\" in dfs:\n",
    "        return dfs[\"RCT\"]\n",
    "    if \"Comparative\" in pref and \"Comparative\" in dfs:\n",
    "        return dfs[\"Comparative\"]\n",
    "    return dfs.get(\"AnyTrial\", dfs[\"RCT\"])\n",
    "\n",
    "def combine_strategy(universe_q: str, topic_filter: str, design_filter: str, langs: List[str]) -> str:\n",
    "    q = f\"({universe_q})\"\n",
    "    if topic_filter:\n",
    "        q += f\" AND ({topic_filter})\"\n",
    "    if design_filter:\n",
    "        q += f\" AND ({design_filter})\"\n",
    "    if langs:\n",
    "        lang_or = \" OR \".join(f\"\\\"{l}\\\"[lang]\" for l in langs)\n",
    "        q += f\" AND ({lang_or})\"\n",
    "    return q\n",
    "\n",
    "def recall_okay(query: str, must_have_pmids: List[str]) -> Tuple[bool, int, List[str]]:\n",
    "    # ensure all gt pmids are in current results\n",
    "    total, ids = esearch_ids(query, retmax=5000)\n",
    "    idset = set(ids)\n",
    "    all_in = all(p in idset for p in must_have_pmids)\n",
    "    return all_in, total, ids\n",
    "\n",
    "def state4_strategy(st: EngineState):\n",
    "    st.log(f\"{time.strftime('%H:%M:%S')}  [S4] Strategy validation & refinement...\")\n",
    "    topic = make_topic_filter(st.mesh_vernac)\n",
    "    if not topic:\n",
    "        st.warnings.append(\"No reliable MeSH topic filter; falling back to lexical anchors only.\")\n",
    "    design = pick_design_filter(st)\n",
    "    langs  = st.locked_protocol.get(\"languages\", st.kb[\"languages\"])\n",
    "\n",
    "    filters = {\"topic\":topic, \"design\":design, \"languages\": langs}\n",
    "    tries=0; max_tries=3\n",
    "    while True:\n",
    "        strat = combine_strategy(st.universe_query, filters[\"topic\"], filters[\"design\"], filters[\"languages\"])\n",
    "        ok, total, ids = recall_okay(strat, st.include_pmids)\n",
    "        st.log(f\"   [Strategy] try={tries} total={total} recall_ok={ok}\")\n",
    "        st.final_strategy_count = total\n",
    "        if ok:\n",
    "            # precision window check\n",
    "            pmin = st.kb.get(\"precision_window\",{}).get(\"min\", 5)\n",
    "            pmax = st.kb.get(\"precision_window\",{}).get(\"max\", 2000)\n",
    "            if pmin <= total <= pmax:\n",
    "                st.recommended_filters = {\"topic\":filters[\"topic\"], \"design\":filters[\"design\"], \"languages\":filters[\"languages\"]}\n",
    "                st.log(\"   [Strategy] PASSED recall & precision checks.\")\n",
    "                break\n",
    "        if tries >= max_tries:\n",
    "            st.log(\"   [Strategy] FAIL after remediation attempts. Terminate.\")\n",
    "            raise SystemExit(\"Strategy cannot satisfy recall/precision simultaneously.\")\n",
    "        # Remediate via LLM\n",
    "        stats = {\"total\":total,\"recall_ok\":ok,\"precision_min\":pmin,\"precision_max\":pmax}\n",
    "        edit = ask_json(QWEN_MODEL, REMEDIATION_STRAT_SYS, remediation_strat_user(filters, stats), {\"op\":\"\",\"term\":\"\"})\n",
    "        op = (edit.get(\"op\") or \"\").upper()\n",
    "        term = edit.get(\"term\",\"\").strip()\n",
    "        if op == \"DROP_TERM\" and term and filters[\"topic\"]:\n",
    "            # try to drop a MeSH term by name (loose remove)\n",
    "            new_topic = re.sub(rf'\\s*\"?{re.escape(term)}\"?\\[MeSH Terms\\]\\s*(OR)?','', filters[\"topic\"])\n",
    "            new_topic = re.sub(r'\\(\\s*OR\\s*\\)','', new_topic).replace(\"  \",\" \").strip()\n",
    "            filters[\"topic\"] = new_topic\n",
    "        elif op == \"ADD_ANCHOR\" and term:\n",
    "            # add as TIAB anchor inside UniverseQuery (soft)\n",
    "            st.universe_query = f\"({st.universe_query}) AND ({or_block([term],'tiab')})\"\n",
    "        elif op == \"BROADEN_DESIGN_FILTER\":\n",
    "            # switch to less restrictive if not already\n",
    "            dfs = st.kb.get(\"design_filters\", DEFAULT_KB[\"design_filters\"])\n",
    "            filters[\"design\"] = dfs.get(\"Comparative\", dfs.get(\"AnyTrial\", filters[\"design\"]))\n",
    "        else:\n",
    "            st.warnings.append(f\"Unrecognized remediation op or term missing: {edit}\")\n",
    "        tries += 1\n",
    "\n",
    "# ---- State 5: Finalization & Handoff\n",
    "def state5_finalize(st: EngineState, nlq: str):\n",
    "    st.log(f\"{time.strftime('%H:%M:%S')}  [S5] Finalization & handoff...\")\n",
    "\n",
    "    # Compose embedding string\n",
    "    embed_str = _lm_call(QWEN_MODEL, [\n",
    "        {\"role\":\"system\",\"content\":EMBED_SYS},\n",
    "        {\"role\":\"user\",\"content\": embed_user(st.locked_protocol, st.mesh_vernac)}\n",
    "    ])\n",
    "\n",
    "    # Human-readable report\n",
    "    rep = []\n",
    "    rep.append(\"==================== SNIFF REPORT ====================\")\n",
    "    rep.append(\"NLQ:\\n  \" + textwrap.shorten(nlq, width=240, placeholder=\"...\"))\n",
    "    rep.append(\"\\nLocked Protocol:\\n  \" + json.dumps(st.locked_protocol, ensure_ascii=False))\n",
    "    rep.append(f\"\\nUniverse:\\n  query: {st.universe_query}\\n  count: {st.universe_count}\")\n",
    "    rep.append(f\"\\nGround truth (plausible INCLUDEs >= {GROUND_TRUTH_MIN}): {len(st.ground_truth)}\")\n",
    "    for r in st.ground_truth[:10]:\n",
    "        rep.append(f\"  • [{r['pmid']}] {r['year']} {r['language']} | {textwrap.shorten(r['title'] or '', 140)}\")\n",
    "    if st.plaus_fails:\n",
    "        rep.append(f\"\\nPlausibility FAILed PMIDs (excluded): {', '.join(st.plaus_fails)}\")\n",
    "    rep.append(\"\\nMeSH vernaculum:\\n  \" + json.dumps(st.mesh_vernac, ensure_ascii=False))\n",
    "    rep.append(\"\\nValidated Filters:\")\n",
    "    rep.append(\"  topic: \" + (st.recommended_filters.get(\"topic\") or \"(none)\"))\n",
    "    rep.append(\"  design: \" + (st.recommended_filters.get(\"design\") or \"(none)\"))\n",
    "    rep.append(\"  languages: \" + \", \".join(st.recommended_filters.get(\"languages\",[])))\n",
    "    rep.append(f\"\\nFinal strategy total: {st.final_strategy_count}\")\n",
    "    if st.warnings:\n",
    "        rep.append(\"\\nWARNINGS:\")\n",
    "        for w in st.warnings:\n",
    "            rep.append(\"  - \" + w)\n",
    "    rep.append(\"\\nEmbed string:\\n  \" + textwrap.shorten(embed_str.strip(), 300))\n",
    "    rep.append(\"\\n================== END OF REPORT =====================\")\n",
    "\n",
    "    report_text = \"\\n\".join(st.report_lines) + \"\\n\\n\" + \"\\n\".join(rep)\n",
    "\n",
    "    # Write files\n",
    "    (OUT_DIR/\"sniff_report.txt\").write_text(report_text, encoding=\"utf-8\")\n",
    "    artifacts = {\n",
    "        \"locked_protocol\": st.locked_protocol,\n",
    "        \"universe_query\": st.universe_query,\n",
    "        \"universe_count\": st.universe_count,\n",
    "        \"ground_truth_pmids\": [r[\"pmid\"] for r in st.ground_truth],\n",
    "        \"mesh_vernaculum\": st.mesh_vernac,\n",
    "        \"recommended_filters\": st.recommended_filters,\n",
    "        \"final_strategy_total\": st.final_strategy_count,\n",
    "        \"warnings\": st.warnings,\n",
    "        \"research_question_string_for_embedding\": embed_str.strip()\n",
    "    }\n",
    "    (OUT_DIR/\"sniff_artifacts.json\").write_text(json.dumps(artifacts, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # quick TSV for ground truth\n",
    "    lines = [\"pmid\\tyear\\tlanguage\\ttitle\"]\n",
    "    for r in st.ground_truth:\n",
    "        lines.append(f\"{r['pmid']}\\t{r.get('year','')}\\t{r.get('language','')}\\t{(r.get('title') or '').replace('\\t',' ')}\")\n",
    "    (OUT_DIR/\"ground_truth.tsv\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "    # Print concise tail summary to notebook/stdout\n",
    "    print(\"\\n==================== FINAL SUMMARY ====================\")\n",
    "    print(f\"Universe: count={st.universe_count}\")\n",
    "    print(f\"Ground truth: n={len(st.ground_truth)} pmids={', '.join([r['pmid'] for r in st.ground_truth[:8]])}{' ...' if len(st.ground_truth)>8 else ''}\")\n",
    "    print(\"Validated filters:\")\n",
    "    print(\"  topic:   \" + (st.recommended_filters.get(\"topic\") or \"(none)\"))\n",
    "    print(\"  design:  \" + (st.recommended_filters.get(\"design\") or \"(none)\"))\n",
    "    print(\"  lang:    \" + \", \".join(st.recommended_filters.get(\"languages\",[])))\n",
    "    print(\"Final total:\", st.final_strategy_count)\n",
    "    if st.warnings:\n",
    "        print(\"Warnings:\")\n",
    "        for w in st.warnings: print(\" -\", w)\n",
    "    print(\"Artifacts:\", OUT_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# Top-level runner\n",
    "# ----------------------------\n",
    "def sniff_validate_engine(USER_NLQ: str, kb_path=\"system_knowledge_base.json\"):\n",
    "    st = EngineState(kb=load_kb(kb_path))\n",
    "    try:\n",
    "        state1_protocol_lockdown(st, USER_NLQ)\n",
    "        state2_universe(st)\n",
    "        state3_ground_truth(st)\n",
    "        state35_plausibility(st)\n",
    "        state4_strategy(st)\n",
    "        state5_finalize(st, USER_NLQ)\n",
    "    except SystemExit as e:\n",
    "        # Persist partial report/warnings for debugging\n",
    "        st.warnings.append(f\"TERMINATED: {str(e)}\")\n",
    "        (OUT_DIR/\"sniff_report.txt\").write_text(\"\\n\".join(st.report_lines) + f\"\\n\\nTERMINATED: {str(e)}\", encoding=\"utf-8\")\n",
    "        raise\n",
    "\n",
    "# ----------------------------\n",
    "# RUN: put your NLQ here\n",
    "# ----------------------------\n",
    "USER_NLQ = \"\"\"Population = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\n",
    "Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE\n",
    "(the intervention of interest is INC, not the surgery).\n",
    "Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block,\n",
    "or systemic multimodal analgesia.\n",
    "Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days.\n",
    "Study designs = RCTs preferred; if RCTs absent, include comparative cohort/case-control.\n",
    "Year_min = 2015. Languages = English, Portuguese, Spanish.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sniff_validate_engine(USER_NLQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a39273f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S1] Protocol lockdown...\n",
      "  [S1] Locked protocol:\n",
      "    {\"population_terms\": [\"adults\", \"Nuss\", \"MIRPE\", \"pectus excavatum\", \"minimally invasive repair\"], \"intervention_terms\": [\"intercostal nerve\", \"cryoablation\", \"cryoanalgesia\", \"INC\", \"analgesia\"], \"comparators_terms\": [\"thoracic epidural\", \"paravertebral block\", \"intercostal nerve block\", \"erector spinae plane block\", \"systemic multimodal analgesia\"], \"outcomes_terms\": [\"postoperative opioid consumption\", \"pain scores\", \"0-7 day pain\", \"discharge opioid use\"], \"must_have\": [\"MIRPE\", \"Nuss\", \"pectus excavatum\", \"cryoablation\"], \"avoid\": [\"pediatric\"], \"designs_preference\": \"Randomized Controlled Trial\", \"languages\": [\"english\", \"portuguese\", \"spanish\"], \"year_min\": 2015}\n",
      "[S2] Universe definition & sizing...\n",
      "   [Universe] try=0 count=299 window=(50, 10000)\n",
      "[S2.5] Rerank universe with PICO-weighted TF-IDF...\n",
      "[S3] Ground-truth discovery & vocabulary mining...\n",
      "  [Screen] PMID 31259649 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: The Use of Cryoanalgesia in Minimally Invasive Repair of Pectus Excavatum: Lessons Learned.\n",
      "    Abstract: Introduction: Cryoanalgesia has been applied to minimally invasive repair of pectus excavatum (MIRPE). After implementation of cryoanalgesia at our institution, we had several cases of delayed postoperative pneumothorax. The purpose of this study was to critically evaluate the complications and efficacy of cryoanalgesia in MIRPE. Materials and Methods: We performed a single institution retrospective review of pediatric patients undergoing MIRPE from June 2017 to July 2018. Multimodal (MM) analge...\n",
      "  [Screen] PMID 33853733 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Cryoanalgesia enhances recovery from minimally invasive repair of pectus excavatum resulting in reduced length of stay: A case-matched analysis of NSQIP-Pediatr\n",
      "    Abstract: Pain control is challenging after minimally invasive repair of pectus excavatum (MIRPE). Cryoanalgesia, which temporarily ablates peripheral nerves, improves pain control and may accelerate post-operative recovery. We hypothesized that cryoanalgesia would be associated with shorter length of stay (LOS) in children undergoing MIRPE. A matched cohort study was conducted of children (<18 years) who underwent MIRPE 2016-2018, using the National Surgical Quality Improvement Program-Pediatric database...\n",
      "  [Screen] PMID 39561666 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Intercostal Nerve Cryoablation in Minimally Invasive Repair of Pectus Excavatum: National Trends, Outcomes, and Predictors of Utilization.\n",
      "    Abstract: Intercostal nerve cryoablation during minimally invasive repair of pectus excavatum (MIRPE) is an effective pain control technique. Some insurers may not reimburse for cryoablation in this context, contending that it's an experimental procedure. This study aimed to describe national trends in cryoablation use and evaluate outcomes and predictors of its use. The Pediatric Health Information System database was queried for pectus excavatum patients aged 9-21 who underwent MIRPE between 2016 and 20...\n",
      "  [Screen] PMID 35945758 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Limited cryoablation reduces hospital stay and opioid consumption compared to thoracic epidural analgesia after minimally invasive repair of pectus excavatum.\n",
      "    Abstract: pain following minimally invasive repair of pectus excavatum (MIRPE) is a critical concern that leads to a prolonged hospital stay and high doses of opiates administered to the patients. This study aimed to evaluate the efficacy of intraoperative cryoanalgesia (cryoablation of the intercostal nerves) during MIRPE. We retrospectively analyzed the data of 64 patients who underwent MIRPE and received cryoanalgesia or epidural analgesia between January 2019 and January 2021. The oral morphine millig...\n",
      "  [Screen] PMID 28341230 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not pectus excavatum/Nuss/MIRPE and lacks postoperative pain data or comparative design.\n",
      "    Title: Intraoperative cryoanalgesia for managing pain after the Nuss procedure.\n",
      "    Abstract: Cryoanalgesia prevents pain by freezing the affected peripheral nerve. We report the use of intraoperative cryoanalgesia during the Nuss procedure for pectus excavatum and describe our initial experience, modifications of technique, and lessons learned. We retrospectively reviewed the medical records of patients who received cryoanalgesia during the Nuss procedure between June 1, 2015, and April 30, 2016, at our institutions and analyzed modifications in surgical technique during this early adop...\n",
      "  [Screen] PMID 35299214 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Are We Ready for Cryoablation in Children Undergoing Nuss Procedures?\n",
      "    Abstract: \n",
      "  [Screen] PMID 30935731 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Intraoperative intercostal nerve cryoablation During the Nuss procedure reduces length of stay and opioid requirement: A randomized clinical trial.\n",
      "    Abstract: Minimally-invasive repair of pectus excavatum by the Nuss procedure is associated with significant postoperative pain, prolonged hospital stay, and high opiate requirement. We hypothesized that intercostal nerve cryoablation during the Nuss procedure reduces hospital length of stay (LOS) compared to thoracic epidural analgesia. This randomized clinical trial evaluated 20 consecutive patients undergoing the Nuss procedure for pectus excavatum between May 2016 and March 2018. Patients were randomi...\n",
      "  [Screen] PMID 34814047 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Cryoanalgesia is Associated With Decreased Postoperative Opioid Use in Minimally Invasive Repair of Pectus Excavatum.\n",
      "    Abstract: Postoperative pain control is challenging after pectus excavatum repair. We aimed to understand the impact that cryoanalgesia had on opioid utilization and outcomes of pediatric patients undergoing minimally invasive repair of pectus excavatum (MIRPE). A single-center retrospective cohort study was conducted of all patients (< 18 y) who underwent MIRPE (2011-2019). Patients receiving cryoanalgesia were compared to those who did not. The primary outcome was total postoperative, inpatient, opioid ...\n",
      "  [Screen] PMID 35797475 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically for pectus excavatum/Nuss/MIRPE in adults. No postoperative pain data.\n",
      "    Title: Multicenter Assessment of Cryoanalgesia Use in Minimally Invasive Repair of Pectus Excavatum: A 20-center Retrospective Cohort Study.\n",
      "    Abstract: To assess the clinical implications of cryoanalgesia for pain management in children undergoing minimally invasive repair of pectus excavatum (MIRPE). MIRPE entails significant pain management challenges, often requiring high postoperative opioid use. Cryoanalgesia, which blocks pain signals by temporarily ablating intercostal nerves, has been recently utilized as an analgesic adjunct. We hypothesized that the use of cryoanalgesia during MIRPE would decrease postoperative opioid use and length o...\n",
      "  [Screen] PMID 26896363 -> decision=EXCLUDE checklist={'P': False, 'I': False, 'O': False, 'D': False} why=Abstract does not explicitly mention intercostal nerve cryoablation or postoperative opioid consumption/pain scores.\n",
      "    Title: Use of transthoracic cryoanalgesia during the Nuss procedure.\n",
      "    Abstract: \n",
      "  [Screen] PMID 38242172 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Intercostal Nerve Cryoablation or Epidural Analgesia for Multimodal Pain Management after the Nuss Procedure: A Cohort Study.\n",
      "    Abstract: Nuss procedure for pectus excavatum is a minimally invasive, but painful procedure. Recently, intercostal nerve cryoablation has been introduced as a pain management technique. In this cohort study, we compared the efficacy of multimodal pain management strategies in children undergoing a Nuss procedure. The effectiveness of intercostal nerve cryoablation combined with patient-controlled systemic opioid analgesia (PCA) was compared with continuous epidural analgesia (CEA) combined with PCA. The ...\n",
      "  [Screen] PMID 36922280 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Impact of Cryoanalgesia Use During Minimally Invasive Pectus Excavatum Repair on Hospital Days and Total Hospital Costs Among Pediatric Patients.\n",
      "    Abstract: Surgical repair of pectus excavatum is a painful procedure requiring multimodal pain control with historically prolonged hospital stay. This study aimed to evaluate the impact of cryoanalgesia during minimally invasive repair of pectus excavatum (MIRPE) on hospital days (HDs), total hospital costs (HCs), and complications. We hypothesized that cryoanalgesia would be associated with reduced HDs and total HCs with no increase in post-operative complications. We conducted a retrospective review of ...\n",
      "  [Screen] PMID 36350702 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Intercostal Nerve Cryoablation in Minimally Invasive Repair of Pectus Excavatum: Effect on Pulmonary Function.\n",
      "    Abstract: Introduction: Cryoablation of intercostal nerves is performed for pain control after minimally invasive repair of pectus excavatum (MIRPE). Cryoablation affects both sensory and motor neurons, resulting in temporary anesthesia to the chest wall and loss of intercostal motor function. The study objective is to determine the effect of cryoablation on incentive spirometry (IS) volumes, as a measure of pulmonary function, after MIRPE. Materials and Methods: A single-institution retrospective review ...\n",
      "  [Screen] PMID 37286412 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Forgotten Branch of the Intercostal Nerve: Implication for Cryoablation Nerve Block for Pectus Excavatum Repair.\n",
      "    Abstract: We first utilized and reported on the use of cryoanalgesia for postoperative pain control for Nuss procedure in 2016. We hypothesized that postoperative pain control could be optimized if the intercostal nerve anatomy is better understood. To test this hypothesis, human cadavers were dissected to elucidate the intercostal nerve anatomy. Cryoablation technique was modified. Cadaver Study: Adult cadavers were used to visualize the branching patterns of the intercostal nerves. Cryoablation: Posteri...\n",
      "  [Screen] PMID 39117536 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not pectus excavatum/Nuss/MIRPE and lacks postoperative pain data or comparative design.\n",
      "    Title: Six Years of Quality Improvement in Pectus Excavatum Repair: Implementation of Intercostal Nerve Cryoablation and ERAS Protocols for Patients Undergoing Nuss Pr\n",
      "    Abstract: The Nuss procedure for pectus excavatum is associated with prolonged hospitalizations due to pain. We evaluated implementation of intercostal nerve cryoablation and enhanced recovery after surgery (ERAS) protocols on outcomes of Nuss procedures performed over six years at a single institution. This retrospective cohort study included patients who underwent Nuss procedure from 10/2017 to 09/2023. Patients received epidurals prior to 06/2019, cryoablation from 06/2019 to 07/2021, and ERAS with cry...\n",
      "  [Screen] PMID 37791468 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Intercostal Nerve Cryoablation Reduces Opioid Use and Length of Stay Without Increasing Adverse Events: A Retrospective Cohort Study of 5442 Patients Undergoing\n",
      "    Abstract: To examine differences in opioid use, length of stay, and adverse events after minimally invasive correction of pectus excavatum (MIRPE) with and without intercostal nerve cryoablation. Small studies show that intraoperative intercostal nerve cryoablation provides effective analgesia with no large-scale evaluations of this technique. The pediatric health information system database was used to perform a retrospective cohort study comparing patients undergoing MIRPE at children's hospitals before...\n",
      "  [Screen] PMID 38531584 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Intercostal nerve cryoablation versus thoracic epidural analgesia for minimal invasive Nuss repair of pectus excavatum: a protocol for a randomised clinical tri\n",
      "    Abstract: Epidural analgesia is currently considered the gold standard in postoperative pain management for the minimally invasive Nuss procedure for pectus excavatum. Alternative analgesic strategies (eg, patient-controlled analgesia and paravertebral nerve block) fail in accomplishing adequate prolonged pain management. Furthermore, the continuous use of opioids, often prescribed in addition to all pain management strategies, comes with side effects. Intercostal nerve cryoablation seems a promising nove...\n",
      "  [Screen] PMID 26670193 -> decision=EXCLUDE checklist={'P': False, 'I': False, 'O': False, 'D': False} why=Abstract does not explicitly mention intercostal nerve cryoablation or postoperative opioid consumption/pain scores.\n",
      "    Title: Freeze the pain away: The role of cryoanalgesia during a Nuss procedure.\n",
      "    Abstract: \n",
      "  [Screen] PMID 39965427 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Nationwide Comparison of Epidural and Regional Analgesia Versus Intercostal Nerve Cryoablation in Pectus Repair.\n",
      "    Abstract: Pectus excavatum is the most common congenital chest wall deformity, occurring in 1 in 250-300 live births. Surgical correction of this pathology is traditionally associated with significant pain. We hypothesize intercostal nerve cryoablation is a superior analgesic modality that can improve patient comfort, improve healthcare resource utilization, and reduce opioid exposure in a high-risk population. The most recently published National Readmissions Database (2016-2020) was queried for patient ...\n",
      "  [Screen] PMID 32929487 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically for pectus excavatum/Nuss/MIRPE in adults. No postoperative pain data.\n",
      "    Title: Intercostal nerve cryoablation versus thoracic epidural for postoperative analgesia following pectus excavatum repair: a systematic review and meta-analysis.\n",
      "    Abstract: Minimally invasive pectus excavatum repair via the Nuss procedure is associated with significant postoperative pain that is considered as the dominant factor affecting the duration of hospitalization. Postoperative pain after the Nuss procedures is commonly controlled by thoracic epidural analgesia. Recently, intercostal nerve cryoablation has been proposed as an alternative method with long-acting pain control and shortened hospitalization. The subsequent objective was to systematically review ...\n",
      "  [Screen] PMID 34823843 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Next day discharge after the Nuss procedure using intercostal nerve cryoablation, intercostal nerve blocks, and a perioperative ERAS pain protocol.\n",
      "    Abstract: The Nuss procedure for pectus excavatum has historically been associated with significant postoperative pain, which has been the major factor contributing to hospital length of stay (LOS). A single-institution, prospective study of 40 consecutive patients undergoing Nuss bar placement for pectus excavatum between November 2019 and January 2021 was conducted to assess the effectiveness of a multimodality pain management protocol. All patients received T3-T8 intercostal nerve cryoablation (INC), T...\n",
      "  [Screen] PMID 12874691 -> decision=EXCLUDE checklist={'P': False, 'I': False, 'O': False, 'D': False} why=Abstract does not explicitly mention intercostal nerve cryoablation or postoperative pain.\n",
      "    Title: Reduced hospitalization cost for patients with pectus excavatum treated using minimally invasive surgery.\n",
      "    Abstract: Currently, few data exist regarding the relative costs associated with open and minimally invasive pectus excavatum repair. The aim of this study was to compare the surgical and hospitalization costs for these two surgical techniques and to identify factors responsible for cost differences. A retrospective review of hospital charts, patient and parent questionnaires, and hospital accounting records was performed for 68 patients who underwent surgical correction of pectus excavatum between June 1...\n",
      "  [Screen] PMID 36494205 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically in adults and lacks postoperative pain data.\n",
      "    Title: Cryoablation in 350 Nuss Procedures: Evolution of Hospital Length of Stay and Opioid Use.\n",
      "    Abstract: Current studies show cryoablation decreases opioid requirements and lengths of stay (LOS) in patients undergoing the Nuss procedure for pectus excavatum. This study evaluated the relationship between cryoablation and clinical outcomes for the Nuss procedure. A retrospective single-center chart review was performed on patients undergoing the Nuss procedure with intercostal cryoablation from December 2017-August 2021. Demographics, hospital course, and postoperative complications were abstracted. ...\n",
      "  [Screen] PMID 28302362 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': False, 'D': False} why=Abstract mentions INC but not specifically for pectus excavatum/Nuss/MIRPE in adults. No postoperative pain data.\n",
      "    Title: Response to intercostal nerve cryoablation versus thoracic epidural catheters for postoperative analgesia following pectus excavatum repair.\n",
      "    Abstract: \n",
      "  [Screen] PMID 39823003 -> decision=EXCLUDE checklist={'P': False, 'I': True, 'O': True, 'D': False} why=Does not explicitly mention pectus excavatum/Nuss/MIRPE AND adults.\n",
      "    Title: Pain management after pediatric minimally invasive repair of pectus excavatum: a narrative review.\n",
      "    Abstract: Pectus excavatum is a common congenital chest wall abnormality characterized by a concave appearance of the chest, and minimally invasive repair of pectus excavatum (MIRPE) is the surgical treatment of choice. A rapidly growing field of research is pain management in children undergoing MIRPE, with many shifts in practice occurring over the last decade. The primary objectives of this narrative review are to describe current methods of perioperative pain management and the development of enhanced...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://127.0.0.1:1234/v1/chat/completions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 807\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    800\u001b[39m     USER_NLQ = \u001b[33m\"\"\"\u001b[39m\u001b[33mPopulation = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[33mIntervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during MIRPE/Nuss (the intervention of interest is INC, not the surgery).\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[33mComparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    805\u001b[39m \u001b[33mYear_min = 2015.\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[33mLanguages = English, Portuguese, Spanish.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[43msniff_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUSER_NLQ\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 791\u001b[39m, in \u001b[36msniff_engine\u001b[39m\u001b[34m(nlq)\u001b[39m\n\u001b[32m    789\u001b[39m universe_query, u_ids = state2_universe(proto)\n\u001b[32m    790\u001b[39m reranked_docs = state2_5_rerank_universe(universe_query, u_ids, proto)\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m gt_pmids, mesh_vocab = \u001b[43mstate3_ground_truth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreranked_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    792\u001b[39m gt_pmids = state3_5_plausibility(proto, gt_pmids)\n\u001b[32m    793\u001b[39m rec_filters, combined_ids = state4_validate_strategy(universe_query, gt_pmids, proto, mesh_vocab)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 668\u001b[39m, in \u001b[36mstate3_ground_truth\u001b[39m\u001b[34m(reranked_docs, protocol)\u001b[39m\n\u001b[32m    666\u001b[39m mesh_vocab = {}\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m reranked_docs[:GROUND_TOP_N]:\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     js = \u001b[43mscreen_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m js[\u001b[33m\"\u001b[39m\u001b[33mdecision\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mINCLUDE\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(js[\u001b[33m\"\u001b[39m\u001b[33mchecklist\u001b[39m\u001b[33m\"\u001b[39m].get(k,\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m    670\u001b[39m         includes.append(rec)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 638\u001b[39m, in \u001b[36mscreen_record\u001b[39m\u001b[34m(protocol, rec)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscreen_record\u001b[39m(protocol: \u001b[38;5;28mdict\u001b[39m, rec: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m     js = \u001b[43mget_validated_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mSCREENER_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS3_SCREEN_SYSTEM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_screen_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_screener_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbg_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscreen\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m     snippet = (rec[\u001b[33m\"\u001b[39m\u001b[33mabstract\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)[:\u001b[32m500\u001b[39m].replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    643\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  [Screen] PMID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrec[\u001b[33m\"\u001b[39m\u001b[33mpmid\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> decision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjs[\u001b[33m\"\u001b[39m\u001b[33mdecision\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m checklist=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjs[\u001b[33m\"\u001b[39m\u001b[33mchecklist\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m why=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjs[\u001b[33m\"\u001b[39m\u001b[33mreason\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mget_validated_json\u001b[39m\u001b[34m(model, system, user_base, validator, template_hint, max_tries, temperature, max_tokens, dbg_stage)\u001b[39m\n\u001b[32m    144\u001b[39m history_user = user_base\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_tries):\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     raw = \u001b[43mlm_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_user\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mReturn ONLY:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mBEGIN_JSON\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m{\u001b[39;49m\u001b[33;43m...}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mEND_JSON\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEND_JSON\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    149\u001b[39m         js = json.loads(extract_json(raw))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mlm_chat\u001b[39m\u001b[34m(model, system, user, temperature, max_tokens, stop)\u001b[39m\n\u001b[32m     95\u001b[39m     body[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m] = stop\n\u001b[32m     96\u001b[39m r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r.json()[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: http://127.0.0.1:1234/v1/chat/completions"
     ]
    }
   ],
   "source": [
    "# Sniff Validation Engine v3.1\n",
    "# Fixes over v3.0:\n",
    "# - Protocol terms MUST be 1–3 words; hard guardrails for MIRPE/Nuss/pectus + cryoablation\n",
    "# - Remediation ops: ADD_POP, ADD_INT, ADD_ANCHOR, SIMPLIFY_TERM, REMOVE_TERM\n",
    "# - True PICO-weighted TF-IDF: sum(pico_weight * tfidf(term, doc))\n",
    "# - MeSH role classification via LLM using the article's actual MeSH list only\n",
    "# - Ask-Validate-Retry writes invalid outputs to sniff_out/llm_debug_*.json\n",
    "# - Model eviction: TTL + 'lms unload --all' + 10s wait\n",
    "\n",
    "import os, re, json, time, math, subprocess, shutil, random, pathlib, textwrap\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# -----------------------------\n",
    "# Config & Paths\n",
    "# -----------------------------\n",
    "LMSTUDIO_BASE = os.getenv(\"LMSTUDIO_BASE\", \"http://127.0.0.1:1234\").rstrip(\"/\")\n",
    "QWEN_MODEL     = os.getenv(\"QWEN_MODEL\", \"unsloth/qwen3-4b\")\n",
    "SCREENER_MODEL = os.getenv(\"SCREENER_MODEL\", \"gemma-3n-e4b-it@q5_k_m\")   # LM Studio display id\n",
    "ENTREZ_EMAIL   = os.getenv(\"ENTREZ_EMAIL\", \"you@example.com\")\n",
    "ENTREZ_API_KEY = os.getenv(\"ENTREZ_API_KEY\", \"\")\n",
    "HTTP_TIMEOUT   = int(os.getenv(\"HTTP_TIMEOUT\", \"120\"))\n",
    "LM_TTL_SECONDS = int(os.getenv(\"LM_TTL_SECONDS\", \"5\"))\n",
    "LM_SWITCH_WAIT_SECONDS = int(os.getenv(\"LM_SWITCH_WAIT_SECONDS\", \"10\"))\n",
    "\n",
    "WORKDIR = pathlib.Path(\".\").resolve()\n",
    "OUTDIR = WORKDIR / \"sniff_out\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "KB_PATH = WORKDIR / \"system_knowledge_base.json\"\n",
    "REPORT_TXT = OUTDIR / \"sniff_report.txt\"\n",
    "ARTIFACTS_JSON = OUTDIR / \"sniff_artifacts.json\"\n",
    "\n",
    "# Universe sizing thresholds\n",
    "UNIVERSE_MIN = 50\n",
    "UNIVERSE_MAX = 10000\n",
    "UNIVERSE_HARD_MIN = 25  # below this, terminate\n",
    "\n",
    "# Rerank corpus/sample sizes\n",
    "RERANK_FETCH_N = 600   # efetch corpus size for reranker\n",
    "GROUND_TOP_N   = 50    # how many to screen after rerank\n",
    "\n",
    "# Ground truth requirements\n",
    "MIN_GROUND_TRUTH = 3\n",
    "\n",
    "# Strategy validation\n",
    "PRECISION_WINDOW = (10, 5000)\n",
    "REMEDIATION_MAX_TRIES = 3\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities: LM Studio management\n",
    "# -----------------------------\n",
    "def lmstudio_unload_all_safely():\n",
    "    try:\n",
    "        exe = shutil.which(\"lms\") or shutil.which(\"lms.exe\")\n",
    "        if exe:\n",
    "            subprocess.run([exe, \"unload\", \"--all\"], check=False,\n",
    "                           stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self):\n",
    "        self.current = None\n",
    "    def ensure_model(self, model_id: str):\n",
    "        if self.current == model_id:\n",
    "            return\n",
    "        lmstudio_unload_all_safely()\n",
    "        time.sleep(LM_SWITCH_WAIT_SECONDS)\n",
    "        self.current = model_id\n",
    "\n",
    "LM = ModelManager()\n",
    "\n",
    "# -----------------------------\n",
    "# HTTP / LLM JSON helpers\n",
    "# -----------------------------\n",
    "def lm_chat(model: str, system: str, user: str,\n",
    "            temperature: float = 0.0,\n",
    "            max_tokens: int = 2048,\n",
    "            stop: List[str] | None = None) -> str:\n",
    "    LM.ensure_model(model)\n",
    "    url = f\"{LMSTUDIO_BASE}/v1/chat/completions\"\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "        \"temperature\": float(temperature),\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"stream\": False,\n",
    "        \"ttl\": LM_TTL_SECONDS  # best-effort TTL eviction\n",
    "    }\n",
    "    if stop:\n",
    "        body[\"stop\"] = stop\n",
    "    r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "FENCE = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)```\", re.I)\n",
    "BEGIN = re.compile(r\"BEGIN_JSON\\s*\", re.I)\n",
    "END   = re.compile(r\"\\s*END_JSON\", re.I)\n",
    "\n",
    "def _sanitize_json_str(s: str) -> str:\n",
    "    s = (s or \"\").replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n",
    "    s = re.sub(r\",\\s*(\\}|\\])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_json(txt: str) -> str:\n",
    "    blocks=[]; pos=0\n",
    "    while True:\n",
    "        m1 = BEGIN.search(txt or \"\", pos)\n",
    "        if not m1: break\n",
    "        m2 = END.search(txt, m1.end())\n",
    "        if not m2: break\n",
    "        blocks.append(txt[m1.end():m2.start()])\n",
    "        pos = m2.end()\n",
    "    if blocks:\n",
    "        return _sanitize_json_str(blocks[-1])\n",
    "    fences = FENCE.findall(txt or \"\")\n",
    "    if fences:\n",
    "        return _sanitize_json_str(fences[-1])\n",
    "    # fallback: last balanced {..}\n",
    "    s = txt or \"\"\n",
    "    last,stack,start=None,0,None\n",
    "    for i,ch in enumerate(s):\n",
    "        if ch==\"{\":\n",
    "            if stack==0: start=i\n",
    "            stack+=1\n",
    "        elif ch==\"}\" and stack>0:\n",
    "            stack-=1\n",
    "            if stack==0 and start is not None:\n",
    "                last = s[start:i+1]\n",
    "    if last: return _sanitize_json_str(last)\n",
    "    raise ValueError(\"no JSON found\")\n",
    "\n",
    "def dump_invalid(stage: str, content: str, idx: int):\n",
    "    (OUTDIR / f\"llm_debug_{stage}_{idx}.txt\").write_text(content, encoding=\"utf-8\")\n",
    "\n",
    "def get_validated_json(model: str, system: str, user_base: str,\n",
    "                       validator, template_hint: str = \"\",\n",
    "                       max_tries: int = 3, temperature: float = 0.0, max_tokens: int = 2048,\n",
    "                       dbg_stage: str = \"generic\") -> dict:\n",
    "    history_user = user_base\n",
    "    for i in range(max_tries):\n",
    "        raw = lm_chat(model, system, history_user + \"\\n\\nReturn ONLY:\\nBEGIN_JSON\\n{...}\\nEND_JSON\",\n",
    "                      temperature=temperature, max_tokens=max_tokens, stop=[\"END_JSON\"])\n",
    "        try:\n",
    "            js = json.loads(extract_json(raw))\n",
    "        except Exception as e:\n",
    "            dump_invalid(f\"{dbg_stage}_parse\", raw, i)\n",
    "            history_user = user_base + f\"\\n\\nYour previous JSON was invalid: {e}\\n{template_hint}\\nUse ONLY true/false booleans; keys exactly as specified.\"\n",
    "            continue\n",
    "        try:\n",
    "            ok, why = validator(js)\n",
    "        except Exception as e:\n",
    "            ok, why = False, f\"Validation exception: {e}\"\n",
    "        if ok:\n",
    "            return js\n",
    "        dump_invalid(f\"{dbg_stage}_schema\", json.dumps(js,indent=2), i)\n",
    "        history_user = user_base + f\"\\n\\nYour previous JSON failed validation: {why}\\n{template_hint}\\nCorrect and resubmit the SAME schema.\"\n",
    "    raise SystemExit(\"Fatal: LLM failed to produce valid JSON after retries.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Knowledge base\n",
    "# -----------------------------\n",
    "DEFAULT_KB = {\n",
    "  \"publication_types\": [\n",
    "    \"Randomized Controlled Trial\",\"Clinical Trial\",\"Controlled Clinical Trial\",\n",
    "    \"Comparative Study\",\"Prospective Studies\",\"Cohort Studies\",\"Case-Control Studies\",\n",
    "    \"Systematic Review\",\"Meta-Analysis\",\"Network Meta-Analysis\"\n",
    "  ],\n",
    "  \"languages\": [\"english\",\"portuguese\",\"spanish\",\"french\",\"german\",\"italian\"]\n",
    "}\n",
    "\n",
    "def ensure_kb() -> dict:\n",
    "    if KB_PATH.exists():\n",
    "        try: return json.loads(KB_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception: pass\n",
    "    KB_PATH.write_text(json.dumps(DEFAULT_KB, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"[KB] No KB found. Wrote defaults to {KB_PATH}\")\n",
    "    return DEFAULT_KB\n",
    "\n",
    "KB = ensure_kb()\n",
    "\n",
    "# -----------------------------\n",
    "# PubMed E-utilities\n",
    "# -----------------------------\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "\n",
    "def esearch(term: str, mindate: int|None=None, retmax=200, usehistory=True) -> dict:\n",
    "    params = {\"db\":\"pubmed\",\"retmode\":\"json\",\"term\":term,\"retmax\":retmax,\"email\":ENTREZ_EMAIL}\n",
    "    if usehistory: params[\"usehistory\"]=\"y\"\n",
    "    if mindate: params[\"mindate\"]=str(mindate)\n",
    "    if ENTREZ_API_KEY: params[\"api_key\"]=ENTREZ_API_KEY\n",
    "    r = requests.get(f\"{EUTILS}/esearch.fcgi\", params=params, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"esearchresult\"]\n",
    "\n",
    "def esearch_all_ids(term: str, mindate: int|None=None, cap:int=10000) -> List[str]:\n",
    "    js = esearch(term, mindate=mindate, retmax=0, usehistory=True)\n",
    "    count = int(js.get(\"count\",\"0\"))\n",
    "    if count == 0: return []\n",
    "    webenv = js[\"webenv\"]; qk = js[\"querykey\"]\n",
    "    ids=[]\n",
    "    retstart=0\n",
    "    while retstart < min(count,cap):\n",
    "        chunk = min(5000, cap-retstart)\n",
    "        r = requests.get(f\"{EUTILS}/esearch.fcgi\", params={\n",
    "            \"db\":\"pubmed\",\"retmode\":\"json\",\"retstart\":retstart,\"retmax\":chunk,\n",
    "            \"WebEnv\":webenv,\"query_key\":qk,\"email\":ENTREZ_EMAIL,\n",
    "            **({\"api_key\":ENTREZ_API_KEY} if ENTREZ_API_KEY else {})\n",
    "        }, timeout=HTTP_TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        ids.extend(r.json()[\"esearchresult\"].get(\"idlist\",[]))\n",
    "        retstart += chunk\n",
    "        time.sleep(0.34)\n",
    "    return ids\n",
    "\n",
    "def efetch_xml(ids: List[str]) -> str:\n",
    "    if not ids: return \"\"\n",
    "    params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"id\":\",\".join(ids),\"email\":ENTREZ_EMAIL}\n",
    "    if ENTREZ_API_KEY: params[\"api_key\"]=ENTREZ_API_KEY\n",
    "    r = requests.get(f\"{EUTILS}/efetch.fcgi\", params=params, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_pubmed_xml(xml_text: str) -> List[dict]:\n",
    "    out=[]\n",
    "    if not xml_text.strip(): return out\n",
    "    root = ET.fromstring(xml_text)\n",
    "    def _join(node):\n",
    "        if node is None: return \"\"\n",
    "        try: return \"\".join(node.itertext())\n",
    "        except Exception: return node.text or \"\"\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = art.findtext(\".//PMID\") or \"\"\n",
    "        title = _join(art.find(\".//ArticleTitle\")).strip()\n",
    "        abs_nodes = art.findall(\".//Abstract/AbstractText\")\n",
    "        abstract = \" \".join(_join(n).strip() for n in abs_nodes) if abs_nodes else \"\"\n",
    "        year = None\n",
    "        for path in (\".//ArticleDate/Year\",\".//PubDate/Year\",\".//DateCreated/Year\",\".//PubDate/MedlineDate\"):\n",
    "            s = art.findtext(path)\n",
    "            if s:\n",
    "                m = re.search(r\"\\d{4}\", s)\n",
    "                if m: year = int(m.group(0)); break\n",
    "        lang = art.findtext(\".//Language\") or None\n",
    "        pubtypes = [pt.text for pt in art.findall(\".//PublicationTypeList/PublicationType\") if pt.text]\n",
    "        mesh = [mh.findtext(\"./DescriptorName\") for mh in art.findall(\".//MeshHeadingList/MeshHeading\") if mh.findtext(\"./DescriptorName\")]\n",
    "        out.append({\"pmid\":pmid,\"title\":title,\"abstract\":abstract,\"year\":year,\"language\":lang,\"publication_types\":pubtypes,\"mesh\":mesh})\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Query building\n",
    "# -----------------------------\n",
    "def or_block(terms: List[str], field=\"tiab\") -> str:\n",
    "    toks=[]\n",
    "    for t in terms or []:\n",
    "        t=t.strip()\n",
    "        if not t: continue\n",
    "        if \" \" in t or \"-\" in t:\n",
    "            toks.append(f\"\\\"{t}\\\"[{field}]\")\n",
    "        else:\n",
    "            toks.append(f\"{t}[{field}]\")\n",
    "    return \"(\"+\" OR \".join(toks)+\")\" if toks else \"\"\n",
    "\n",
    "def lang_filter(langs: List[str]) -> str:\n",
    "    toks=[f\"\\\"{L}\\\"[Language]\" for L in langs or []]\n",
    "    return \"(\"+\" OR \".join(toks)+\")\" if toks else \"\"\n",
    "\n",
    "def build_universe_query(protocol: dict) -> str:\n",
    "    P = or_block(protocol.get(\"population_terms\"), \"tiab\")\n",
    "    I = or_block(protocol.get(\"intervention_terms\"), \"tiab\")\n",
    "    A = or_block(protocol.get(\"must_have\"), \"tiab\")\n",
    "    parts=[p for p in [P,I,A] if p]\n",
    "    q = \" AND \".join(parts) if parts else \"\"\n",
    "    if protocol.get(\"languages\"):\n",
    "        q = f\"({q}) AND {lang_filter(protocol['languages'])}\" if q else lang_filter(protocol['languages'])\n",
    "    return q\n",
    "\n",
    "# -----------------------------\n",
    "# Validators\n",
    "# -----------------------------\n",
    "def _is_strlist(x): return isinstance(x, list) and all(isinstance(t,str) and t.strip() for t in x)\n",
    "def _max_words_ok(lst, max_words=3):\n",
    "    for t in lst or []:\n",
    "        if len(t.strip().split()) > max_words:\n",
    "            return False, t\n",
    "    return True, \"\"\n",
    "\n",
    "def validate_protocol(js: dict) -> Tuple[bool,str]:\n",
    "    req = [\"population_terms\",\"intervention_terms\",\"comparators_terms\",\"outcomes_terms\",\"must_have\",\"avoid\",\"designs_preference\",\"languages\",\"year_min\"]\n",
    "    for k in req:\n",
    "        if k not in js: return False, f\"missing key {k}\"\n",
    "    if not _is_strlist(js[\"population_terms\"]): return False,\"population_terms must be list[str]\"\n",
    "    if not _is_strlist(js[\"intervention_terms\"]): return False,\"intervention_terms must be list[str]\"\n",
    "    if not isinstance(js[\"comparators_terms\"], list): return False,\"comparators_terms list\"\n",
    "    if not _is_strlist(js[\"outcomes_terms\"]): return False,\"outcomes_terms list[str]\"\n",
    "    if not isinstance(js[\"must_have\"], list): return False,\"must_have list[str]\"\n",
    "    if not isinstance(js[\"avoid\"], list): return False,\"avoid list[str]\"\n",
    "    if js[\"designs_preference\"] not in KB[\"publication_types\"]: return False,\"designs_preference must be in KB.publication_types\"\n",
    "    if not set(js[\"languages\"]).issubset(set(KB[\"languages\"])): return False,\"languages must be subset of KB.languages\"\n",
    "    if not isinstance(js[\"year_min\"], int): return False,\"year_min int\"\n",
    "    # length constraints: each term <= 3 words\n",
    "    for key in [\"population_terms\",\"intervention_terms\",\"must_have\"]:\n",
    "        ok, bad = _max_words_ok(js[key], 3)\n",
    "        if not ok: return False, f\"{key} contains overly long phrase: '{bad}'. Use <=3-word tokens.\"\n",
    "    # anchor constraints\n",
    "    P_join = \" \".join(js[\"population_terms\"]).lower()\n",
    "    if not any(a in P_join for a in [\"pectus\",\"nuss\",\"mirpe\"]):\n",
    "        return False, \"population_terms must include MIRPE/Nuss/pectus anchors\"\n",
    "    I_join = \" \".join(js[\"intervention_terms\"]).lower()\n",
    "    if not any(\"cryoablat\" in I_join or \"cryoanalg\" in I_join or \"inc\" == t.lower() for t in js[\"intervention_terms\"]):\n",
    "        return False, \"intervention_terms must include cryoablation/cryoanalgesia/INC\"\n",
    "    # adult-only guard\n",
    "    if any(\"pediatric\" in t.lower() for t in js[\"population_terms\"] + js[\"avoid\"]):\n",
    "        # allowed in avoid, forbidden in population_terms\n",
    "        if any(\"pediatric\" in t.lower() for t in js[\"population_terms\"]):\n",
    "            return False,\"population_terms must not include pediatric\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "def validate_screener_output(js: dict) -> Tuple[bool,str]:\n",
    "    if \"pmid\" not in js or not isinstance(js[\"pmid\"], str): return False,\"pmid missing\"\n",
    "    chk = js.get(\"checklist\",{})\n",
    "    for k in [\"P\",\"I\",\"O\",\"D\"]:\n",
    "        if not isinstance(chk.get(k), bool): return False, f\"checklist.{k} must be bool\"\n",
    "    if js.get(\"decision\") not in [\"INCLUDE\",\"EXCLUDE\",\"BORDERLINE\"]:\n",
    "        return False,\"decision invalid\"\n",
    "    if \"reason\" not in js: return False,\"reason missing\"\n",
    "    # mesh_roles optional here; roles will be computed separately from actual MeSH list\n",
    "    return True,\"ok\"\n",
    "\n",
    "def validate_mesh_roles(js: dict) -> Tuple[bool,str]:\n",
    "    if \"pmid\" not in js or \"labels\" not in js: return False,\"missing keys\"\n",
    "    if not isinstance(js[\"labels\"], list): return False,\"labels list\"\n",
    "    for it in js[\"labels\"]:\n",
    "        if not isinstance(it, dict): return False,\"label must be dict\"\n",
    "        if \"mesh\" not in it or \"role\" not in it: return False,\"mesh+role required\"\n",
    "        if it[\"role\"] not in [\"P\",\"I\",\"O\",\"C\",\"G\",\"X\"]: return False,\"invalid role\"\n",
    "    return True,\"ok\"\n",
    "\n",
    "def validate_remediation(js: dict) -> Tuple[bool,str]:\n",
    "    allowed_ops = {\"DROP_TERM\",\"ADD_ANCHOR\",\"ADD_POP\",\"ADD_INT\",\"SIMPLIFY_TERM\",\"REMOVE_TERM\",\"BROADEN_DESIGN_FILTER\"}\n",
    "    if js.get(\"op\") not in allowed_ops: return False,\"op invalid\"\n",
    "    return True,\"ok\"\n",
    "\n",
    "# -----------------------------\n",
    "# Prompts\n",
    "# -----------------------------\n",
    "S1_SYSTEM = \"\"\"You are a rigorous protocol compiler. Return a LOCKED PROTOCOL with short, searchable tokens (each 1–3 words).\"\"\"\n",
    "\n",
    "def s1_user(nlq: str, kb: dict) -> str:\n",
    "    return f\"\"\"From the NLQ below, produce a STRICT protocol JSON.\n",
    "\n",
    "**Hard rules**\n",
    "- Use short tokens (each 1–3 words). DO NOT output long phrases like \"intraoperative intercostal nerve cryoablation for analgesia\".\n",
    "- Population MUST anchor to MIRPE/Nuss/pectus excavatum for ADULTS (pediatric forbidden in population).\n",
    "- Intervention MUST include intercostal nerve cryoablation / cryoanalgesia / INC (for analgesia).\n",
    "- designs_preference must be one of: {kb[\"publication_types\"]}\n",
    "- languages subset of: {kb[\"languages\"]}\n",
    "\n",
    "**Bad vs Good examples**\n",
    "Bad: \"intraoperative intercostal nerve cryoablation for analgesia\"\n",
    "Good: [\"intercostal nerve\",\"cryoablation\",\"analgesia\",\"INC\"]\n",
    "\n",
    "NLQ:\n",
    "<<<\n",
    "{nlq}\n",
    ">>>\n",
    "\n",
    "Return ONLY:\n",
    "BEGIN_JSON\n",
    "{{\"population_terms\": [\"adults\",\"Nuss\",\"MIRPE\",\"pectus excavatum\",\"minimally invasive repair\"],\n",
    " \"intervention_terms\": [\"intercostal nerve\",\"cryoablation\",\"cryoanalgesia\",\"INC\",\"analgesia\"],\n",
    " \"comparators_terms\": [\"thoracic epidural\",\"paravertebral block\",\"intercostal nerve block\",\"erector spinae plane block\",\"systemic multimodal analgesia\"],\n",
    " \"outcomes_terms\": [\"postoperative opioid consumption\",\"pain scores\",\"0-7 day pain\",\"discharge opioid use\"],\n",
    " \"must_have\": [\"MIRPE\",\"Nuss\",\"pectus excavatum\",\"cryoablation\"],\n",
    " \"avoid\": [\"pediatric\"],\n",
    " \"designs_preference\": \"Randomized Controlled Trial\",\n",
    " \"languages\": [\"english\",\"portuguese\",\"spanish\"],\n",
    " \"year_min\": 2015}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "S2_REMEDIATION_SYSTEM = \"You are a cautious query remediation assistant. You NEVER violate core constraints and you can remove or simplify problematic tokens.\"\n",
    "def s2_remediation_user(protocol: dict, query: str, count: int, why: str) -> str:\n",
    "    constraints = \"\"\"Core constraints:\n",
    "- Adults only\n",
    "- Population must mention MIRPE/Nuss/pectus excavatum\n",
    "- Intervention must be intercostal nerve cryoablation (INC)/cryoanalgesia for analgesia\n",
    "- Terms must be 1–3 words only (simplify long ones)\"\"\"\n",
    "    return f\"\"\"The universe query is too {why}. Current count={count}.\n",
    "Query:\n",
    "{query}\n",
    "\n",
    "{constraints}\n",
    "\n",
    "Propose ONE fix as JSON:\n",
    "- {{\"op\":\"SIMPLIFY_TERM\",\"where\":\"population|intervention|anchor\",\"term\":\"<existing long token>\",\"replacement\":\"<1–3 word token>\"}}\n",
    "- {{\"op\":\"REMOVE_TERM\",\"where\":\"population|intervention|anchor\",\"term\":\"<overly-specific token>\"}}\n",
    "- {{\"op\":\"ADD_POP\",\"term\":\"<short population token>\"}}\n",
    "- {{\"op\":\"ADD_INT\",\"term\":\"<short intervention token>\"}}\n",
    "- {{\"op\":\"ADD_ANCHOR\",\"term\":\"<short anchor token>\"}}\n",
    "\n",
    "Return ONLY:\n",
    "BEGIN_JSON\n",
    "{{\"op\":\"ADD_ANCHOR\",\"term\":\"pectus excavatum\"}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "S3_SCREEN_SYSTEM = \"You are a strict senior PRISMA screener. Use ONLY true/false booleans. Return JSON only.\"\n",
    "def s3_screen_user(protocol: dict, rec: dict) -> str:\n",
    "    return f\"\"\"Screen against protocol (ALL must be true for INCLUDE):\n",
    "- P: Title/abstract explicitly mentions pectus excavatum OR Nuss OR MIRPE AND adults.\n",
    "- I: Title/abstract explicitly mentions intercostal nerve cryoablation (INC) / cryoanalgesia for analgesia (not tumor/derm cryotherapy).\n",
    "- O: Reports/plans postoperative opioid consumption OR pain scores within 0–7 days.\n",
    "- D: Primary comparative study (RCT preferred; accept clearly comparative cohort/case-control).\n",
    "\n",
    "Return:\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{rec['pmid']}\",\n",
    "  \"checklist\": {{\"P\": false, \"I\": false, \"O\": false, \"D\": false}},\n",
    "  \"decision\":\"EXCLUDE|BORDERLINE|INCLUDE\",\n",
    "  \"reason\":\"<=1 line why\"}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "S3_MESH_ROLE_SYSTEM = \"You classify MeSH descriptors by role for PICOS.\"\n",
    "def s3_mesh_role_user(rec: dict) -> str:\n",
    "    return f\"\"\"Assign roles to THIS RECORD'S MeSH ONLY (do not invent new terms).\n",
    "Allowed roles: P (population/procedure context), I (intervention/analgesia), O (outcome), C (comparator), G (generic), X (irrelevant).\n",
    "\n",
    "MeSH descriptors:\n",
    "{rec.get('mesh',[])}\n",
    "\n",
    "Return ONLY:\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{rec['pmid']}\", \"labels\":[{{\"mesh\":\"Funnel Chest\",\"role\":\"P\"}}]}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "S35_PLAUS_SYS = \"You are the senior plausibility gate. Be ruthless and concise.\"\n",
    "def s35_plaus_user(protocol: dict, rec: dict) -> str:\n",
    "    return f\"\"\"Does the core topic plausibly match the protocol?\n",
    "Protocol core: adults + (pectus excavatum / MIRPE / Nuss) + intercostal nerve cryoablation for analgesia.\n",
    "\n",
    "Title: {rec['title']}\n",
    "Abstract: {rec['abstract'][:1000]}\n",
    "\n",
    "Return:\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{rec['pmid']}\", \"verdict\":\"PASS|FAIL\", \"why\":\"<=1 line\"}} \n",
    "END_JSON\"\"\"\n",
    "\n",
    "S4_STRATEGY_SYSTEM = \"You are a cautious filter strategist. Choose among allowed options only.\"\n",
    "def s4_strategy_user(protocol: dict, mesh_vocab: Dict[str,str]) -> str:\n",
    "    roles = defaultdict(list)\n",
    "    for m,r in mesh_vocab.items():\n",
    "        roles[r].append(m)\n",
    "    return f\"\"\"Build recommended filters based ONLY on included articles' MeSH (exact descriptors).\n",
    "- topic_filter: OR of P/I/O MeSH terms (use [MeSH Terms]); <= 15 tokens total; prefer P and I anchors.\n",
    "- design_filter: ONE publication type equal to designs_preference={protocol[\"designs_preference\"]} from {KB[\"publication_types\"]}.\n",
    "\n",
    "Available MeSH by role:\n",
    "P={roles.get('P',[])}\n",
    "I={roles.get('I',[])}\n",
    "O={roles.get('O',[])}\n",
    "\n",
    "Return ONLY:\n",
    "BEGIN_JSON\n",
    "{{\"topic_filter\":\"(Funnel Chest[MeSH Terms] OR Cryosurgery[MeSH Terms])\",\n",
    "  \"design_filter\":\"Randomized Controlled Trial[Publication Type]\"}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "S4_REMEDIATE_SYSTEM = \"You repair strategies with a single, constrained action.\"\n",
    "def s4_remediate_user(snapshot: dict) -> str:\n",
    "    return f\"\"\"Strategy failed.\n",
    "Snapshot:\n",
    "{json.dumps(snapshot, ensure_ascii=False)}\n",
    "\n",
    "Propose ONE:\n",
    "- {{\"op\":\"DROP_TERM\",\"term\":\"<MeSH>\",\"where\":\"topic\"}}\n",
    "- {{\"op\":\"ADD_ANCHOR\",\"term\":\"<short tiab token>\"}}\n",
    "- {{\"op\":\"BROADEN_DESIGN_FILTER\"}}\n",
    "\n",
    "Return ONLY:\n",
    "BEGIN_JSON\n",
    "{{\"op\":\"DROP_TERM\",\"term\":\"X\",\"where\":\"topic\"}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# S1: Protocol lockdown\n",
    "# -----------------------------\n",
    "def state1_protocol_lockdown(nlq: str) -> dict:\n",
    "    print(\"[S1] Protocol lockdown...\")\n",
    "    proto = get_validated_json(\n",
    "        QWEN_MODEL, S1_SYSTEM, s1_user(nlq, KB),\n",
    "        validator=validate_protocol,\n",
    "        template_hint=\"Each term must be 1–3 words. Anchors required: MIRPE/Nuss/pectus in Population; cryoablation/cryoanalgesia/INC in Intervention.\",\n",
    "        max_tokens=1536, dbg_stage=\"protocol\"\n",
    "    )\n",
    "    # Dedup + lowercase normalize where sensible\n",
    "    def dedup_keep_order(lst):\n",
    "        seen=set(); out=[]\n",
    "        for t in lst:\n",
    "            k=t.strip()\n",
    "            if not k or k.lower() in seen: continue\n",
    "            seen.add(k.lower()); out.append(k)\n",
    "        return out\n",
    "    for key in [\"population_terms\",\"intervention_terms\",\"comparators_terms\",\"outcomes_terms\",\"must_have\",\"avoid\",\"languages\"]:\n",
    "        proto[key] = dedup_keep_order(proto.get(key,[]))\n",
    "    print(\"  [S1] Locked protocol:\\n   \", json.dumps(proto, ensure_ascii=False))\n",
    "    return proto\n",
    "\n",
    "# -----------------------------\n",
    "# S2: Universe definition & remediation\n",
    "# -----------------------------\n",
    "def state2_universe(protocol: dict) -> Tuple[str, List[str]]:\n",
    "    print(\"[S2] Universe definition & sizing...\")\n",
    "    query = build_universe_query(protocol)\n",
    "    tries = 0\n",
    "    while True:\n",
    "        ids = esearch_all_ids(query, mindate=protocol[\"year_min\"], cap=UNIVERSE_MAX+500)\n",
    "        count = len(ids)\n",
    "        print(f\"   [Universe] try={tries} count={count} window=({UNIVERSE_MIN}, {UNIVERSE_MAX})\")\n",
    "        if UNIVERSE_MIN <= count <= UNIVERSE_MAX:\n",
    "            return query, ids\n",
    "        if tries >= 2:\n",
    "            if count < UNIVERSE_HARD_MIN:\n",
    "                raise SystemExit(f\"Fatal: universe too small ({count}<{UNIVERSE_HARD_MIN}).\")\n",
    "            print(f\"   [Universe] Proceeding with suboptimal size={count} and a WARNING.\")\n",
    "            return query, ids\n",
    "        why = \"narrow\" if count < UNIVERSE_MIN else \"broad\"\n",
    "        fix = get_validated_json(QWEN_MODEL, S2_REMEDIATION_SYSTEM,\n",
    "                                 s2_remediation_user(protocol, query, count, why),\n",
    "                                 validator=validate_remediation, max_tokens=512, dbg_stage=\"universe_remed\")\n",
    "        op = fix[\"op\"]\n",
    "        if op in (\"ADD_ANCHOR\",\"ADD_POP\",\"ADD_INT\"):\n",
    "            term = fix.get(\"term\",\"\").strip()\n",
    "            if not term: \n",
    "                tries += 1; continue\n",
    "            if op==\"ADD_ANCHOR\": protocol[\"must_have\"].append(term)\n",
    "            elif op==\"ADD_POP\": protocol[\"population_terms\"].append(term)\n",
    "            elif op==\"ADD_INT\": protocol[\"intervention_terms\"].append(term)\n",
    "        elif op==\"SIMPLIFY_TERM\":\n",
    "            where = fix.get(\"where\")\n",
    "            term = fix.get(\"term\",\"\").strip()\n",
    "            repl = fix.get(\"replacement\",\"\").strip()\n",
    "            target = {\"population\": \"population_terms\", \"intervention\":\"intervention_terms\", \"anchor\":\"must_have\"}.get(where)\n",
    "            if target and term and repl and target in protocol:\n",
    "                lst = protocol[target]\n",
    "                protocol[target] = [repl if x.strip().lower()==term.lower() else x for x in lst]\n",
    "        elif op==\"REMOVE_TERM\":\n",
    "            where = fix.get(\"where\")\n",
    "            term = fix.get(\"term\",\"\").strip().lower()\n",
    "            target = {\"population\": \"population_terms\", \"intervention\":\"intervention_terms\", \"anchor\":\"must_have\"}.get(where)\n",
    "            if target and term and target in protocol:\n",
    "                protocol[target] = [x for x in protocol[target] if x.strip().lower()!=term]\n",
    "        # rebuild\n",
    "        # also enforce 1–3 word rule post-change (defensive)\n",
    "        for key in [\"population_terms\",\"intervention_terms\",\"must_have\"]:\n",
    "            protocol[key] = [t for t in protocol[key] if len(t.split())<=3]\n",
    "        query = build_universe_query(protocol)\n",
    "        tries += 1\n",
    "\n",
    "# -----------------------------\n",
    "# S2.5: True PICO-weighted TF-IDF reranker\n",
    "# -----------------------------\n",
    "def safe_import_sklearn():\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        import numpy as np\n",
    "        return TfidfVectorizer, np\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def state2_5_rerank_universe(query: str, ids: List[str], protocol: dict) -> List[dict]:\n",
    "    print(\"[S2.5] Rerank universe with PICO-weighted TF-IDF...\")\n",
    "    fetch_ids = ids[:RERANK_FETCH_N]\n",
    "    docs=[]\n",
    "    for i in range(0, len(fetch_ids), 300):\n",
    "        chunk = fetch_ids[i:i+300]\n",
    "        xml = efetch_xml(chunk)\n",
    "        docs.extend(parse_pubmed_xml(xml))\n",
    "        time.sleep(0.34)\n",
    "    if not docs: return []\n",
    "\n",
    "    texts = [(d[\"title\"] or \"\") + \" \" + (d[\"abstract\"] or \"\") for d in docs]\n",
    "    TfidfVectorizer, np = safe_import_sklearn()\n",
    "    if TfidfVectorizer is None:\n",
    "        # fallback: keyword presence heuristic\n",
    "        def score_text(t):\n",
    "            low=t.lower(); s=0.0\n",
    "            for term in protocol[\"population_terms\"] + protocol[\"intervention_terms\"]:\n",
    "                if term.lower() in low: s += 1.5\n",
    "            for term in protocol[\"outcomes_terms\"]:\n",
    "                if term.lower() in low: s += 1.0\n",
    "            for term in protocol[\"avoid\"]:\n",
    "                if term.lower() in low: s -= 2.0\n",
    "            return s\n",
    "        for d,t in zip(docs, texts):\n",
    "            d[\"_score\"] = score_text(t)\n",
    "        docs.sort(key=lambda x:x.get(\"_score\",0.0), reverse=True)\n",
    "        return docs\n",
    "\n",
    "    # Real TF-IDF with weighted term columns\n",
    "    vec = TfidfVectorizer(min_df=2, ngram_range=(1,2), stop_words=\"english\")\n",
    "    X = vec.fit_transform(texts)  # shape (n_docs, n_terms)\n",
    "    vocab = vec.vocabulary_\n",
    "    # collect unique protocol tokens\n",
    "    def toks(lst): \n",
    "        return [w.lower() for w in lst or []]\n",
    "    P = toks(protocol[\"population_terms\"]); I = toks(protocol[\"intervention_terms\"]); O = toks(protocol[\"outcomes_terms\"]); A = toks(protocol[\"must_have\"])\n",
    "    weights = defaultdict(float)\n",
    "    for term in P: weights[term] += 1.5\n",
    "    for term in I: weights[term] += 1.5\n",
    "    for term in O: weights[term] += 1.0\n",
    "    for term in A: weights[term] += 0.5  # light anchor boost\n",
    "\n",
    "    # compute doc scores as sum(weight * tfidf_col)\n",
    "    import numpy as np\n",
    "    score = np.zeros(X.shape[0], dtype=float)\n",
    "    for term, w in weights.items():\n",
    "        if term in vocab:\n",
    "            col = vocab[term]\n",
    "            score += w * X[:, col].toarray().ravel()\n",
    "    # penalty for avoid tokens if present in vocab\n",
    "    for term in [t.lower() for t in protocol.get(\"avoid\",[])]:\n",
    "        if term in vocab:\n",
    "            col = vocab[term]\n",
    "            score -= 2.0 * X[:, col].toarray().ravel()\n",
    "\n",
    "    for d, s in zip(docs, score):\n",
    "        d[\"_score\"] = float(s)\n",
    "    docs.sort(key=lambda x:x.get(\"_score\",0.0), reverse=True)\n",
    "    return docs\n",
    "\n",
    "# -----------------------------\n",
    "# S3: Ground truth discovery (strict screener) + MeSH roles via LLM (actual MeSH only)\n",
    "# -----------------------------\n",
    "def screen_record(protocol: dict, rec: dict) -> dict:\n",
    "    js = get_validated_json(\n",
    "        SCREENER_MODEL, S3_SCREEN_SYSTEM, s3_screen_user(protocol, rec),\n",
    "        validator=validate_screener_output, max_tokens=1024, dbg_stage=\"screen\"\n",
    "    )\n",
    "    snippet = (rec[\"abstract\"] or \"\")[:500].replace(\"\\n\",\" \")\n",
    "    print(f'  [Screen] PMID {rec[\"pmid\"]} -> decision={js[\"decision\"]} checklist={js[\"checklist\"]} why={js[\"reason\"]}')\n",
    "    print(f'    Title: {rec[\"title\"][:160]}')\n",
    "    print(f'    Abstract: {snippet}{\"...\" if len(rec[\"abstract\"] or \"\")>500 else \"\"}')\n",
    "    return js\n",
    "\n",
    "def classify_mesh_roles(rec: dict) -> Dict[str,str]:\n",
    "    # ask LLM to role-tag THIS record's MeSH ONLY\n",
    "    js = get_validated_json(\n",
    "        QWEN_MODEL, S3_MESH_ROLE_SYSTEM, s3_mesh_role_user(rec),\n",
    "        validator=validate_mesh_roles, max_tokens=768, dbg_stage=\"mesh_roles\"\n",
    "    )\n",
    "    roles={}\n",
    "    record_mesh = set([m.strip() for m in (rec.get(\"mesh\") or []) if m])\n",
    "    for item in js.get(\"labels\",[]):\n",
    "        m=item.get(\"mesh\",\"\").strip()\n",
    "        r=item.get(\"role\",\"G\").strip()\n",
    "        if m in record_mesh:\n",
    "            roles[m]=r\n",
    "    return roles\n",
    "\n",
    "def state3_ground_truth(reranked_docs: List[dict], protocol: dict) -> Tuple[List[str], Dict[str,str]]:\n",
    "    print(\"[S3] Ground-truth discovery & vocabulary mining...\")\n",
    "    includes=[]\n",
    "    mesh_vocab = {}\n",
    "    for rec in reranked_docs[:GROUND_TOP_N]:\n",
    "        js = screen_record(protocol, rec)\n",
    "        if js[\"decision\"] == \"INCLUDE\" and all(js[\"checklist\"].get(k,False) for k in [\"P\",\"I\",\"O\",\"D\"]):\n",
    "            includes.append(rec)\n",
    "            # roles strictly from this record's actual MeSH (via LLM)\n",
    "            roles = classify_mesh_roles(rec)\n",
    "            for m,r in roles.items():\n",
    "                if r in [\"P\",\"I\",\"O\"]:  # keep only useful roles\n",
    "                    mesh_vocab[m]=r\n",
    "    pmids = [r[\"pmid\"] for r in includes]\n",
    "    print(f\"  [S3] Ground truth PMIDs: {pmids}\")\n",
    "    return pmids, mesh_vocab\n",
    "\n",
    "# -----------------------------\n",
    "# S3.5: Senior plausibility gate\n",
    "# -----------------------------\n",
    "def state3_5_plausibility(protocol: dict, included_pmids: List[str]) -> List[str]:\n",
    "    print(\"[S3.5] Senior plausibility spot-check...\")\n",
    "    if not included_pmids: return []\n",
    "    xml = efetch_xml(included_pmids[:300])\n",
    "    recs = {r[\"pmid\"]: r for r in parse_pubmed_xml(xml)}\n",
    "    keep=[]\n",
    "    for pmid in included_pmids:\n",
    "        rec = recs.get(pmid)\n",
    "        if not rec: continue\n",
    "        js = get_validated_json(QWEN_MODEL, S35_PLAUS_SYS, s35_plaus_user(protocol, rec),\n",
    "                                validator=lambda x: (x.get(\"verdict\") in (\"PASS\",\"FAIL\") and x.get(\"pmid\")==pmid, \"bad verdict or pmid\"),\n",
    "                                max_tokens=256, dbg_stage=\"plausibility\")\n",
    "        print(f'  [Plaus] PMID {pmid} -> {js[\"verdict\"]} : {js[\"why\"]}')\n",
    "        if js[\"verdict\"] == \"PASS\":\n",
    "            keep.append(pmid)\n",
    "    if len(keep) < MIN_GROUND_TRUTH:\n",
    "        raise SystemExit(f\"Fatal: insufficient plausible ground truth (got {len(keep)}/{MIN_GROUND_TRUTH}).\")\n",
    "    return keep\n",
    "\n",
    "# -----------------------------\n",
    "# S4: Strategy validation & refinement\n",
    "# -----------------------------\n",
    "def state4_validate_strategy(universe_query: str, ground_pmids: List[str], protocol: dict, mesh_vocab: Dict[str,str]) -> Tuple[dict, List[str]]:\n",
    "    print(\"[S4] Strategy validation & refinement...\")\n",
    "    filt = get_validated_json(QWEN_MODEL, S4_STRATEGY_SYSTEM, s4_strategy_user(protocol, mesh_vocab),\n",
    "                              validator=lambda x: (\"topic_filter\" in x and \"design_filter\" in x, \"missing fields\"),\n",
    "                              max_tokens=512, dbg_stage=\"strategy_build\")\n",
    "    attempts = 0\n",
    "    while attempts <= REMEDIATION_MAX_TRIES:\n",
    "        combined = f\"({universe_query}) AND ({filt['topic_filter']}) AND ({filt['design_filter']})\"\n",
    "        ids = esearch_all_ids(combined, mindate=protocol[\"year_min\"], cap=10000)\n",
    "        total = len(ids)\n",
    "        recall_ok = set(ground_pmids).issubset(set(ids))\n",
    "        precision_ok = PRECISION_WINDOW[0] <= total <= PRECISION_WINDOW[1]\n",
    "        print(f\"   [Strategy] try={attempts} total={total} recall_ok={recall_ok} precision_ok={precision_ok}\")\n",
    "        if recall_ok and precision_ok:\n",
    "            return {\"topic\":filt[\"topic_filter\"], \"design\":filt[\"design_filter\"]}, ids\n",
    "        if attempts == REMEDIATION_MAX_TRIES:\n",
    "            break\n",
    "        snapshot = {\"universe_query\": universe_query, \"filters\": filt, \"total\": total, \"ground_truth\": ground_pmids}\n",
    "        fix = get_validated_json(QWEN_MODEL, S4_REMEDIATE_SYSTEM, s4_remediate_user(snapshot),\n",
    "                                 validator=validate_remediation, max_tokens=256, dbg_stage=\"strategy_remed\")\n",
    "        op = fix[\"op\"]\n",
    "        if op == \"DROP_TERM\" and fix.get(\"where\") == \"topic\":\n",
    "            term = fix.get(\"term\",\"\").strip()\n",
    "            # remove a single OR'd MeSH term from topic_filter\n",
    "            # crude but effective: remove \"<term>[MeSH Terms]\" occurrences + surrounding ORs\n",
    "            filt[\"topic_filter\"] = re.sub(rf'\\s*\\(?{re.escape(term)}\\[MeSH Terms\\]\\s*OR\\s*', '(', filt[\"topic_filter\"])\n",
    "            filt[\"topic_filter\"] = re.sub(rf'\\s*OR\\s*{re.escape(term)}\\[MeSH Terms\\]\\s*\\)?', ')', filt[\"topic_filter\"])\n",
    "            filt[\"topic_filter\"] = re.sub(r'\\(\\s*\\)', '(*)', filt[\"topic_filter\"])\n",
    "        elif op == \"ADD_ANCHOR\" and fix.get(\"term\"):\n",
    "            protocol[\"must_have\"].append(fix[\"term\"])\n",
    "            universe_query = build_universe_query(protocol)\n",
    "        elif op == \"BROADEN_DESIGN_FILTER\":\n",
    "            filt[\"design_filter\"] = \"Clinical Trial[Publication Type]\"\n",
    "        attempts += 1\n",
    "    raise SystemExit(\"Fatal: strategy could not be validated within remediation budget.\")\n",
    "\n",
    "# -----------------------------\n",
    "# S5: Finalization\n",
    "# -----------------------------\n",
    "def state5_finalize(nlq: str, protocol: dict, universe_query: str, rec_filters: dict,\n",
    "                    ground_pmids: List[str], mesh_vocab: Dict[str,str], warnings: List[str]):\n",
    "    rq_embed = \"Adults undergoing MIRPE/Nuss; intercostal nerve cryoablation for analgesia; outcomes: opioid use & 0–7 day pain.\"\n",
    "    artifacts = {\n",
    "        \"locked_protocol\": protocol,\n",
    "        \"universe_query\": universe_query,\n",
    "        \"recommended_filters\": rec_filters,\n",
    "        \"ground_truth_pmids\": ground_pmids,\n",
    "        \"mesh_vernaculum\": mesh_vocab,\n",
    "        \"warnings\": warnings,\n",
    "        \"research_question_string_for_embedding\": rq_embed,\n",
    "        \"nlq\": nlq\n",
    "    }\n",
    "    ARTIFACTS_JSON.write_text(json.dumps(artifacts, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"==================== SNIFF REPORT ====================\")\n",
    "    lines.append(\"NLQ:\\n  \" + textwrap.shorten(nlq, 220))\n",
    "    lines.append(\"\\nLOCKED PROTOCOL:\")\n",
    "    lines.append(\"  \" + json.dumps(protocol, ensure_ascii=False))\n",
    "    lines.append(\"\\nUNIVERSE:\")\n",
    "    lines.append(f\"  Query: {universe_query}\")\n",
    "    lines.append(\"\\nRECOMMENDED FILTERS:\")\n",
    "    lines.append(f\"  topic:  {rec_filters['topic']}\")\n",
    "    lines.append(f\"  design: {rec_filters['design']}\")\n",
    "    lines.append(\"\\nGROUND TRUTH PMIDs:\")\n",
    "    lines.append(\"  \" + \", \".join(ground_pmids))\n",
    "    if mesh_vocab:\n",
    "        top = \", \".join([f\"{m}({r})\" for m,r in list(mesh_vocab.items())[:20]])\n",
    "        lines.append(\"\\nMESH vocab (top):\")\n",
    "        lines.append(\"  \" + top)\n",
    "    if warnings:\n",
    "        lines.append(\"\\nWARNINGS:\")\n",
    "        for w in warnings: lines.append(f\"  - {w}\")\n",
    "    lines.append(\"\\nArtifacts saved to: \" + str(OUTDIR))\n",
    "    lines.append(\"================== END OF REPORT =====================\")\n",
    "    REPORT_TXT.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(\"\\n\".join(lines))\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN ENGINE\n",
    "# -----------------------------\n",
    "def sniff_engine(nlq: str):\n",
    "    warnings=[]\n",
    "    proto = state1_protocol_lockdown(nlq)\n",
    "    universe_query, u_ids = state2_universe(proto)\n",
    "    reranked_docs = state2_5_rerank_universe(universe_query, u_ids, proto)\n",
    "    gt_pmids, mesh_vocab = state3_ground_truth(reranked_docs, proto)\n",
    "    gt_pmids = state3_5_plausibility(proto, gt_pmids)\n",
    "    rec_filters, combined_ids = state4_validate_strategy(universe_query, gt_pmids, proto, mesh_vocab)\n",
    "    state5_finalize(nlq, proto, universe_query, rec_filters, gt_pmids, mesh_vocab, warnings)\n",
    "\n",
    "# -----------------------------\n",
    "# RUN EXAMPLE\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    USER_NLQ = \"\"\"Population = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\n",
    "Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during MIRPE/Nuss (the intervention of interest is INC, not the surgery).\n",
    "Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia.\n",
    "Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days.\n",
    "Study designs = RCTs preferred; if RCTs absent, include comparative cohort/case-control.\n",
    "Year_min = 2015.\n",
    "Languages = English, Portuguese, Spanish.\"\"\"\n",
    "    sniff_engine(USER_NLQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af1669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S1] Protocol lockdown...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'designs_primary'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 861\u001b[39m\n\u001b[32m    850\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    851\u001b[39m     USER_NLQ = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[33mPopulation = children/adolescents undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\u001b[39m\n\u001b[32m    853\u001b[39m \u001b[33mIntervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    859\u001b[39m \u001b[33mScreening notes: Be conservative; INCLUDE if P & I present and (O or D) is present; do not exclude for lack of exact day window if acute postop outcomes are clearly reported.\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     \u001b[43msniff_engine_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUSER_NLQ\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 818\u001b[39m, in \u001b[36msniff_engine_run\u001b[39m\u001b[34m(USER_NLQ)\u001b[39m\n\u001b[32m    816\u001b[39m warnings=[]\n\u001b[32m    817\u001b[39m \u001b[38;5;66;03m# S1\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m protocol = \u001b[43mstate1_protocol_lockdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUSER_NLQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# S2\u001b[39;00m\n\u001b[32m    821\u001b[39m universe_query, universe_count, universe_ids = state2_universe(protocol)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 641\u001b[39m, in \u001b[36mstate1_protocol_lockdown\u001b[39m\u001b[34m(nlq)\u001b[39m\n\u001b[32m    639\u001b[39m     system = PROTO_SYSTEM\n\u001b[32m    640\u001b[39m     \u001b[38;5;66;03m# Add explicit guardrails/examples to discourage long tokens\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m     user = \u001b[43mproto_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKB\u001b[49m\u001b[43m)\u001b[49m + \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    642\u001b[39m \n\u001b[32m    643\u001b[39m \u001b[33mGuidance:\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[33m- BAD token example: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mintraoperative intercostal nerve cryoablation for analgesia\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    645\u001b[39m \u001b[33m- GOOD tokens: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mintercostal nerve\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcryoablation\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcryoanalgesia\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mINC\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33manalgesia\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    647\u001b[39m     proto = get_validated_json(QWEN_MODEL, system, user, validate_protocol, retries=\u001b[32m3\u001b[39m, max_tokens=\u001b[32m2048\u001b[39m)\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m proto.get(\u001b[33m\"\u001b[39m\u001b[33mneeds_clarification\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 282\u001b[39m, in \u001b[36mproto_user\u001b[39m\u001b[34m(nlq, kb)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mproto_user\u001b[39m(nlq: \u001b[38;5;28mstr\u001b[39m, kb: Dict[\u001b[38;5;28mstr\u001b[39m,Any]) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mNatural-Language Question:\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[33m<<<\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mnlq.strip()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33m>>>\u001b[39m\n\u001b[32m    280\u001b[39m \n\u001b[32m    281\u001b[39m \u001b[33mKnowledge Base (valid choices):\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mjson.dumps({\u001b[33m\"\u001b[39m\u001b[33mdesigns_primary\u001b[39m\u001b[33m\"\u001b[39m:\u001b[43mKB\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdesigns_primary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlanguages\u001b[39m\u001b[33m\"\u001b[39m:KB[\u001b[33m\"\u001b[39m\u001b[33mlanguages\u001b[39m\u001b[33m\"\u001b[39m]},\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    283\u001b[39m \n\u001b[32m    284\u001b[39m \u001b[33mOutput schema:\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnarrative_question\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<1 paragraph restatement>\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33minclusion_criteria\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m],\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexclusion_criteria\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m],\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mscreening_rules_note\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33muser_notes\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<verbatim any adjunct/instructions embedded in NLQ>\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mllm_guidance\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<short additional instructions inferred>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[33m  \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m,\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpico_tokens\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m],\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m],\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m],\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m]\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[33m  \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m,\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33manchors_must_have\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m],   // topical anchors to enforce (e.g., MIRPE, Nuss)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mavoid_terms\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m],\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdesigns_preference\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<ONE of designs_primary>\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdeterministic_filters\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33m     \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlanguages\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m],  // subset of KB.languages\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[33m     \u001b[39m\u001b[33m\"\u001b[39m\u001b[33myear_min\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: 2015\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[33m  \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m,\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mneeds_clarification\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: false,\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mclarification_request\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'designs_primary'"
     ]
    }
   ],
   "source": [
    "# sniff_validation_engine_v3_1.py\n",
    "# Refactored \"Sniff Validation Engine\" (state-machine architecture)\n",
    "# - Single \"universe\" query + validated filters (no legacy BROAD/FOCUSED)\n",
    "# - Deterministic prefilter (language/year/design) BEFORE LLM\n",
    "# - PICO-weighted TF-IDF re-ranker (true TF-IDF × role weights)\n",
    "# - Strict screener with Ask-Validate-Retry JSON\n",
    "# - Senior plausibility check (second LLM pass) to prevent topic drift\n",
    "# - Ground-truth vocabulary (MeSH) only from confirmed INCLUDEs\n",
    "# - Robust model switching with idle TTL and conservative waits to avoid CPU fallback\n",
    "#\n",
    "# Requirements:\n",
    "#   - LM Studio running at LMSTUDIO_BASE (default http://127.0.0.1:1234)\n",
    "#   - Two local models served by LM Studio:\n",
    "#       QWEN_MODEL  (for protocol, remediation, plausibility)\n",
    "#       SCREENER_MODEL (fast model for checklist screening)\n",
    "#   - Internet for NCBI E-utilities\n",
    "#\n",
    "# Usage:\n",
    "#   1) Edit USER_NLQ below (natural language RQ; you may include notes for screening).\n",
    "#   2) Optionally edit constants (MODEL names, thresholds) or set via env vars.\n",
    "#   3) Run as a single cell/script. See printed summary + output files in OUT_DIR.\n",
    "#\n",
    "# Outputs:\n",
    "#   - sniff_report.txt  : human-readable report of states, warnings, and final strategy\n",
    "#   - sniff_artifacts.json : machine-readable details (locked protocol, universe query,\n",
    "#                            recommended filters, ground truth PMIDs, MeSH vernaculum,\n",
    "#                            research_question_string_for_embedding, warnings)\n",
    "#\n",
    "# NOTE: We DO NOT hard-filter by \"Humans\" or MeSH age bands deterministically.\n",
    "#       Deterministic gates: year, language, and publication type (design allowlist).\n",
    "#       The LLM uses PubTypes + MeSH contextually during screening.\n",
    "\n",
    "import os, json, time, re, textwrap, pathlib, random, math\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Any, Tuple, Callable, Optional\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# ----------------------------\n",
    "# Config / Constants\n",
    "# ----------------------------\n",
    "LMSTUDIO_BASE = os.getenv(\"LMSTUDIO_BASE\", \"http://127.0.0.1:1234\")\n",
    "QWEN_MODEL    = os.getenv(\"QWEN_MODEL\", \"qwen/qwen3-4b\")\n",
    "SCREENER_MODEL= os.getenv(\"SCREENER_MODEL\", \"gemma-3n-e2b-it\")  # fast checklist screener\n",
    "\n",
    "ENTREZ_EMAIL   = os.getenv(\"ENTREZ_EMAIL\", \"you@example.com\")\n",
    "ENTREZ_API_KEY = os.getenv(\"ENTREZ_API_KEY\", \"\")\n",
    "\n",
    "HTTP_TIMEOUT   = int(os.getenv(\"HTTP_TIMEOUT\", \"300\"))\n",
    "MODEL_TTL_SEC  = float(os.getenv(\"MODEL_TTL_SEC\", \"5.0\"))  # idle TTL hint (LM Studio)\n",
    "MODEL_SWAP_WAIT= float(os.getenv(\"MODEL_SWAP_WAIT\", \"10.0\"))  # conservative wait between model swaps\n",
    "\n",
    "OUT_DIR = pathlib.Path(\"sniff_out\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Universe sizing thresholds\n",
    "UNIVERSE_TARGET = (50, 10000)   # ideal window\n",
    "UNIVERSE_HARD_MIN = 25          # if final count < 25 after remediation -> terminate\n",
    "\n",
    "# Rerank / screening sizes\n",
    "UNIVERSE_FETCH_MAX = 800        # number of PubMed records to fetch for rerank (cap)\n",
    "SCREEN_TOP_K       = 60         # how many (after rerank+prefilter) to send to screener\n",
    "\n",
    "# Screener rules\n",
    "SCREENER_RETRY_MAX = 3\n",
    "PLAUSIBILITY_MIN_INCLUDES = 3   # need ≥ this many includes, post-plausibility, or terminate\n",
    "\n",
    "# Role weights for PICO TF-IDF reranker\n",
    "WEIGHTS = {\n",
    "    \"P\": 1.5,\n",
    "    \"I\": 1.75,\n",
    "    \"C\": 1.0,\n",
    "    \"O\": 1.0,\n",
    "    \"ANCHOR\": 2.0,\n",
    "    \"AVOID\": -2.5\n",
    "}\n",
    "\n",
    "# PubMed E-utilities base + headers\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "HEADERS = {\"User-Agent\": \"sniff-validation-engine/3.1 (+local)\", \"Accept\": \"application/json\"}\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities: LM Studio model management\n",
    "# ----------------------------\n",
    "class ModelManager:\n",
    "    def __init__(self, base: str, idle_ttl_sec: float = MODEL_TTL_SEC, swap_wait: float = MODEL_SWAP_WAIT):\n",
    "        self.base = base.rstrip(\"/\")\n",
    "        self.idle_ttl = idle_ttl_sec\n",
    "        self.swap_wait = swap_wait\n",
    "        self.current_model = None\n",
    "        self.last_used_ts = 0.0\n",
    "\n",
    "    def _maybe_wait_for_idle_eviction(self):\n",
    "        now = time.time()\n",
    "        idle = now - self.last_used_ts\n",
    "        if idle < self.idle_ttl:\n",
    "            time.sleep(self.idle_ttl - idle)\n",
    "        # conservative extra wait to allow LM Studio to evict models\n",
    "        time.sleep(max(0.0, self.swap_wait - self.idle_ttl))\n",
    "\n",
    "    def _best_effort_unload_all(self):\n",
    "        # LM Studio does not officially document unload; try likely endpoints, ignore errors.\n",
    "        for path in [\"/v1/models/unload_all\", \"/v1/engines/unload_all\", \"/v1/models/unload\"]:\n",
    "            try:\n",
    "                requests.post(self.base + path, timeout=3.0)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def switch(self, model: str):\n",
    "        if self.current_model and self.current_model != model:\n",
    "            # allow time for the previous model to be evicted\n",
    "            self._maybe_wait_for_idle_eviction()\n",
    "            self._best_effort_unload_all()\n",
    "        self.current_model = model\n",
    "        self.last_used_ts = time.time()\n",
    "\n",
    "    def mark_used(self):\n",
    "        self.last_used_ts = time.time()\n",
    "\n",
    "MM = ModelManager(LMSTUDIO_BASE)\n",
    "\n",
    "def lm_chat(model: str, system: str, user: str, temperature=0.0, max_tokens=2048) -> str:\n",
    "    MM.switch(model)\n",
    "    url = f\"{LMSTUDIO_BASE.rstrip('/')}/v1/chat/completions\"\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "        \"temperature\": float(temperature),\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"stream\": False\n",
    "    }\n",
    "    r = requests.post(url, json=body, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    MM.mark_used()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# ----------------------------\n",
    "# JSON extraction & Ask-Validate-Retry\n",
    "# ----------------------------\n",
    "_BEGIN = re.compile(r\"BEGIN_JSON\\s*\", re.I)\n",
    "_END   = re.compile(r\"\\s*END_JSON\", re.I)\n",
    "FENCE  = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)```\", re.I)\n",
    "\n",
    "def _sanitize_json_str(s: str) -> str:\n",
    "    s = s.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n",
    "    s = re.sub(r\",\\s*(\\}|\\])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_json_block_or_fence(txt: str) -> str:\n",
    "    blocks = []\n",
    "    pos=0\n",
    "    while True:\n",
    "        m1 = _BEGIN.search(txt, pos)\n",
    "        if not m1: break\n",
    "        m2 = _END.search(txt, m1.end())\n",
    "        if not m2: break\n",
    "        blocks.append(txt[m1.end():m2.start()])\n",
    "        pos = m2.end()\n",
    "    if blocks:\n",
    "        return _sanitize_json_str(blocks[-1])\n",
    "\n",
    "    fences = FENCE.findall(txt)\n",
    "    if fences:\n",
    "        return _sanitize_json_str(fences[-1])\n",
    "\n",
    "    # last balanced {...}\n",
    "    s = txt\n",
    "    last_obj=None; stack=0; start=None\n",
    "    for i,ch in enumerate(s):\n",
    "        if ch=='{':\n",
    "            if stack==0: start=i\n",
    "            stack+=1\n",
    "        elif ch=='}':\n",
    "            if stack>0:\n",
    "                stack-=1\n",
    "                if stack==0 and start is not None:\n",
    "                    last_obj = s[start:i+1]\n",
    "    if last_obj:\n",
    "        return _sanitize_json_str(last_obj)\n",
    "    raise ValueError(\"No JSON-like content found\")\n",
    "\n",
    "STRICT_JSON_RULES = (\n",
    "  \"Return ONLY one JSON object. No analysis, no preface, no notes. \"\n",
    "  \"Wrap it EXACTLY with:\\nBEGIN_JSON\\n{...}\\nEND_JSON\"\n",
    ")\n",
    "\n",
    "def get_validated_json(\n",
    "    model: str,\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    validator: Callable[[Dict[str,Any]], Tuple[bool,str]],\n",
    "    retries: int = 3,\n",
    "    max_tokens: int = 2048\n",
    ") -> Dict[str,Any]:\n",
    "    history_user = user_prompt\n",
    "    for i in range(retries):\n",
    "        raw = lm_chat(model, system_prompt, history_user + \"\\n\\n\" + STRICT_JSON_RULES, max_tokens=max_tokens)\n",
    "        try:\n",
    "            js = json.loads(extract_json_block_or_fence(raw))\n",
    "        except Exception as e:\n",
    "            err = f\"malformed JSON: {e}\"\n",
    "            if i == retries-1:\n",
    "                raise SystemExit(f\"Fatal: LLM failed to produce valid JSON after retries. Last error: {err}\")\n",
    "            history_user += f\"\\n\\nYour previous output was invalid due to: {err}\\nPlease fix and return a single valid JSON object.\"\n",
    "            continue\n",
    "        ok, why = validator(js)\n",
    "        if ok:\n",
    "            return js\n",
    "        if i == retries-1:\n",
    "            raise SystemExit(f\"Fatal: LLM JSON schema invalid after retries: {why}\")\n",
    "        history_user += f\"\\n\\nYour previous JSON failed validation: {why}\\nPlease correct your output and adhere to the required schema.\"\n",
    "\n",
    "# ----------------------------\n",
    "# KB defaults (designs, languages, pubtype map)\n",
    "# ----------------------------\n",
    "KB_PATH = pathlib.Path(\"system_knowledge_base.json\")\n",
    "KB_DEFAULT = {\n",
    "    \"publication_types_allowable\": [\n",
    "        \"Randomized Controlled Trial\",\n",
    "        \"Controlled Clinical Trial\",\n",
    "        \"Clinical Trial\",\n",
    "        \"Comparative Study\",\n",
    "        \"Cohort Studies\",\n",
    "        \"Case-Control Studies\",\n",
    "        \"Observational Study\",\n",
    "        \"Multicenter Study\",\n",
    "        \"Cross-Sectional Studies\",\n",
    "        \"Clinical Trial Protocol\",\n",
    "        \"Evaluation Study\"\n",
    "    ],\n",
    "    \"languages\": [\"english\",\"spanish\",\"portuguese\",\"french\",\"german\",\"italian\",\"chinese\",\"japanese\",\"korean\"],\n",
    "    \"designs_primary\": [\"Randomized Controlled Trial\",\"Controlled Clinical Trial\",\"Clinical Trial\"],\n",
    "    \"designs_secondary\": [\"Comparative Study\",\"Cohort Studies\",\"Case-Control Studies\",\"Observational Study\",\"Multicenter Study\",\"Evaluation Study\"],\n",
    "    \"pubtype_aliases\": {\n",
    "        \"Randomized Controlled Trial\": [\"Randomized Controlled Trial\"],\n",
    "        \"Controlled Clinical Trial\": [\"Controlled Clinical Trial\"],\n",
    "        \"Clinical Trial\": [\"Clinical Trial\"],\n",
    "        \"Comparative Study\": [\"Comparative Study\"],\n",
    "        \"Cohort Studies\": [\"Cohort Studies\",\"Prospective Studies\",\"Retrospective Studies\"],\n",
    "        \"Case-Control Studies\": [\"Case-Control Studies\"],\n",
    "        \"Observational Study\": [\"Observational Study\"],\n",
    "        \"Multicenter Study\": [\"Multicenter Study\"],\n",
    "        \"Cross-Sectional Studies\": [\"Cross-Sectional Studies\"],\n",
    "        \"Clinical Trial Protocol\": [\"Clinical Trial Protocol\",\"Study Protocols\"],\n",
    "        \"Evaluation Study\": [\"Evaluation Study\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_or_init_kb() -> Dict[str,Any]:\n",
    "    if KB_PATH.exists():\n",
    "        try:\n",
    "            on_disk = json.loads(KB_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            # if corrupted, reset to defaults\n",
    "            KB_PATH.write_text(json.dumps(KB_DEFAULT, indent=2), encoding=\"utf-8\")\n",
    "            return KB_DEFAULT\n",
    "\n",
    "        # Merge defaults → fill any missing keys from KB_DEFAULT\n",
    "        merged = dict(KB_DEFAULT)\n",
    "        for k, v in on_disk.items():\n",
    "            merged[k] = v\n",
    "\n",
    "        # Persist the merged file so future runs are stable\n",
    "        KB_PATH.write_text(json.dumps(merged, indent=2), encoding=\"utf-8\")\n",
    "        return merged\n",
    "\n",
    "    KB_PATH.write_text(json.dumps(KB_DEFAULT, indent=2), encoding=\"utf-8\")\n",
    "    return KB_DEFAULT\n",
    "\n",
    "\n",
    "KB = load_or_init_kb()\n",
    "\n",
    "# ----------------------------\n",
    "# State 1: Protocol Lockdown\n",
    "# ----------------------------\n",
    "PROTO_SYSTEM = \"\"\"You are designing a structured, search-ready SR protocol from a natural-language question.\n",
    "\n",
    "Produce a protocol that includes BOTH narrative fields for LLMs and structured fields for code.\n",
    "\n",
    "Rules:\n",
    "- Use concise search tokens for P/I/C/O (each token ≤ 3-4 words). Avoid overlong phrases.\n",
    "- Populate 'designs_preference' by selecting ONE from the provided KB 'designs_primary'.\n",
    "- 'deterministic_filters' MUST include: languages (subset of KB.languages) and year_min (from user or a reasonable default).\n",
    "- Do not hallucinate comparators or outcomes not implied; it's OK to leave lists empty if not provided.\n",
    "- If the question is incoherent or underspecified, set \"needs_clarification\"=true and write a short \"clarification_request\".\n",
    "\n",
    "Return ONLY JSON as requested.\"\"\"\n",
    "\n",
    "def proto_user(nlq: str, kb: Dict[str,Any]) -> str:\n",
    "    kb_view = {\n",
    "        \"designs_primary\": kb.get(\"designs_primary\", KB_DEFAULT[\"designs_primary\"]),\n",
    "        \"languages\": kb.get(\"languages\", KB_DEFAULT[\"languages\"])\n",
    "    }\n",
    "    return f\"\"\"Natural-Language Question:\n",
    "<<<\n",
    "{nlq.strip()}\n",
    ">>>\n",
    "\n",
    "Knowledge Base (valid choices):\n",
    "{json.dumps({\"designs_primary\":KB[\"designs_primary\"], \"languages\":KB[\"languages\"]}, indent=2)}\n",
    "\n",
    "Output schema:\n",
    "{{\n",
    "  \"narrative_question\": \"<1 paragraph restatement>\",\n",
    "  \"inclusion_criteria\": [\"...\",\"...\"],\n",
    "  \"exclusion_criteria\": [\"...\"],\n",
    "  \"screening_rules_note\": {{\n",
    "    \"user_notes\": \"<verbatim any adjunct/instructions embedded in NLQ>\",\n",
    "    \"llm_guidance\": \"<short additional instructions inferred>\"\n",
    "  }},\n",
    "  \"pico_tokens\": {{\n",
    "    \"P\": [\"...\"],\n",
    "    \"I\": [\"...\"],\n",
    "    \"C\": [\"...\"],\n",
    "    \"O\": [\"...\"]\n",
    "  }},\n",
    "  \"anchors_must_have\": [\"...\"],   // topical anchors to enforce (e.g., MIRPE, Nuss)\n",
    "  \"avoid_terms\": [\"...\"],\n",
    "  \"designs_preference\": \"<ONE of designs_primary>\",\n",
    "  \"deterministic_filters\": {{\n",
    "     \"languages\": [\"...\"],  // subset of KB.languages\n",
    "     \"year_min\": 2015\n",
    "  }},\n",
    "  \"needs_clarification\": false,\n",
    "  \"clarification_request\": \"\"\n",
    "}}\"\"\"\n",
    "\n",
    "def validate_protocol(js: Dict[str,Any]) -> Tuple[bool,str]:\n",
    "    try:\n",
    "        # minimal schema checks\n",
    "        req_top = [\"narrative_question\",\"inclusion_criteria\",\"exclusion_criteria\",\n",
    "                   \"screening_rules_note\",\"pico_tokens\",\"anchors_must_have\",\n",
    "                   \"avoid_terms\",\"designs_preference\",\"deterministic_filters\",\n",
    "                   \"needs_clarification\",\"clarification_request\"]\n",
    "        for k in req_top:\n",
    "            if k not in js: return False, f\"missing key: {k}\"\n",
    "        if not isinstance(js[\"pico_tokens\"], dict): return False, \"pico_tokens must be object\"\n",
    "        for k in [\"P\",\"I\",\"C\",\"O\"]:\n",
    "            if k not in js[\"pico_tokens\"]: return False, f\"pico_tokens missing {k}\"\n",
    "            if not isinstance(js[\"pico_tokens\"][k], list): return False, f\"pico_tokens[{k}] must be list\"\n",
    "        df = js[\"deterministic_filters\"]\n",
    "        if not isinstance(df.get(\"languages\",[]), list) or not df.get(\"languages\"):\n",
    "            return False, \"languages must be non-empty list\"\n",
    "\n",
    "        y = df.get(\"year_min\", 0)\n",
    "        if isinstance(y, str) and y.isdigit():\n",
    "            df[\"year_min\"] = int(y)\n",
    "        elif not isinstance(y, int):\n",
    "            return False, \"year_min must be int or numeric string\"\n",
    "\n",
    "        if js[\"designs_preference\"] not in KB[\"designs_primary\"]:\n",
    "            return False, \"designs_preference must be one of KB.designs_primary\"\n",
    "\n",
    "        # keep the short-token safeguard\n",
    "        long_bad = [t for t in (js[\"pico_tokens\"][\"P\"]+js[\"pico_tokens\"][\"I\"]+js[\"pico_tokens\"][\"C\"]+js[\"pico_tokens\"][\"O\"]) if len(t.split())>5]\n",
    "        if long_bad:\n",
    "            return False, f\"tokens too long: {long_bad[:3]}\"\n",
    "        return True, \"\"\n",
    "    except Exception as e:\n",
    "        return False, f\"exception in protocol validation: {e}\"\n",
    "\n",
    "# ----------------------------\n",
    "# PubMed: search & fetch\n",
    "# ----------------------------\n",
    "def esearch_ids(term: str, mindate: Optional[int], retmax: int = 5000) -> Tuple[int, List[str]]:\n",
    "    p = {\"db\":\"pubmed\",\"retmode\":\"json\",\"term\":term,\"retmax\":retmax,\"email\":ENTREZ_EMAIL,\"usehistory\":\"y\"}\n",
    "    if ENTREZ_API_KEY: p[\"api_key\"]=ENTREZ_API_KEY\n",
    "    if mindate: p[\"mindate\"]=str(mindate)\n",
    "    r = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params=p, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    js = r.json().get(\"esearchresult\", {})\n",
    "    count = int(js.get(\"count\",\"0\"))\n",
    "    webenv = js.get(\"webenv\"); qk = js.get(\"querykey\")\n",
    "    if not count or not webenv or not qk:\n",
    "        return 0, []\n",
    "    r2 = requests.get(f\"{EUTILS}/esearch.fcgi\", headers=HEADERS, params={\n",
    "        \"db\":\"pubmed\",\"retmode\":\"json\",\"retmax\":retmax,\"retstart\":0,\"email\":ENTREZ_EMAIL,\"WebEnv\":webenv,\"query_key\":qk,\n",
    "        **({\"api_key\":ENTREZ_API_KEY} if ENTREZ_API_KEY else {})\n",
    "    }, timeout=HTTP_TIMEOUT)\n",
    "    r2.raise_for_status()\n",
    "    ids = r2.json().get(\"esearchresult\",{}).get(\"idlist\",[])\n",
    "    return count, [str(x) for x in ids]\n",
    "\n",
    "def efetch_xml(pmids: List[str]) -> str:\n",
    "    if not pmids: return \"\"\n",
    "    params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"id\":\",\".join(pmids),\"email\":ENTREZ_EMAIL}\n",
    "    if ENTREZ_API_KEY: params[\"api_key\"]=ENTREZ_API_KEY\n",
    "    r = requests.get(f\"{EUTILS}/efetch.fcgi\", headers={\"User-Agent\":\"sniff-validation-engine/3.1\"}, params=params, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def parse_pubmed_xml(xml_text: str) -> List[Dict[str,Any]]:\n",
    "    out = []\n",
    "    if not xml_text.strip(): return out\n",
    "    root = ET.fromstring(xml_text)\n",
    "    def _join(node):\n",
    "        if node is None: return \"\"\n",
    "        try: return \"\".join(node.itertext())\n",
    "        except Exception: return node.text or \"\"\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = art.findtext(\".//PMID\") or \"\"\n",
    "        title = _join(art.find(\".//ArticleTitle\")).strip()\n",
    "        abs_nodes = art.findall(\".//Abstract/AbstractText\")\n",
    "        abstract = \" \".join(_join(n).strip() for n in abs_nodes) if abs_nodes else \"\"\n",
    "        year = None\n",
    "        for path in (\".//ArticleDate/Year\",\".//PubDate/Year\",\".//DateCreated/Year\",\".//PubDate/MedlineDate\"):\n",
    "            s = art.findtext(path)\n",
    "            if s:\n",
    "                m = re.search(r\"\\d{4}\", s)\n",
    "                if m: year = int(m.group(0)); break\n",
    "        lang = art.findtext(\".//Language\") or None\n",
    "        pubtypes = [pt.text for pt in art.findall(\".//PublicationTypeList/PublicationType\") if pt.text]\n",
    "        mesh = [mh.findtext(\"./DescriptorName\") for mh in art.findall(\".//MeshHeadingList/MeshHeading\") if mh.findtext(\"./DescriptorName\")]\n",
    "        out.append({\"pmid\":pmid,\"title\":title,\"abstract\":abstract,\"year\":year,\"language\":lang,\"pubtypes\":pubtypes,\"mesh\":mesh})\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# Query assembly, remediation\n",
    "# ----------------------------\n",
    "def or_block(terms: List[str], field=\"tiab\") -> str:\n",
    "    toks=[]\n",
    "    for t in terms:\n",
    "        t=t.strip()\n",
    "        if not t: continue\n",
    "        if \" \" in t or \"-\" in t:\n",
    "            toks.append(f\"\\\"{t}\\\"[{field}]\")\n",
    "        else:\n",
    "            toks.append(f\"{t}[{field}]\")\n",
    "    if not toks: return \"\"\n",
    "    return \"(\" + \" OR \".join(toks) + \")\"\n",
    "\n",
    "def build_universe_query(P: List[str], I: List[str], anchors: List[str]) -> str:\n",
    "    Pq = or_block(P, \"tiab\"); Iq = or_block(I, \"tiab\")\n",
    "    Aq = or_block(anchors, \"tiab\") if anchors else \"\"\n",
    "    parts = [x for x in [Pq, Iq, Aq] if x]\n",
    "    return \" AND \".join(parts)\n",
    "\n",
    "REM_SYS = \"\"\"You are a search strategy repair assistant. The current query is underperforming (too few hits).\n",
    "\n",
    "Constraints (do NOT violate):\n",
    "- Keep the core topic: population and intervention must remain faithful to the protocol.\n",
    "- Only operate on the P/I token lists: REMOVE_TERM, SIMPLIFY_TERM (shorten phrase), or ADD_ALTERNATE (synonym).\n",
    "- Return at most 2 operations.\n",
    "- Do NOT introduce terms that contradict population or intervention focus.\n",
    "Return JSON only.\"\"\"\n",
    "\n",
    "def rem_user(query: str, count: int, protocol: Dict[str,Any]) -> str:\n",
    "    return f\"\"\"Current universe query (hits={count}):\n",
    "{query}\n",
    "\n",
    "Protocol (brief):\n",
    "P tokens: {protocol[\"pico_tokens\"][\"P\"]}\n",
    "I tokens: {protocol[\"pico_tokens\"][\"I\"]}\n",
    "Anchors: {protocol[\"anchors_must_have\"]}\n",
    "Avoid: {protocol[\"avoid_terms\"]}\n",
    "Design preference: {protocol[\"designs_preference\"]}\n",
    "\n",
    "Allowed ops (array of steps):\n",
    "[{{\"op\":\"REMOVE_TERM\",\"where\":\"P|I\",\"term\":\"...\"}}, {{\"op\":\"SIMPLIFY_TERM\",\"where\":\"P|I\",\"term\":\"full phrase\",\"simplified\":\"short term\"}}, {{\"op\":\"ADD_ALTERNATE\",\"where\":\"P|I\",\"term\":\"root\",\"alternate\":\"synonym\"}}]\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"ops\":[]}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "def validate_remediation(js: Dict[str,Any]) -> Tuple[bool,str]:\n",
    "    if \"ops\" not in js or not isinstance(js[\"ops\"], list): return False, \"missing ops[]\"\n",
    "    if len(js[\"ops\"])>2: return False, \"too many ops\"\n",
    "    for op in js[\"ops\"]:\n",
    "        if op.get(\"op\") not in [\"REMOVE_TERM\",\"SIMPLIFY_TERM\",\"ADD_ALTERNATE\"]:\n",
    "            return False, f\"bad op: {op.get('op')}\"\n",
    "        if op.get(\"where\") not in [\"P\",\"I\"]:\n",
    "            return False, \"where must be P or I\"\n",
    "    return True, \"\"\n",
    "\n",
    "def apply_remediation(P: List[str], I: List[str], ops: List[Dict[str,str]]) -> Tuple[List[str], List[str]]:\n",
    "    Pn = P[:]; In = I[:]\n",
    "    def _apply(lst, op):\n",
    "        if op[\"op\"]==\"REMOVE_TERM\":\n",
    "            lst = [t for t in lst if t.lower()!=op.get(\"term\",\"\").lower()]\n",
    "        elif op[\"op\"]==\"SIMPLIFY_TERM\":\n",
    "            t = op.get(\"term\",\"\"); s=op.get(\"simplified\",\"\")\n",
    "            lst = [s if x.lower()==t.lower() and s else x for x in lst]\n",
    "        elif op[\"op\"]==\"ADD_ALTERNATE\":\n",
    "            alt = op.get(\"alternate\",\"\")\n",
    "            if alt and alt.lower() not in [x.lower() for x in lst]:\n",
    "                lst.append(alt)\n",
    "        return lst\n",
    "    for op in ops:\n",
    "        if op[\"where\"]==\"P\":\n",
    "            Pn = _apply(Pn, op)\n",
    "        else:\n",
    "            In = _apply(In, op)\n",
    "    return Pn, In\n",
    "\n",
    "# ----------------------------\n",
    "# Deterministic prefilter (language/year/design only)\n",
    "# ----------------------------\n",
    "def passes_prefilter(rec: Dict[str,Any], languages: List[str], year_min: int, design_allowlist: List[str], pubtype_alias: Dict[str,List[str]]) -> bool:\n",
    "    if rec.get(\"year\") and rec[\"year\"] < year_min:\n",
    "        return False\n",
    "    if rec.get(\"language\") and rec[\"language\"].lower() not in [x.lower() for x in languages]:\n",
    "        return False\n",
    "    if design_allowlist:\n",
    "        # Any intersection between aliases for allowed designs and rec.pubtypes\n",
    "        rpts = set(rec.get(\"pubtypes\") or [])\n",
    "        for design in design_allowlist:\n",
    "            aliases = set(pubtype_alias.get(design, [design]))\n",
    "            if rpts & aliases:\n",
    "                return True\n",
    "        # allow if no pubtypes present (unknown design) -> keep for LLM\n",
    "        if not rpts:\n",
    "            return True\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# ----------------------------\n",
    "# TF-IDF PICO reranker\n",
    "# ----------------------------\n",
    "def build_tfidf_and_score(records: List[Dict[str,Any]], protocol: Dict[str,Any]) -> List[Tuple[float,Dict[str,Any]]]:\n",
    "    texts = []\n",
    "    for r in records:\n",
    "        t = (r.get(\"title\",\"\") + \" \" + r.get(\"abstract\",\"\")).strip()\n",
    "        texts.append(t if t else r.get(\"title\",\"\"))\n",
    "    # import TF-IDF with fallback\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vec = TfidfVectorizer(stop_words=\"english\", max_features=50000)\n",
    "        X = vec.fit_transform(texts)\n",
    "        vocab = vec.vocabulary_\n",
    "        idf_diag = None  # scikit handles internally\n",
    "        def tfidf(term, row_idx):\n",
    "            j = vocab.get(term.lower())\n",
    "            if j is None: return 0.0\n",
    "            return X[row_idx, j]\n",
    "    except Exception:\n",
    "        # very simple fallback: case-insensitive term frequency proxy\n",
    "        vocab = {}\n",
    "        def tfidf(term, row_idx):\n",
    "            low = texts[row_idx].lower()\n",
    "            return float(low.count(term.lower()))\n",
    "\n",
    "    # compile weighted term list\n",
    "    wt_terms = []\n",
    "    for t in protocol[\"pico_tokens\"][\"P\"]:\n",
    "        wt_terms.append( (t, WEIGHTS[\"P\"]) )\n",
    "    for t in protocol[\"pico_tokens\"][\"I\"]:\n",
    "        wt_terms.append( (t, WEIGHTS[\"I\"]) )\n",
    "    for t in protocol[\"pico_tokens\"][\"C\"]:\n",
    "        wt_terms.append( (t, WEIGHTS[\"C\"]) )\n",
    "    for t in protocol[\"pico_tokens\"][\"O\"]:\n",
    "        wt_terms.append( (t, WEIGHTS[\"O\"]) )\n",
    "    for t in protocol[\"anchors_must_have\"]:\n",
    "        wt_terms.append( (t, WEIGHTS[\"ANCHOR\"]) )\n",
    "    for t in protocol[\"avoid_terms\"]:\n",
    "        wt_terms.append( (t, WEIGHTS[\"AVOID\"]) )\n",
    "\n",
    "    scored = []\n",
    "    for i, rec in enumerate(records):\n",
    "        s = 0.0\n",
    "        for term, w in wt_terms:\n",
    "            if not term: continue\n",
    "            s += float(tfidf(term, i)) * w\n",
    "        scored.append( (s, rec) )\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return scored\n",
    "\n",
    "# ----------------------------\n",
    "# Screener prompts & validators\n",
    "# ----------------------------\n",
    "SCREEN_SYS = \"\"\"You are a strict but realistic title+abstract screener for an evidence scan.\n",
    "\n",
    "Checklist logic (INCLUDE requires P & I true AND (O OR D) true):\n",
    "- P (Population/Context): study matches the target clinical context; synonyms acceptable.\n",
    "- I (Intervention): intercostal nerve cryoablation / cryoanalgesia used intraoperatively for the target surgery; synonyms acceptable.\n",
    "- O (Outcomes): any acute postoperative analgesia outcomes acceptable (pain, opioid use, LOS, early complications). Do NOT require exact day windows at abstract level unless protocol explicitly demands it.\n",
    "- D (Design): randomized/comparative preferred; strong cohorts acceptable if protocol allows. Use PubTypes if available; otherwise infer from abstract.\n",
    "\n",
    "Return ONLY JSON with schema below; be conservative but do not nitpick details that require full-text.\n",
    "If the record is clearly pediatric while protocol is adults-only (or vice-versa), you may EXCLUDE for population mismatch.\"\"\"\n",
    "\n",
    "def screen_user(protocol: Dict[str,Any], record: Dict[str,Any]) -> str:\n",
    "    return f\"\"\"Protocol (narrative):\n",
    "{protocol[\"narrative_question\"]}\n",
    "\n",
    "Key lists:\n",
    "P: {protocol[\"pico_tokens\"][\"P\"]}\n",
    "I: {protocol[\"pico_tokens\"][\"I\"]}\n",
    "C: {protocol[\"pico_tokens\"][\"C\"]}\n",
    "O: {protocol[\"pico_tokens\"][\"O\"]}\n",
    "Design preference: {protocol[\"designs_preference\"]}\n",
    "Anchors: {protocol[\"anchors_must_have\"]}\n",
    "Avoid: {protocol[\"avoid_terms\"]}\n",
    "Inclusion criteria: {protocol[\"inclusion_criteria\"]}\n",
    "Exclusion criteria: {protocol[\"exclusion_criteria\"]}\n",
    "Screening notes: {protocol[\"screening_rules_note\"]}\n",
    "\n",
    "Record:\n",
    "PMID: {record['pmid']}\n",
    "Title: {record['title']}\n",
    "PubTypes: {record.get('pubtypes',[])}\n",
    "MeSH: {record.get('mesh',[])}\n",
    "Abstract:\n",
    "{record.get('abstract','')}\n",
    "\n",
    "Return schema:\n",
    "{{\n",
    "  \"pmid\": \"{record['pmid']}\",\n",
    "  \"decision\": \"INCLUDE|BORDERLINE|EXCLUDE\",\n",
    "  \"why\": \"<one concise reason>\",\n",
    "  \"checklist\": {{\"P\": true|false, \"I\": true|false, \"O\": true|false, \"D\": true|false}},\n",
    "  \"mesh_roles\": [{{\"mesh\":\"...\",\"role\":\"P|I|C|O|G\"}}]\n",
    "}}\"\"\"\n",
    "\n",
    "def validate_screen(js: Dict[str,Any]) -> Tuple[bool,str]:\n",
    "    try:\n",
    "        if js.get(\"decision\") not in [\"INCLUDE\",\"BORDERLINE\",\"EXCLUDE\"]:\n",
    "            return False, \"bad decision\"\n",
    "        ch = js.get(\"checklist\",{})\n",
    "        for k in [\"P\",\"I\",\"O\",\"D\"]:\n",
    "            if not isinstance(ch.get(k), bool):\n",
    "                return False, f\"checklist.{k} must be bool\"\n",
    "        m = js.get(\"mesh_roles\",[])\n",
    "        if not isinstance(m, list):\n",
    "            return False, \"mesh_roles must be list\"\n",
    "        for it in m:\n",
    "            if not isinstance(it, dict): return False, \"mesh_roles items must be dict\"\n",
    "            if \"mesh\" not in it or \"role\" not in it: return False, \"mesh_roles items need mesh & role\"\n",
    "        return True, \"\"\n",
    "    except Exception as e:\n",
    "        return False, f\"exception in screen validation: {e}\"\n",
    "\n",
    "# Senior plausibility check\n",
    "PLAUS_SYS = \"\"\"You are a senior reviewer validating junior screening decisions to prevent topic drift.\n",
    "Given the protocol and an already-INCLUDED record, answer PASS if the record’s core topic clearly matches the protocol’s core P+I context; otherwise FAIL.\n",
    "Be brief and conservative. Return JSON only with {\"pmid\":\"...\",\"verdict\":\"PASS|FAIL\",\"why\":\"...\"}\"\"\"\n",
    "\n",
    "def plaus_user(protocol: Dict[str,Any], record: Dict[str,Any]) -> str:\n",
    "    core = f\"P core terms: {protocol['pico_tokens']['P']} ; I core terms: {protocol['pico_tokens']['I']} ; Anchors: {protocol['anchors_must_have']}\"\n",
    "    return f\"\"\"Protocol core:\n",
    "{core}\n",
    "\n",
    "Record:\n",
    "PMID: {record['pmid']}\n",
    "Title: {record['title']}\n",
    "PubTypes: {record.get('pubtypes',[])}\n",
    "MeSH: {record.get('mesh',[])}\n",
    "Abstract:\n",
    "{record.get('abstract','')}\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{record['pmid']}\", \"verdict\":\"PASS\", \"why\":\"\"}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "def validate_plaus(js: Dict[str,Any]) -> Tuple[bool,str]:\n",
    "    v = js.get(\"verdict\")\n",
    "    if v not in [\"PASS\",\"FAIL\"]: return False, \"verdict must be PASS|FAIL\"\n",
    "    if \"pmid\" not in js: return False, \"missing pmid\"\n",
    "    return True, \"\"\n",
    "\n",
    "# ----------------------------\n",
    "# State machine\n",
    "# ----------------------------\n",
    "def state1_protocol_lockdown(nlq: str) -> Dict[str,Any]:\n",
    "    print(\"[S1] Protocol lockdown...\")\n",
    "    system = PROTO_SYSTEM\n",
    "    # Add explicit guardrails/examples to discourage long tokens\n",
    "    user = proto_user(nlq, KB) + \"\"\"\n",
    "\n",
    "Guidance:\n",
    "- BAD token example: \"intraoperative intercostal nerve cryoablation for analgesia\"\n",
    "- GOOD tokens: [\"intercostal nerve\",\"cryoablation\",\"cryoanalgesia\",\"INC\",\"analgesia\"]\n",
    "\"\"\"\n",
    "    proto = get_validated_json(QWEN_MODEL, system, user, validate_protocol, retries=3, max_tokens=2048)\n",
    "    if proto.get(\"needs_clarification\"):\n",
    "        raise SystemExit(\"Protocol needs clarification: \" + proto.get(\"clarification_request\",\"\"))\n",
    "    print(\"  [S1] Locked protocol:\")\n",
    "    print(\"   \", json.dumps(proto, ensure_ascii=False))\n",
    "    return proto\n",
    "\n",
    "def state2_universe(protocol: Dict[str,Any]) -> Tuple[str, int, List[str]]:\n",
    "    print(\"[S2] Universe definition & sizing...\")\n",
    "    P = protocol[\"pico_tokens\"][\"P\"]\n",
    "    I = protocol[\"pico_tokens\"][\"I\"]\n",
    "    anchors = protocol[\"anchors_must_have\"]\n",
    "    query = build_universe_query(P, I, anchors)\n",
    "    count, ids = esearch_ids(query, protocol[\"deterministic_filters\"][\"year_min\"], retmax=UNIVERSE_FETCH_MAX)\n",
    "    print(f\"   [Universe] try=0 count={count} window={UNIVERSE_TARGET}\")\n",
    "    tries = 0\n",
    "    while (count < UNIVERSE_TARGET[0] or count > UNIVERSE_TARGET[1]) and tries < 2:\n",
    "        # remediation loop\n",
    "        rem = get_validated_json(QWEN_MODEL, REM_SYS, rem_user(query, count, protocol), validate_remediation, retries=2, max_tokens=1024)\n",
    "        P, I = apply_remediation(P, I, rem.get(\"ops\",[]))\n",
    "        query = build_universe_query(P, I, anchors)\n",
    "        count, ids = esearch_ids(query, protocol[\"deterministic_filters\"][\"year_min\"], retmax=UNIVERSE_FETCH_MAX)\n",
    "        tries += 1\n",
    "        print(f\"   [Universe] try={tries} count={count} window={UNIVERSE_TARGET}\")\n",
    "    if count < UNIVERSE_HARD_MIN:\n",
    "        raise SystemExit(f\"Fatal: universe too small after remediation (count={count} < {UNIVERSE_HARD_MIN}).\")\n",
    "    return query, count, ids\n",
    "\n",
    "def deterministic_prefilter(records: List[Dict[str,Any]], protocol: Dict[str,Any]) -> List[Dict[str,Any]]:\n",
    "    langs = protocol[\"deterministic_filters\"][\"languages\"]\n",
    "    ymin  = protocol[\"deterministic_filters\"][\"year_min\"]\n",
    "    # Build allowlist: prefer primary first; include secondary too at sniff stage\n",
    "    allowlist = list(dict.fromkeys(KB[\"designs_primary\"] + KB[\"designs_secondary\"]))\n",
    "    out=[]\n",
    "    for r in records:\n",
    "        if passes_prefilter(r, langs, ymin, allowlist, KB[\"pubtype_aliases\"]):\n",
    "            out.append(r)\n",
    "    return out\n",
    "\n",
    "def state2_5_rerank_universe(query: str, ids: List[str], protocol: Dict[str,Any]) -> List[Dict[str,Any]]:\n",
    "    print(\"[S2.5] Rerank universe with PICO-weighted TF-IDF...\")\n",
    "    # fetch up to UNIVERSE_FETCH_MAX for reranking\n",
    "    ids = ids[:UNIVERSE_FETCH_MAX]\n",
    "    xml = efetch_xml(ids)\n",
    "    recs = parse_pubmed_xml(xml)\n",
    "    pre = deterministic_prefilter(recs, protocol)\n",
    "    scored = build_tfidf_and_score(pre, protocol)\n",
    "    # return re-ordered records only\n",
    "    return [r for (s,r) in scored]\n",
    "\n",
    "def state3_ground_truth(reranked_records: List[Dict[str,Any]], protocol: Dict[str,Any]) -> Tuple[List[Dict[str,Any]], List[Dict[str,Any]]]:\n",
    "    print(\"[S3] Ground-truth discovery & vocabulary mining...\")\n",
    "    to_screen = reranked_records[:SCREEN_TOP_K]\n",
    "    includes=[]; borderlines=[]\n",
    "    for r in to_screen:\n",
    "        js = get_validated_json(SCREENER_MODEL, SCREEN_SYS, screen_user(protocol, r), validate_screen, retries=SCREENER_RETRY_MAX, max_tokens=1536)\n",
    "        d = js.get(\"decision\")\n",
    "        why = js.get(\"why\",\"\")\n",
    "        chk = js.get(\"checklist\",{})\n",
    "        # concise logging\n",
    "        short_abs = (r.get(\"abstract\",\"\")[:320] + \"…\") if r.get(\"abstract\") and len(r[\"abstract\"])>320 else (r.get(\"abstract\",\"\") or \"\")\n",
    "        print(f\"  [Screen] PMID {r['pmid']} -> decision={d} checklist={chk} why={why}\")\n",
    "        print(f\"    Title: {r['title']}\")\n",
    "        print(f\"    Abstract: {short_abs}\")\n",
    "        if d==\"INCLUDE\":\n",
    "            # attach mesh_roles if any\n",
    "            r[\"_mesh_roles\"] = js.get(\"mesh_roles\",[])\n",
    "            includes.append(r)\n",
    "        elif d==\"BORDERLINE\":\n",
    "            r[\"_mesh_roles\"] = js.get(\"mesh_roles\",[])\n",
    "            borderlines.append(r)\n",
    "    return includes, borderlines\n",
    "\n",
    "def state3_5_plausibility(includes: List[Dict[str,Any]], protocol: Dict[str,Any]) -> List[Dict[str,Any]]:\n",
    "    print(\"[S3.5] Senior plausibility check (guard against topic drift)...\")\n",
    "    confirmed=[]\n",
    "    for r in includes:\n",
    "        js = get_validated_json(QWEN_MODEL, PLAUS_SYS, plaus_user(protocol, r), validate_plaus, retries=2, max_tokens=768)\n",
    "        if js.get(\"verdict\")==\"PASS\":\n",
    "            confirmed.append(r)\n",
    "        else:\n",
    "            print(f\"   [Plausibility] DROP PMID {r['pmid']} — {js.get('why','')}\")\n",
    "    return confirmed\n",
    "\n",
    "def mesh_vernaculum_from(includes: List[Dict[str,Any]]) -> Dict[str,List[str]]:\n",
    "    roles = {\"P\":set(),\"I\":set(),\"C\":set(),\"O\":set(),\"G\":set()}\n",
    "    for r in includes:\n",
    "        for mr in r.get(\"_mesh_roles\",[]):\n",
    "            m = mr.get(\"mesh\"); role = mr.get(\"role\",\"G\")\n",
    "            if m and role in roles:\n",
    "                roles[role].add(m)\n",
    "    return {k:sorted(v) for k,v in roles.items()}\n",
    "\n",
    "def state4_validate_strategy(universe_query: str, confirmed_includes: List[Dict[str,Any]], protocol: Dict[str,Any], vernac: Dict[str,List[str]]) -> Dict[str,str]:\n",
    "    print(\"[S4] Search-strategy validation & refinement...\")\n",
    "    # Build \"topic_filter\" deterministically from vernaculum (use P+I+O meshes as TIAB surface tokens)\n",
    "    topic_tokens = list(dict.fromkeys(vernac.get(\"P\",[]) + vernac.get(\"I\",[]) + vernac.get(\"O\",[])))\n",
    "    topic_filter = or_block(topic_tokens, \"tiab\") if topic_tokens else \"\"\n",
    "    # Build design filter deterministically from protocol preference (map to aliases)\n",
    "    pref = protocol[\"designs_preference\"]\n",
    "    aliases = KB[\"pubtype_aliases\"].get(pref, [pref])\n",
    "    # recommended_filters are strings meant to be combined during Harvest:\n",
    "    #   final_query := (universe_query) AND (topic_filter)  then apply 'design_filter' at execution time\n",
    "    recommended = {\n",
    "        \"topic_filter\": topic_filter,\n",
    "        \"design_filter\": \" OR \".join(f'\"{a}\"[Publication Type]' for a in aliases)\n",
    "    }\n",
    "\n",
    "    # Validation: recall of includes\n",
    "    # We check that each include is still retrievable with (universe AND topic_filter)\n",
    "    recall_ok = True\n",
    "    for r in confirmed_includes:\n",
    "        # cheap check: topic_filter tokens appear in title/abstract (proxy for final execution)\n",
    "        if topic_filter:\n",
    "            any_tok = False\n",
    "            low = (r.get(\"title\",\"\") + \" \" + r.get(\"abstract\",\"\")).lower()\n",
    "            # parse tokens out of the topic_filter string approximately\n",
    "            toks = re.findall(r'\"([^\"]+)\"\\[tiab\\]|(\\w+)\\[tiab\\]', topic_filter)\n",
    "            flat = [a or b for a,b in toks if (a or b)]\n",
    "            for t in flat:\n",
    "                if t.lower() in low:\n",
    "                    any_tok = True; break\n",
    "            if not any_tok:\n",
    "                recall_ok = False\n",
    "                print(f\"   [S4] Recall risk: topic_filter might drop PMID {r['pmid']}\")\n",
    "\n",
    "    if not recall_ok:\n",
    "        print(\"   [S4] Relaxing topic_filter (drop vernaculum; rely on universe_query only).\")\n",
    "        recommended[\"topic_filter\"] = \"\"  # fall back to universe-only; design filter still applied downstream\n",
    "\n",
    "    return recommended\n",
    "\n",
    "def state5_finalize(protocol: Dict[str,Any], universe_query: str, recommended_filters: Dict[str,str],\n",
    "                    confirmed_includes: List[Dict[str,Any]], vernac: Dict[str,List[str]], warnings: List[str]) -> None:\n",
    "    print(\"[S5] Finalization & handoff...\")\n",
    "    # embedding string: concise, validated question\n",
    "    rq_embed = f\"{protocol['narrative_question']} | P:{', '.join(protocol['pico_tokens']['P'])} I:{', '.join(protocol['pico_tokens']['I'])} O:{', '.join(protocol['pico_tokens']['O'])} Anchors:{', '.join(protocol['anchors_must_have'])}\"\n",
    "\n",
    "    artifacts = {\n",
    "        \"locked_protocol\": protocol,\n",
    "        \"universe_query\": universe_query,\n",
    "        \"recommended_filters\": recommended_filters,\n",
    "        \"ground_truth_pmids\": [r[\"pmid\"] for r in confirmed_includes],\n",
    "        \"mesh_vernaculum\": vernac,\n",
    "        \"research_question_string_for_embedding\": rq_embed,\n",
    "        \"warnings\": warnings\n",
    "    }\n",
    "    (OUT_DIR/\"sniff_artifacts.json\").write_text(json.dumps(artifacts, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # Human-readable report\n",
    "    lines=[]\n",
    "    lines.append(\"========= SNIFF VALIDATION ENGINE REPORT (v3.1) =========\\n\")\n",
    "    lines.append(\"Protocol (narrative):\\n\" + textwrap.fill(protocol[\"narrative_question\"], 100) + \"\\n\")\n",
    "    lines.append(\"Deterministic filters: languages=\" + \", \".join(protocol[\"deterministic_filters\"][\"languages\"]) +\n",
    "                 f\" ; year_min={protocol['deterministic_filters']['year_min']}\\n\")\n",
    "    lines.append(\"Universe query:\\n\" + universe_query + \"\\n\")\n",
    "    lines.append(\"Recommended filters:\\n  topic_filter=\" + (recommended_filters[\"topic_filter\"] or \"<none>\") +\n",
    "                 \"\\n  design_filter=\" + recommended_filters[\"design_filter\"] + \"\\n\")\n",
    "    lines.append(f\"Ground truth includes (n={len(artifacts['ground_truth_pmids'])}): \" + \", \".join(artifacts[\"ground_truth_pmids\"]) + \"\\n\")\n",
    "    lines.append(\"MeSH vernaculum (from includes only):\\n\" + json.dumps(vernac, indent=2, ensure_ascii=False) + \"\\n\")\n",
    "    if warnings:\n",
    "        lines.append(\"WARNINGS:\\n- \" + \"\\n- \".join(warnings) + \"\\n\")\n",
    "    (OUT_DIR/\"sniff_report.txt\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(\"  wrote:\", OUT_DIR/\"sniff_artifacts.json\", \"and\", OUT_DIR/\"sniff_report.txt\")\n",
    "\n",
    "# ----------------------------\n",
    "# Orchestration\n",
    "# ----------------------------\n",
    "def sniff_engine_run(USER_NLQ: str):\n",
    "    warnings=[]\n",
    "    # S1\n",
    "    protocol = state1_protocol_lockdown(USER_NLQ)\n",
    "\n",
    "    # S2\n",
    "    universe_query, universe_count, universe_ids = state2_universe(protocol)\n",
    "\n",
    "    # S2.5\n",
    "    reranked = state2_5_rerank_universe(universe_query, universe_ids, protocol)\n",
    "    if not reranked:\n",
    "        raise SystemExit(\"Fatal: no records after deterministic prefilter.\")\n",
    "\n",
    "    # S3\n",
    "    includes, borderlines = state3_ground_truth(reranked, protocol)\n",
    "    if not includes:\n",
    "        raise SystemExit(\"Fatal: no includes after screening. Revisit protocol or universe scope.\")\n",
    "\n",
    "    # S3.5\n",
    "    confirmed = state3_5_plausibility(includes, protocol)\n",
    "    if len(confirmed) < PLAUSIBILITY_MIN_INCLUDES:\n",
    "        raise SystemExit(f\"Fatal: insufficient confirmed includes after plausibility ({len(confirmed)}<{PLAUSIBILITY_MIN_INCLUDES}).\")\n",
    "\n",
    "    # vernaculum strictly from confirmed includes\n",
    "    vernac = mesh_vernaculum_from(confirmed)\n",
    "\n",
    "    # S4\n",
    "    recommended_filters = state4_validate_strategy(universe_query, confirmed, protocol, vernac)\n",
    "\n",
    "    # S5\n",
    "    state5_finalize(protocol, universe_query, recommended_filters, confirmed, vernac, warnings)\n",
    "\n",
    "# ----------------------------\n",
    "# Example run\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    USER_NLQ = \"\"\"\n",
    "Population = children/adolescents undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\n",
    "Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE.\n",
    "Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia.\n",
    "Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days (abstract-level timing not strictly required).\n",
    "Study designs = RCTs preferred; if absent, include comparative cohorts/case-control/observational.\n",
    "Year_min = 2015.\n",
    "Languages = English, Portuguese, Spanish.\n",
    "Screening notes: Be conservative; INCLUDE if P & I present and (O or D) is present; do not exclude for lack of exact day window if acute postop outcomes are clearly reported.\n",
    "\"\"\"\n",
    "    sniff_engine_run(USER_NLQ.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e6b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REQUEST BODY ===\n",
      "{\n",
      "  \"model\": \"qwen/qwen3-4b\",\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"Return exactly one JSON object between BEGIN_JSON and END_JSON. No other text.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"TASK\\nYou will extract concise biomedical term lists from the natural-language question (NLQ) below.\\n\\nOutput policy:\\n- Return EXACTLY ONE JSON object between:\\n  BEGIN_JSON\\n  { ... }\\n  END_JSON\\n- No other text. No backticks. No “think” prefaces.\\n- Arrays only; 2–10 items per list when possible.\\n- Items are plain phrases (no boolean operators, quotes, field tags, or brackets).\\n- Prefer standard medical wording and common acronyms (e.g., MIRPE, INC).\\n- Keep scope tightly on the NLQ intent.\\n\\nFill these keys:\\n- population: synonyms/labels for adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\\n- intervention: synonyms/labels for intercostal nerve cryoablation used for analgesia during Nuss/MIRPE (e.g., cryoanalgesia, INC).\\n- comparators: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia (and common variants).\\n- outcomes: postoperative opioid consumption and pain scores within 0–7 days (include common phrasings).\\n- must_have: 3–6 anchor tokens that should appear to ensure topicality (e.g., MIRPE, Nuss, cryoablation).\\n- avoid: 3–6 obvious confounders to avoid if they dominate (e.g., pediatric oncology, cardiac surgery).\\n\\nNLQ\\nPopulation = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE (the intervention of interest is INC, not the surgery). Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia. Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days. Study designs = RCTs preferred; if RCTs absent, include comparative cohort/case-control. Year_min = 2015. Languages = English, Portuguese, Spanish.\\n\\nTEMPLATE_TO_FILL (structure only; you must populate the arrays with phrases)\\nBEGIN_JSON\\n{\\n  \\\"population\\\": [],\\n  \\\"intervention\\\": [],\\n  \\\"comparators\\\": [],\\n  \\\"outcomes\\\": [],\\n  \\\"must_have\\\": [],\\n  \\\"avoid\\\": []\\n}\\nEND_JSON\"\n",
      "    }\n",
      "  ],\n",
      "  \"temperature\": 0.0,\n",
      "  \"max_tokens\": 4000,\n",
      "  \"stream\": false\n",
      "}\n",
      "\n",
      "=== HTTP STATUS ===\n",
      "200\n",
      "\n",
      "=== RAW TEXT ===\n",
      "{\n",
      "  \"id\": \"chatcmpl-4rx4y2wfq94v3odnq4rzj\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1757565160,\n",
      "  \"model\": \"qwen/qwen3-4b\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"<think>\\nOkay, let's tackle this query. The user wants me to extract biomedical terms from the given NLQ. First, I need to understand each section: population, intervention, comparators, outcomes, must_have, and avoid.\\n\\nStarting with population. The NLQ says adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). So synonyms for that. MIRPE and Nuss are already mentioned. Maybe \\\"adults with pectus excavatum\\\" or \\\"patients undergoing minimally invasive pectus repair\\\". Also, \\\"postoperative\\\" might be relevant since it's about after surgery. But wait, the example in the template includes \\\"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\\\", so maybe I should include that exact phrase? Or synonyms. Let me check the output policy: prefer standard medical terms and common acronyms. So MIRPE and Nuss are key here. Also, \\\"minimally invasive repair\\\" is part of the intervention, but for population, maybe \\\"adults with pectus excavatum\\\" or \\\"patients undergoing minimally invasive pectus repair\\\". But the example in the problem statement includes \\\"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\\\" as a possible entry. So perhaps that's one term. Also, maybe \\\"postoperative\\\" if relevant, but the NLQ specifies the population as those undergoing the surgery. Hmm.\\n\\nNext, intervention: intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE. The example includes \\\"cryoanalgesia\\\", \\\"INC\\\", and \\\"intercostal nerve cryoablation\\\". Also, maybe \\\"intraoperative cryoablation\\\" or \\\"cryoanesthesia\\\"? But the key terms here are INC, cryoablation, intercostal nerve. So possible entries: \\\"intercostal nerve cryoablation\\\", \\\"cryoanalgesia\\\", \\\"INC\\\", and maybe \\\"intraoperative cryoablation\\\".\\n\\nComparators: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia. The NLQ lists these as comparators. So those are the exact terms. But need to check if there are common variants. For example, \\\"thoracic epidural\\\" is already listed. Maybe \\\"paravertebral blockade\\\"? Or \\\"intercostal nerve block\\\" vs \\\"intercostal block\\\". But the problem says to include common phrasings. The original list includes all the exact terms, so maybe those are the ones.\\n\\nOutcomes: postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days. Common phrasings could be \\\"postoperative opioid use\\\", \\\"opioid consumption\\\", \\\"pain scores in first week\\\", \\\"acute postoperative pain\\\". Also, \\\"in-hospital opioid use\\\" or \\\"discharge opioid use\\\". But the NLQ specifies \\\"postoperative opioid consumption (in-hospital and at discharge)\\\" so maybe include that exact phrase. Also \\\"pain scores within 7 days\\\".\\n\\nMust_have: 3-6 anchor tokens. The example mentions MIRPE, Nuss, cryoablation. So those are key terms. Also, the intervention is INC, so maybe include \\\"INC\\\", \\\"cryoablation\\\", \\\"Nuss\\\". But the NLQ says \\\"must_have\\\" should be 3-6. Let's check: MIRPE, Nuss, cryoablation, and maybe \\\"intercostal nerve cryoablation\\\"? Or perhaps \\\"cryoanalgesia\\\"? Wait, the must_have is to ensure topicality. The key terms from the NLQ are MIRPE, Nuss, INC (cryoablation). So maybe those three? But the example in the problem statement includes MIRPE, Nuss, cryoablation as must_have. Let me check the original NLQ: \\\"must_have: 3–6 anchor tokens that should appear to ensure topicality (e.g., MIRPE, Nuss, cryoablation).\\\" So those three are examples. So include MIRPE, Nuss, and cryoablation.\\n\\nAvoid: 3-6 obvious confounders. The example says e.g., pediatric oncology, cardiac surgery. But the NLQ's context is about pectus excavatum repair. So avoid terms related to other surgeries or conditions. For instance, \\\"pediatric\\\", \\\"cardiac\\\", \\\"oncology\\\", \\\"orthopedic\\\" (if not relevant), but maybe \\\"thoracic surgery\\\" if it's a confounder? Wait, the NLQ is about pectus excavatum repair, so avoid terms that are unrelated. The example says to avoid things like pediatric oncology, cardiac surgery. So in this case, perhaps \\\"pediatric\\\", \\\"cardiac\\\", \\\"oncologic\\\", or \\\"orthopedic\\\" if they're not relevant. But the NLQ is about adults, so maybe \\\"pediatric\\\" is a confounder. Also, \\\"thoracic surgery\\\" might be a comparator, but since comparators are already listed, perhaps avoid terms that are too broad. Maybe \\\"cardiac\\\", \\\"oncologic\\\", \\\"pediatric\\\". So those three.\\n\\nNow, putting it all together:\\n\\nPopulation: adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE), adults with pectus excavatum, postoperative patients with pectus excavatum. Wait, but the NLQ says population is adults undergoing the surgery. Maybe \\\"adults undergoing minimally invasive pectus repair\\\" and \\\"patients with pectus excavatum\\\". Also, maybe \\\"postoperative\\\" if relevant, but the population is those undergoing the procedure. So perhaps include \\\"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\\\" as one term, then \\\"adults with pectus excavatum\\\", and \\\"patients undergoing minimally invasive pectus repair\\\".\\n\\nIntervention: intercostal nerve cryoablation, cryoanalgesia, INC, intraoperative cryoablation.\\n\\nComparators: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia. The NLQ lists these exactly, so include them as is.\\n\\nOutcomes: postoperative opioid consumption (in-hospital and at discharge), pain scores within 7 days, postoperative opioid use, acute postoperative pain scores.\\n\\nMust_have: MIRPE, Nuss, cryoablation.\\n\\nAvoid: pediatric, cardiac, oncologic.\\n\\nWait, but the avoid list should be 3-6 terms. The example says e.g., pediatric oncology, cardiac surgery. So maybe \\\"pediatric\\\", \\\"cardiac\\\", \\\"oncologic\\\" as three terms. Alternatively, \\\"thoracic surgery\\\", \\\"orthopedic\\\", etc. But the NLQ is about pectus excavatum repair, so avoid terms that are not related. Maybe \\\"pediatric\\\", \\\"cardiac\\\", \\\"oncologic\\\".\\n\\nSo final check:\\n\\nPopulation: [\\\"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\\\", \\\"adults with pectus excavatum\\\", \\\"patients undergoing minimally invasive pectus repair\\\"]\\n\\nIntervention: [\\\"intercostal nerve cryoablation\\\", \\\"cryoanalgesia\\\", \\\"INC\\\", \\\"intraoperative cryoablation\\\"]\\n\\nComparators: [\\\"thoracic epidural\\\", \\\"paravertebral block\\\", \\\"intercostal nerve block\\\", \\\"erector spinae plane block\\\", \\\"systemic multimodal analgesia\\\"]\\n\\nOutcomes: [\\\"postoperative opioid consumption (in-hospital and at discharge)\\\", \\\"pain scores within 7 days\\\", \\\"postoperative opioid use\\\", \\\"acute postoperative pain scores\\\"]\\n\\nMust_have: [\\\"MIRPE\\\", \\\"Nuss\\\", \\\"cryoablation\\\"]\\n\\nAvoid: [\\\"pediatric\\\", \\\"cardiac\\\", \\\"oncologic\\\"]\\n\\nBut need to check if the arrays are between 2-10 items. Population has three, intervention four, comparators five, outcomes four, must_have three, avoid three. That fits.\\n\\nNow, make sure that all terms are plain phrases without any brackets or tags. Also, check for standard medical terms. For example, \\\"systemic multimodal analgesia\\\" is correct. \\\"Erector spinae plane block\\\" is a known term. \\\"Cryoanalgesia\\\" is acceptable as an acronym. \\\"INC\\\" is the abbreviation for intercostal nerve cryoablation.\\n\\nYes, this should meet the requirements.\\n</think>\\n\\n{\\n  \\\"population\\\": [\\n    \\\"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\\\",\\n    \\\"patients with pectus excavatum\\\",\\n    \\\"postoperative patients with pectus excavatum\\\"\\n  ],\\n  \\\"intervention\\\": [\\n    \\\"intercostal nerve cryoablation\\\",\\n    \\\"cryoanalgesia\\\",\\n    \\\"INC\\\",\\n    \\\"intraoperative cryoablation\\\"\\n  ],\\n  \\\"comparators\\\": [\\n    \\\"thoracic epidural\\\",\\n    \\\"paravertebral block\\\",\\n    \\\"intercostal nerve block\\\",\\n    \\\"erector spinae plane block\\\",\\n    \\\"systemic multimodal analgesia\\\"\\n  ],\\n  \\\"outcomes\\\": [\\n    \\\"postoperative opioid consumption (in-hospital and at discharge)\\\",\\n    \\\"pain scores within 7 days\\\",\\n    \\\"postoperative opioid use\\\",\\n    \\\"acute postoperative pain scores\\\"\\n  ],\\n  \\\"must_have\\\": [\\n    \\\"MIRPE\\\",\\n    \\\"Nuss\\\",\\n    \\\"cryoablation\\\"\\n  ],\\n  \\\"avoid\\\": [\\n    \\\"pediatric\\\",\\n    \\\"cardiac\\\",\\n    \\\"oncologic\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 563,\n",
      "    \"completion_tokens\": 2065,\n",
      "    \"total_tokens\": 2628\n",
      "  },\n",
      "  \"stats\": {},\n",
      "  \"system_fingerprint\": \"qwen/qwen3-4b\"\n",
      "}\n",
      "\n",
      "(saved exact response to lmstudio_raw_20250911_013418.txt)\n",
      "\n",
      "=== PARSED: choices[0].message.content ===\n",
      "<think>\n",
      "Okay, let's tackle this query. The user wants me to extract biomedical terms from the given NLQ. First, I need to understand each section: population, intervention, comparators, outcomes, must_have, and avoid.\n",
      "\n",
      "Starting with population. The NLQ says adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). So synonyms for that. MIRPE and Nuss are already mentioned. Maybe \"adults with pectus excavatum\" or \"patients undergoing minimally invasive pectus repair\". Also, \"postoperative\" might be relevant since it's about after surgery. But wait, the example in the template includes \"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\", so maybe I should include that exact phrase? Or synonyms. Let me check the output policy: prefer standard medical terms and common acronyms. So MIRPE and Nuss are key here. Also, \"minimally invasive repair\" is part of the intervention, but for population, maybe \"adults with pectus excavatum\" or \"patients undergoing minimally invasive pectus repair\". But the example in the problem statement includes \"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\" as a possible entry. So perhaps that's one term. Also, maybe \"postoperative\" if relevant, but the NLQ specifies the population as those undergoing the surgery. Hmm.\n",
      "\n",
      "Next, intervention: intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE. The example includes \"cryoanalgesia\", \"INC\", and \"intercostal nerve cryoablation\". Also, maybe \"intraoperative cryoablation\" or \"cryoanesthesia\"? But the key terms here are INC, cryoablation, intercostal nerve. So possible entries: \"intercostal nerve cryoablation\", \"cryoanalgesia\", \"INC\", and maybe \"intraoperative cryoablation\".\n",
      "\n",
      "Comparators: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia. The NLQ lists these as comparators. So those are the exact terms. But need to check if there are common variants. For example, \"thoracic epidural\" is already listed. Maybe \"paravertebral blockade\"? Or \"intercostal nerve block\" vs \"intercostal block\". But the problem says to include common phrasings. The original list includes all the exact terms, so maybe those are the ones.\n",
      "\n",
      "Outcomes: postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days. Common phrasings could be \"postoperative opioid use\", \"opioid consumption\", \"pain scores in first week\", \"acute postoperative pain\". Also, \"in-hospital opioid use\" or \"discharge opioid use\". But the NLQ specifies \"postoperative opioid consumption (in-hospital and at discharge)\" so maybe include that exact phrase. Also \"pain scores within 7 days\".\n",
      "\n",
      "Must_have: 3-6 anchor tokens. The example mentions MIRPE, Nuss, cryoablation. So those are key terms. Also, the intervention is INC, so maybe include \"INC\", \"cryoablation\", \"Nuss\". But the NLQ says \"must_have\" should be 3-6. Let's check: MIRPE, Nuss, cryoablation, and maybe \"intercostal nerve cryoablation\"? Or perhaps \"cryoanalgesia\"? Wait, the must_have is to ensure topicality. The key terms from the NLQ are MIRPE, Nuss, INC (cryoablation). So maybe those three? But the example in the problem statement includes MIRPE, Nuss, cryoablation as must_have. Let me check the original NLQ: \"must_have: 3–6 anchor tokens that should appear to ensure topicality (e.g., MIRPE, Nuss, cryoablation).\" So those three are examples. So include MIRPE, Nuss, and cryoablation.\n",
      "\n",
      "Avoid: 3-6 obvious confounders. The example says e.g., pediatric oncology, cardiac surgery. But the NLQ's context is about pectus excavatum repair. So avoid terms related to other surgeries or conditions. For instance, \"pediatric\", \"cardiac\", \"oncology\", \"orthopedic\" (if not relevant), but maybe \"thoracic surgery\" if it's a confounder? Wait, the NLQ is about pectus excavatum repair, so avoid terms that are unrelated. The example says to avoid things like pediatric oncology, cardiac surgery. So in this case, perhaps \"pediatric\", \"cardiac\", \"oncologic\", or \"orthopedic\" if they're not relevant. But the NLQ is about adults, so maybe \"pediatric\" is a confounder. Also, \"thoracic surgery\" might be a comparator, but since comparators are already listed, perhaps avoid terms that are too broad. Maybe \"cardiac\", \"oncologic\", \"pediatric\". So those three.\n",
      "\n",
      "Now, putting it all together:\n",
      "\n",
      "Population: adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE), adults with pectus excavatum, postoperative patients with pectus excavatum. Wait, but the NLQ says population is adults undergoing the surgery. Maybe \"adults undergoing minimally invasive pectus repair\" and \"patients with pectus excavatum\". Also, maybe \"postoperative\" if relevant, but the population is those undergoing the procedure. So perhaps include \"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\" as one term, then \"adults with pectus excavatum\", and \"patients undergoing minimally invasive pectus repair\".\n",
      "\n",
      "Intervention: intercostal nerve cryoablation, cryoanalgesia, INC, intraoperative cryoablation.\n",
      "\n",
      "Comparators: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia. The NLQ lists these exactly, so include them as is.\n",
      "\n",
      "Outcomes: postoperative opioid consumption (in-hospital and at discharge), pain scores within 7 days, postoperative opioid use, acute postoperative pain scores.\n",
      "\n",
      "Must_have: MIRPE, Nuss, cryoablation.\n",
      "\n",
      "Avoid: pediatric, cardiac, oncologic.\n",
      "\n",
      "Wait, but the avoid list should be 3-6 terms. The example says e.g., pediatric oncology, cardiac surgery. So maybe \"pediatric\", \"cardiac\", \"oncologic\" as three terms. Alternatively, \"thoracic surgery\", \"orthopedic\", etc. But the NLQ is about pectus excavatum repair, so avoid terms that are not related. Maybe \"pediatric\", \"cardiac\", \"oncologic\".\n",
      "\n",
      "So final check:\n",
      "\n",
      "Population: [\"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\", \"adults with pectus excavatum\", \"patients undergoing minimally invasive pectus repair\"]\n",
      "\n",
      "Intervention: [\"intercostal nerve cryoablation\", \"cryoanalgesia\", \"INC\", \"intraoperative cryoablation\"]\n",
      "\n",
      "Comparators: [\"thoracic epidural\", \"paravertebral block\", \"intercostal nerve block\", \"erector spinae plane block\", \"systemic multimodal analgesia\"]\n",
      "\n",
      "Outcomes: [\"postoperative opioid consumption (in-hospital and at discharge)\", \"pain scores within 7 days\", \"postoperative opioid use\", \"acute postoperative pain scores\"]\n",
      "\n",
      "Must_have: [\"MIRPE\", \"Nuss\", \"cryoablation\"]\n",
      "\n",
      "Avoid: [\"pediatric\", \"cardiac\", \"oncologic\"]\n",
      "\n",
      "But need to check if the arrays are between 2-10 items. Population has three, intervention four, comparators five, outcomes four, must_have three, avoid three. That fits.\n",
      "\n",
      "Now, make sure that all terms are plain phrases without any brackets or tags. Also, check for standard medical terms. For example, \"systemic multimodal analgesia\" is correct. \"Erector spinae plane block\" is a known term. \"Cryoanalgesia\" is acceptable as an acronym. \"INC\" is the abbreviation for intercostal nerve cryoablation.\n",
      "\n",
      "Yes, this should meet the requirements.\n",
      "</think>\n",
      "\n",
      "{\n",
      "  \"population\": [\n",
      "    \"adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE)\",\n",
      "    \"patients with pectus excavatum\",\n",
      "    \"postoperative patients with pectus excavatum\"\n",
      "  ],\n",
      "  \"intervention\": [\n",
      "    \"intercostal nerve cryoablation\",\n",
      "    \"cryoanalgesia\",\n",
      "    \"INC\",\n",
      "    \"intraoperative cryoablation\"\n",
      "  ],\n",
      "  \"comparators\": [\n",
      "    \"thoracic epidural\",\n",
      "    \"paravertebral block\",\n",
      "    \"intercostal nerve block\",\n",
      "    \"erector spinae plane block\",\n",
      "    \"systemic multimodal analgesia\"\n",
      "  ],\n",
      "  \"outcomes\": [\n",
      "    \"postoperative opioid consumption (in-hospital and at discharge)\",\n",
      "    \"pain scores within 7 days\",\n",
      "    \"postoperative opioid use\",\n",
      "    \"acute postoperative pain scores\"\n",
      "  ],\n",
      "  \"must_have\": [\n",
      "    \"MIRPE\",\n",
      "    \"Nuss\",\n",
      "    \"cryoablation\"\n",
      "  ],\n",
      "  \"avoid\": [\n",
      "    \"pediatric\",\n",
      "    \"cardiac\",\n",
      "    \"oncologic\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# dump_lmstudio.py\n",
    "import os, json, datetime, requests\n",
    "\n",
    "BASE = os.getenv(\"LMSTUDIO_BASE\", \"http://127.0.0.1:1234\")\n",
    "URL  = f\"{BASE.rstrip('/')}/v1/chat/completions\"\n",
    "\n",
    "def dump_completion(\n",
    "    model: str,\n",
    "    system: str,\n",
    "    user: str,\n",
    "    *,\n",
    "    temperature: float = 0.0,\n",
    "    max_tokens: int = 1200,\n",
    "    stop=None,\n",
    "    stream: bool = False,\n",
    "):\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": user},\n",
    "        ],\n",
    "        \"temperature\": float(temperature),\n",
    "        \"max_tokens\": int(max_tokens),\n",
    "        \"stream\": bool(stream),\n",
    "    }\n",
    "    if stop:\n",
    "        body[\"stop\"] = stop\n",
    "\n",
    "    print(\"=== REQUEST BODY ===\")\n",
    "    print(json.dumps(body, ensure_ascii=False, indent=2))\n",
    "\n",
    "    r = requests.post(URL, headers={\"Content-Type\": \"application/json\"}, json=body, stream=stream)\n",
    "    print(\"\\n=== HTTP STATUS ===\")\n",
    "    print(r.status_code)\n",
    "\n",
    "    if stream:\n",
    "        print(\"\\n=== RAW STREAM ===\")\n",
    "        raw_chunks = []\n",
    "        for line in r.iter_lines(decode_unicode=True):\n",
    "            if line:\n",
    "                print(line)\n",
    "                raw_chunks.append(line)\n",
    "        raw_text = \"\\n\".join(raw_chunks)\n",
    "    else:\n",
    "        print(\"\\n=== RAW TEXT ===\")\n",
    "        raw_text = r.text\n",
    "        print(raw_text)\n",
    "\n",
    "    # Save raw response exactly as returned\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_path = f\"lmstudio_raw_{ts}.txt\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_text)\n",
    "    print(f\"\\n(saved exact response to {out_path})\")\n",
    "\n",
    "    # Try to parse JSON (optional)\n",
    "    if not stream:\n",
    "        try:\n",
    "            js = r.json()\n",
    "            print(\"\\n=== PARSED: choices[0].message.content ===\")\n",
    "            print(js[\"choices\"][0][\"message\"][\"content\"])\n",
    "        except Exception as e:\n",
    "            print(\"\\n(JSON parse failed or no message.content):\", repr(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "\n",
    "    # Model + system\n",
    "    model  = os.getenv(\"LM_MODEL\",  \"qwen/qwen3-4b\")\n",
    "    system = os.getenv(\"LM_SYSTEM\", \"Return exactly one JSON object between BEGIN_JSON and END_JSON. No other text.\")\n",
    "\n",
    "    # Prompt for extracting clean term lists (P/I/C/O + must_have + avoid)\n",
    "    user = \"\"\"TASK\n",
    "You will extract concise biomedical term lists from the natural-language question (NLQ) below.\n",
    "\n",
    "Output policy:\n",
    "- Return EXACTLY ONE JSON object between:\n",
    "  BEGIN_JSON\n",
    "  { ... }\n",
    "  END_JSON\n",
    "- No other text. No backticks. No “think” prefaces.\n",
    "- Arrays only; 2–10 items per list when possible.\n",
    "- Items are plain phrases (no boolean operators, quotes, field tags, or brackets).\n",
    "- Prefer standard medical wording and common acronyms (e.g., MIRPE, INC).\n",
    "- Keep scope tightly on the NLQ intent.\n",
    "\n",
    "Fill these keys:\n",
    "- population: synonyms/labels for adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\n",
    "- intervention: synonyms/labels for intercostal nerve cryoablation used for analgesia during Nuss/MIRPE (e.g., cryoanalgesia, INC).\n",
    "- comparators: thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, systemic multimodal analgesia (and common variants).\n",
    "- outcomes: postoperative opioid consumption and pain scores within 0–7 days (include common phrasings).\n",
    "- must_have: 3–6 anchor tokens that should appear to ensure topicality (e.g., MIRPE, Nuss, cryoablation).\n",
    "- avoid: 3–6 obvious confounders to avoid if they dominate (e.g., pediatric oncology, cardiac surgery).\n",
    "\n",
    "NLQ\n",
    "Population = adults undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE). Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE (the intervention of interest is INC, not the surgery). Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia. Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores within 0–7 days. Study designs = RCTs preferred; if RCTs absent, include comparative cohort/case-control. Year_min = 2015. Languages = English, Portuguese, Spanish.\n",
    "\n",
    "TEMPLATE_TO_FILL (structure only; you must populate the arrays with phrases)\n",
    "BEGIN_JSON\n",
    "{\n",
    "  \"population\": [],\n",
    "  \"intervention\": [],\n",
    "  \"comparators\": [],\n",
    "  \"outcomes\": [],\n",
    "  \"must_have\": [],\n",
    "  \"avoid\": []\n",
    "}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "    # No stops by default (safer for models that emit <think> blocks)\n",
    "    stops  = os.getenv(\"LM_STOP\", \"\").split(\"|\") if os.getenv(\"LM_STOP\") else None\n",
    "    # If you want to try cutting prefaces, set:\n",
    "    # stops = [\"</think>\", \"```\"]\n",
    "\n",
    "\n",
    "    dump_completion(\n",
    "        model=model,\n",
    "        system=system,\n",
    "        user=user,\n",
    "        temperature=0.0,\n",
    "        max_tokens=4000,\n",
    "        stop=stops,        # e.g., export LM_STOP=\"END_JSON|</think>|```\"\n",
    "        stream=False,      # set True to see event stream\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
