Project structure for '/c/Users/Galaxy/LEVI/projects/Python/qucik_sr':
===============================================================================
  plan.md
  src/__init__.py
  src/__pycache__/__init__.cpython-312.pyc
  src/cli/__init__.py
  src/cli/__pycache__/__init__.cpython-312.pyc
  src/cli/__pycache__/main.cpython-312.pyc
  src/cli/main.py
  src/config/__init__.py
  src/config/__pycache__/__init__.cpython-312.pyc
  src/config/__pycache__/defaults.cpython-312.pyc
  src/config/__pycache__/schema.cpython-312.pyc
  src/config/defaults.py
  src/config/schema.py
  src/graph/__init__.py
  src/graph/__pycache__/__init__.cpython-312.pyc
  src/graph/__pycache__/cile.cpython-312.pyc
  src/graph/cile.py
  src/io/__init__.py
  src/io/__pycache__/__init__.cpython-312.pyc
  src/io/__pycache__/store.cpython-312.pyc
  src/io/store.py
  src/net/__init__.py
  src/net/__pycache__/__init__.cpython-312.pyc
  src/net/__pycache__/entrez.cpython-312.pyc
  src/net/__pycache__/icite.cpython-312.pyc
  src/net/entrez.py
  src/net/icite.py
  src/prisma/__init__.py
  src/prisma/counts.py
  src/screen/__init__.py
  src/screen/__pycache__/__init__.cpython-312.pyc
  src/screen/__pycache__/features.cpython-312.pyc
  src/screen/__pycache__/gates.cpython-312.pyc
  src/screen/__pycache__/llm_decide.cpython-312.pyc
  src/screen/__pycache__/regressor.cpython-312.pyc
  src/screen/features.py
  src/screen/gates.py
  src/screen/llm_decide.py
  src/screen/regressor.py
  src/text/__init__.py
  src/text/__pycache__/__init__.cpython-312.pyc
  src/text/__pycache__/embed.cpython-312.pyc
  src/text/__pycache__/llm.cpython-312.pyc
  src/text/__pycache__/prompts.cpython-312.pyc
  src/text/embed.py
  src/text/llm.py
  src/text/prompts.py



###############################################################################
### FILE: src/cli/main.py
###############################################################################
from __future__ import annotations
import sys, json, typer
from typing import List, Dict
from src.config.defaults import (ESEARCH_RETMAX_PER_QUERY, SEED_SEM_TAU_HI, SEED_MIN_COUNT, SEED_RELAX_STEP,
                              CILE_REL_GATE_FRAC, CILE_EXT_BUDGET, CILE_MAX_ACCEPT, CILE_HUB_QUARANTINE, CILE_MIN_HUB_SOFT,
                              REG_P_HI, REG_P_LO, LLM_BUDGET, DEFAULT_LANGS, DEFAULT_YEAR_MIN)
from src.config.schema import Criteria, Document, LedgerRow
from src.text.prompts import P0_SYSTEM, p0_user_prompt
from src.text.llm import chat_json
from src.net.entrez import esearch, efetch_abstracts
from src.text.embed import embed_texts
from src.screen.gates import objective_gate, is_primary_design
from src.screen.features import build_embeddings, compute_signals
from src.screen.regressor import OnlineRegressor, featurize_row
from src.screen.llm_decide import llm_decide_batch
from src.graph.cile import one_wave_expand
from src.io.store import ensure_run_dir, write_json, write_tsv

app = typer.Typer(add_completion=False)

def _to_docs(meta_map: Dict[str,dict]) -> List[Document]:
    docs = []
    for pmid, m in meta_map.items():
        docs.append(Document(
            pmid=str(pmid), title=m.get("title") or "", abstract=m.get("abstract") or "",
            year=m.get("year"), journal=m.get("journal"), language=m.get("language"),
            pub_types=m.get("pub_types") or [], doi=m.get("doi")
        ))
    return docs

@app.command()
def run(prompt: str = typer.Argument(..., help="Topic intent / preferences paragraph"),
        languages: str = typer.Option(",".join(DEFAULT_LANGS), "--languages"),
        year_min: int = typer.Option(DEFAULT_YEAR_MIN, "--year-min"),
        out_dir: str = typer.Option("runs/demo", "--out"),
        llm_budget: int = typer.Option(LLM_BUDGET, "--llm-budget"),
        ):
    # ---- 1) P0 criteria ----
    criteria_js = chat_json(P0_SYSTEM, p0_user_prompt(prompt))
    # sanitize missing fields
    if "picos" not in criteria_js:
        criteria_js["picos"] = {"population":"","intervention":"","comparison":None,"outcomes":[],"study_design":[],
                                "year_min":year_min,"languages":languages.split(",")}
    else:
        criteria_js["picos"].setdefault("year_min", year_min)
        criteria_js["picos"].setdefault("languages", languages.split(","))
    criteria = Criteria(**criteria_js)

    run_dir = ensure_run_dir(out_dir)
    write_json(run_dir/"criteria.json", criteria.model_dump())

    # ---- 2) Retrieval ----
    pmid_set = set()
    for name, q in criteria.boolean_queries.items():
        ids = esearch(q, retmax=ESEARCH_RETMAX_PER_QUERY, mindate=criteria.picos.year_min)
        pmid_set.update(ids)
    meta = efetch_abstracts(list(pmid_set))
    docs = _to_docs(meta)
    write_json(run_dir/"identification.json", {"queries": list(criteria.boolean_queries.items()), "hits": len(docs)})

    if not docs:
        print("No records retrieved.", file=sys.stderr)
        raise typer.Exit(code=1)

    # ---- 3) Embeddings & intent vector ----
    emb, idx = build_embeddings(docs)
    intent_vec = embed_texts([prompt])[0]

    # ---- 4) Seeds S+ ----
    tau = SEED_SEM_TAU_HI
    seeds = [d.pmid for d in docs if is_primary_design(d) and (float(emb[idx[d.pmid]] @ intent_vec) >= tau)]
    while len(seeds) < SEED_MIN_COUNT and tau > 0.80:
        tau -= SEED_RELAX_STEP
        seeds = [d.pmid for d in docs if is_primary_design(d) and (float(emb[idx[d.pmid]] @ intent_vec) >= tau)]

    # ---- 5) CILE expansion (1 wave) ----
    H0 = set(int(x) for x in seeds)
    H1, meta_cile = one_wave_expand(
        seeds_pos=[int(x) for x in seeds],
        H_existing=H0,
        rel_gate_frac=CILE_REL_GATE_FRAC,
        external_budget=CILE_EXT_BUDGET,
        max_accept=CILE_MAX_ACCEPT,
        hub_quarantine_external=CILE_HUB_QUARANTINE,
        min_hub_soft=CILE_MIN_HUB_SOFT
    )
    # add new pmids to pool (fetch if missing)
    new_pmids = [str(x) for x in list(H1 - H0)]
    if new_pmids:
        extra = efetch_abstracts(new_pmids)
        meta.update(extra)
        docs = _to_docs(meta)
        emb, idx = build_embeddings(docs)  # extend embedding matrix

    # ---- 6) Signals ----
    signals = compute_signals(docs, emb, idx, intent_vec, seeds, criteria.picos.year_min or year_min)

    # ---- 7) Objective gates ----
    ledger: List[LedgerRow] = []
    pool_for_model = []
    for d in docs:
        g = objective_gate(d, criteria.picos)
        if g is not None:
            reason_code, reason = g
            row = LedgerRow(
                pmid=d.pmid, lane_before_llm="auto_exclude", gate_reason=reason,
                model_p=None, llm=None,
                final_decision="exclude", final_reason=reason,
                signals=signals[d.pmid], pub_types=d.pub_types, year=d.year,
                title=d.title, abstract=d.abstract
            )
            ledger.append(row)
        else:
            # possible auto-include (slam-dunk)
            sig = signals[d.pmid]
            if (is_primary_design(d) and sig.sem_intent >= 0.92 and (sig.graph_ppr_pct >= 90.0 or sig.graph_links_frac >= 0.20)):
                row = LedgerRow(
                    pmid=d.pmid, lane_before_llm="auto_include", gate_reason=None,
                    model_p=1.0, llm=None,
                    final_decision="include", final_reason="insufficient_info",  # reason not used for includes
                    signals=sig, pub_types=d.pub_types, year=d.year,
                    title=d.title, abstract=d.abstract
                )
                ledger.append(row)
            else:
                pool_for_model.append(d)

    # ---- 8) Regressor bootstrap ----
    # positives: seeds; negatives: auto_excludes + low semantic tail
    X_boot, y_boot = [], []
    seed_set = set(seeds)
    for d in docs:
        sig = signals[d.pmid]
        X_boot.append(featurize_row(sig, d))
        y_boot.append(1 if d.pmid in seed_set else 0)
    import numpy as np
    X_boot = np.stack(X_boot, axis=0); y_boot = np.array(y_boot, dtype="int64")
    reg = OnlineRegressor()
    reg.fit_bootstrap(X_boot, y_boot)

    # ---- 9) Model triage ----
    uncertain_docs = []
    for d in pool_for_model:
        p = float(reg.predict_proba(np.stack([featurize_row(signals[d.pmid], d)], axis=0))[0])
        if p >= REG_P_HI:
            ledger.append(LedgerRow(pmid=d.pmid, lane_before_llm="model_include", gate_reason=None, model_p=p, llm=None,
                                    final_decision="include", final_reason="insufficient_info",  # not used
                                    signals=signals[d.pmid], pub_types=d.pub_types, year=d.year, title=d.title, abstract=d.abstract))
        elif p <= REG_P_LO:
            ledger.append(LedgerRow(pmid=d.pmid, lane_before_llm="model_exclude", gate_reason="off_topic", model_p=p, llm=None,
                                    final_decision="exclude", final_reason="off_topic",
                                    signals=signals[d.pmid], pub_types=d.pub_types, year=d.year, title=d.title, abstract=d.abstract))
        else:
            uncertain_docs.append(d)

    # ---- 10) LLM budget on most uncertain ----
    # rank by closeness to 0.5 using current regressor
    if uncertain_docs:
        import numpy as np
        ps = []
        for d in uncertain_docs:
            p = float(reg.predict_proba(np.stack([featurize_row(signals[d.pmid], d)], axis=0))[0])
            ps.append((d, abs(p - 0.5), p))
        ps.sort(key=lambda t: t[1])  # smallest distance first
        to_llm = [d for d,_,_ in ps[:llm_budget]]
        llm_out = llm_decide_batch(criteria, to_llm, signals)
        # update model with LLM labels (include=1, exclude=0; borderline -> skip now)
        X_upd, y_upd = [], []
        for dec in llm_out:
            d = next(dd for dd in to_llm if dd.pmid == dec.pmid)
            row = LedgerRow(
                pmid=d.pmid, lane_before_llm="sent_to_llm", gate_reason=None, model_p=None,
                llm=dec, final_decision=dec.decision,
                final_reason=dec.primary_reason,
                signals=signals[d.pmid], pub_types=d.pub_types, year=d.year,
                title=d.title, abstract=d.abstract
            )
            ledger.append(row)
            if dec.decision == "include":
                X_upd.append(featurize_row(signals[d.pmid], d)); y_upd.append(1)
            elif dec.decision == "exclude":
                X_upd.append(featurize_row(signals[d.pmid], d)); y_upd.append(0)
        if X_upd:
            reg.partial_update(np.stack(X_upd, axis=0), np.array(y_upd, dtype="int64"))

        # any remaining uncertain not sent to LLM -> borderline (for next layer P2)
        skipped = set(dd.pmid for dd,_,_ in ps[llm_budget:])
        for d in uncertain_docs:
            if d.pmid in skipped:
                ledger.append(LedgerRow(
                    pmid=d.pmid, lane_before_llm="uncertain", gate_reason=None, model_p=None, llm=None,
                    final_decision="borderline", final_reason="insufficient_info",
                    signals=signals[d.pmid], pub_types=d.pub_types, year=d.year,
                    title=d.title, abstract=d.abstract
                ))

    # ---- 11) Write outputs ----
    # screening.tsv (ledger)
    fieldnames = ["pmid","lane_before_llm","gate_reason","model_p","final_decision","final_reason","year","pub_types","title","abstract"]
    rows = []
    for r in ledger:
        rows.append({
            "pmid": r.pmid,
            "lane_before_llm": r.lane_before_llm,
            "gate_reason": r.gate_reason or "",
            "model_p": "" if r.model_p is None else f"{r.model_p:.3f}",
            "final_decision": r.final_decision,
            "final_reason": r.final_reason,
            "year": r.year or "",
            "pub_types": ";".join(r.pub_types),
            "title": r.title.replace("\t"," ").replace("\n"," ").strip(),
            "abstract": (r.abstract or "").replace("\t"," ").replace("\n"," ").strip(),
        })
    write_tsv(run_dir/"screening.tsv", rows, fieldnames)

    # prisma counts
    from src.prisma.counts import prisma_counts
    write_json(run_dir/"prisma.json", prisma_counts([r for r in ledger]))

    # cile meta
    write_json(run_dir/"cile.json", meta_cile)

    print(f"Done. Records: {len(docs)} | Ledger: {len(ledger)} | Out: {run_dir}")

if __name__ == "__main__":
    app()



###############################################################################
### FILE: src/config/defaults.py
###############################################################################
from __future__ import annotations
import os

# ---- External services ----
LMSTUDIO_BASE    = os.getenv("LMSTUDIO_BASE", "http://127.0.0.1:1234")
LMSTUDIO_EMB     = os.getenv("LMSTUDIO_EMB_MODEL", "text-embedding-qwen3-embedding-0.6b")
LMSTUDIO_CHAT    = os.getenv("LMSTUDIO_CHAT_MODEL", "gemma-3n-e2b-it")

ENTREZ_EMAIL     = os.getenv("ENTREZ_EMAIL", "you@example.com")
ENTREZ_API_KEY   = os.getenv("ENTREZ_API_KEY", "")
HTTP_TIMEOUT     = int(os.getenv("HTTP_TIMEOUT", "30"))
USER_AGENT       = os.getenv("USER_AGENT", "sr-screener/0.1 (+local)")

ICITE_BASE       = os.getenv("ICITE_BASE", "https://icite.od.nih.gov/api/pubs")

# ---- Retrieval ----
ESEARCH_RETMAX_PER_QUERY = int(os.getenv("ESEARCH_RETMAX_PER_QUERY", "2000"))

# ---- Embeddings ----
EMB_BATCH        = int(os.getenv("EMB_BATCH", "48"))

# ---- Seeds & thresholds ----
SEED_SEM_TAU_HI  = float(os.getenv("SEED_SEM_TAU_HI", "0.92"))
SEED_MIN_COUNT   = int(os.getenv("SEED_MIN_COUNT", "8"))
SEED_RELAX_STEP  = float(os.getenv("SEED_RELAX_STEP", "0.02"))

# ---- CILE knobs (compute-bounded, 1 wave typical) ----
CILE_REL_GATE_FRAC      = float(os.getenv("CILE_REL_GATE_FRAC", "0.08"))  # links_to_S+/deg
CILE_EXT_BUDGET         = int(os.getenv("CILE_EXT_BUDGET", "2500"))       # sum of external deg
CILE_MAX_ACCEPT         = int(os.getenv("CILE_MAX_ACCEPT", "600"))        # reservoir cap
CILE_HUB_QUARANTINE     = os.getenv("CILE_HUB_QUARANTINE", "1") == "1"    # external mode
CILE_MIN_HUB_SOFT       = int(os.getenv("CILE_MIN_HUB_SOFT", "450"))

# ---- Regressor thresholds ----
REG_P_HI         = float(os.getenv("REG_P_HI", "0.85"))
REG_P_LO         = float(os.getenv("REG_P_LO", "0.15"))

# ---- LLM budget ----
LLM_BUDGET       = int(os.getenv("LLM_BUDGET", "150"))

# ---- Languages & year defaults (can be overridden by P0) ----
DEFAULT_LANGS    = os.getenv("DEFAULT_LANGS", "English,Portuguese,Spanish").split(",")
DEFAULT_YEAR_MIN = int(os.getenv("DEFAULT_YEAR_MIN", "2000"))

# ---- FS ----
DATA_DIR         = os.getenv("DATA_DIR", "data")
RUNS_DIR         = os.getenv("RUNS_DIR", "runs")



###############################################################################
### FILE: src/config/schema.py
###############################################################################
from __future__ import annotations
from typing import List, Dict, Optional, Literal
from pydantic import BaseModel, Field

Reason = Literal[
    "design_mismatch","population_mismatch","intervention_mismatch",
    "language","year","insufficient_info","off_topic"
]

class PICOS(BaseModel):
    population: str
    intervention: str
    comparison: Optional[str] = None
    outcomes: List[str]
    study_design: List[str]
    year_min: Optional[int] = None
    languages: List[str] = ["English"]

class Criteria(BaseModel):
    picos: PICOS
    inclusion_criteria: Dict[str, str]
    exclusion_criteria: Dict[str, str]
    reason_taxonomy: List[Reason]
    boolean_queries: Dict[str, str]

class Document(BaseModel):
    pmid: str
    title: Optional[str] = ""
    abstract: Optional[str] = ""
    year: Optional[int] = None
    journal: Optional[str] = None
    language: Optional[str] = None
    pub_types: List[str] = []
    doi: Optional[str] = None

class Signals(BaseModel):
    sem_intent: float
    sem_seed: float
    graph_ppr_pct: float
    graph_links_frac: float
    year_scaled: float
    abstract_len_bin: Literal["none","short","normal","long"]

class DecisionLLM(BaseModel):
    pmid: str
    decision: Literal["include","exclude","borderline"]
    primary_reason: Reason
    confidence: float
    evidence: Dict[str, str]  # population_quote, intervention_quote, design_evidence, notes

class LedgerRow(BaseModel):
    pmid: str
    lane_before_llm: Literal["auto_exclude","auto_include","sent_to_llm","model_exclude","model_include","uncertain"]
    gate_reason: Optional[Reason] = None
    model_p: Optional[float] = None
    llm: Optional[DecisionLLM] = None
    final_decision: Literal["include","exclude","borderline"]
    final_reason: Reason
    signals: Signals
    pub_types: List[str]
    year: Optional[int]
    title: str
    abstract: str



###############################################################################
### FILE: src/graph/cile.py
###############################################################################
from __future__ import annotations
from typing import List, Dict, Set, Tuple
import random
from src.net.icite import icite_neighbors_map, icite_degree_total

def one_wave_expand(
    seeds_pos: List[int],
    H_existing: Set[int] | None,
    rel_gate_frac: float = 0.08,
    external_budget: int = 2500,
    max_accept: int = 600,
    hub_quarantine_external: bool = True,
    min_hub_soft: int = 450,
    rng_seed: int = 13
) -> Tuple[Set[int], Dict[str,int]]:
    random.seed(rng_seed)
    seeds_pos = [int(x) for x in seeds_pos]
    H_set = set(H_existing or set())
    neigh_map = icite_neighbors_map([str(s) for s in seeds_pos])
    neighborhood: Set[int] = set()
    for s in seeds_pos:
        neighborhood |= set(neigh_map.get(str(s), set()))
        neighborhood.add(s)
    candidates = list(neighborhood - H_set)
    accepted: List[Tuple[int,int]] = []
    total_ext = 0
    tmp = []
    for v in candidates:
        deg_tot = icite_degree_total(v)
        if deg_tot <= 0:
            continue
        vn = neigh_map.get(str(v))
        if vn is None:
            vn = icite_neighbors_map([str(v)]).get(str(v), set())
        rel = len(set(int(x) for x in vn) & set(seeds_pos)) / max(1, deg_tot)
        if rel < rel_gate_frac:
            continue
        ext = len(set(int(x) for x in vn) - H_set)
        if hub_quarantine_external and ext >= min_hub_soft:
            continue
        tmp.append((v, ext))
    tmp.sort(key=lambda t: (t[1], t[0]))
    for v, ext in tmp:
        if total_ext + ext > external_budget:
            continue
        accepted.append((v, ext))
        total_ext += ext
        if len(accepted) >= max_accept:
            break
    acc_nodes = set(v for v, _ in accepted)
    meta = {"H_prev": len(H_set), "neighborhood": len(neighborhood), "candidates": len(candidates),
            "accepted": len(acc_nodes), "sum_ext_after": total_ext}
    return (H_set | acc_nodes), meta



###############################################################################
### FILE: src/io/store.py
###############################################################################
from __future__ import annotations
import os, pathlib, json, csv
from typing import Any, List, Dict
from src.config.defaults import RUNS_DIR

def ensure_run_dir(out_dir: str) -> pathlib.Path:
    p = pathlib.Path(out_dir) if out_dir else pathlib.Path(RUNS_DIR) / "run"
    p.mkdir(parents=True, exist_ok=True)
    return p

def write_json(path: pathlib.Path, obj: Any):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def write_tsv(path: pathlib.Path, rows: List[Dict[str,Any]], fieldnames: List[str]):
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, delimiter="\t", fieldnames=fieldnames)
        w.writeheader()
        for r in rows:
            w.writerow(r)



###############################################################################
### FILE: src/net/entrez.py
###############################################################################
from __future__ import annotations
import requests, re, xml.etree.ElementTree as ET
from typing import List, Dict, Any, Iterable, Optional
from src.config.defaults import ENTREZ_EMAIL, ENTREZ_API_KEY, HTTP_TIMEOUT, USER_AGENT

EUTILS = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
HEADERS = {"User-Agent": USER_AGENT, "Accept": "application/json"}

def esearch(query: str, db: str = "pubmed", retmax: int = 10000, mindate: Optional[int]=None, maxdate: Optional[int]=None, sort: str="date") -> List[str]:
    params = {"db": db, "term": query, "retmode": "json", "retmax": retmax, "sort": sort, "email": ENTREZ_EMAIL}
    if ENTREZ_API_KEY: params["api_key"] = ENTREZ_API_KEY
    if mindate: params["mindate"] = str(mindate)
    if maxdate: params["maxdate"] = str(maxdate)
    r = requests.get(f"{EUTILS}/esearch.fcgi", headers=HEADERS, params=params, timeout=HTTP_TIMEOUT)
    r.raise_for_status()
    return r.json().get("esearchresult", {}).get("idlist", [])

def efetch_abstracts(pmids: Iterable[str]) -> Dict[str, Dict[str,Any]]:
    pmids = [str(p) for p in pmids]
    out: Dict[str,Dict[str,Any]] = {}
    def _join(node) -> str:
        if node is None: return ""
        try: return "".join(node.itertext())
        except Exception: return (getattr(node, "text", None) or "")
    for i in range(0, len(pmids), 200):
        chunk = pmids[i:i+200]
        params = {"db":"pubmed", "retmode":"xml", "rettype":"abstract", "id":",".join(chunk), "email":ENTREZ_EMAIL}
        if ENTREZ_API_KEY: params["api_key"] = ENTREZ_API_KEY
        r = requests.get(f"{EUTILS}/efetch.fcgi", headers={"User-Agent":USER_AGENT}, params=params, timeout=HTTP_TIMEOUT)
        r.raise_for_status()
        root = ET.fromstring(r.text)
        for art in root.findall(".//PubmedArticle"):
            pmid = art.findtext(".//PMID") or ""
            title = _join(art.find(".//ArticleTitle")).strip()
            abs_nodes = art.findall(".//Abstract/AbstractText")
            abstract = " ".join(_join(n).strip() for n in abs_nodes) if abs_nodes else ""
            year = None
            for path in (".//ArticleDate/Year",".//PubDate/Year",".//DateCreated/Year",".//PubDate/MedlineDate"):
                s = art.findtext(path)
                if s:
                    m = re.search(r"\d{4}", s)
                    if m: year = int(m.group(0)); break
            journal = art.findtext(".//Journal/Title") or ""
            pubtypes = [pt.text for pt in art.findall(".//PublicationTypeList/PublicationType") if pt.text]
            doi = None
            for idn in art.findall(".//ArticleIdList/ArticleId"):
                if (idn.attrib.get("IdType","").lower()=="doi") and idn.text:
                    doi = idn.text.strip().lower()
            lang = art.findtext(".//Language") or None
            out[pmid] = {"pmid": pmid, "title": title, "abstract": abstract, "year": year,
                         "journal": journal, "pub_types": pubtypes, "doi": doi, "language": lang}
    return out



###############################################################################
### FILE: src/net/icite.py
###############################################################################
from __future__ import annotations
import requests, sqlite3, json, pathlib, time
from typing import Dict, List
from src.config.defaults import ICITE_BASE, HTTP_TIMEOUT, USER_AGENT, DATA_DIR

HEADERS = {"User-Agent": USER_AGENT, "Accept": "application/json"}
CACHE_DIR = pathlib.Path(DATA_DIR) / "cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)
DB_PATH = CACHE_DIR / "icite.sqlite3"

class ICiteCache:
    def __init__(self, path: pathlib.Path = DB_PATH):
        self._conn = sqlite3.connect(str(path))
        self._conn.execute("""CREATE TABLE IF NOT EXISTS pubs(
            pmid TEXT PRIMARY KEY, json TEXT NOT NULL
        )""")
        self._conn.commit()

    def get_many(self, pmids: List[str]) -> Dict[str, dict]:
        if not pmids: return {}
        q = ",".join("?"*len(pmids))
        cur = self._conn.execute(f"SELECT pmid,json FROM pubs WHERE pmid IN ({q})", pmids)
        out = {}
        for pmid, blob in cur.fetchall():
            try: out[pmid] = json.loads(blob)
            except Exception: pass
        return out

    def put_many(self, rows: List[dict]) -> int:
        data = []
        for rec in rows:
            pmid = str(rec.get("pmid") or rec.get("_id") or "")
            if not pmid: continue
            data.append((pmid, json.dumps(rec)))
        if not data: return 0
        self._conn.executemany("INSERT OR REPLACE INTO pubs(pmid,json) VALUES(?,?)", data)
        self._conn.commit()
        return len(data)

def icite_pubs_fetch(pmids: List[str]) -> List[dict]:
    out: List[dict] = []
    base = ICITE_BASE.rstrip("/")
    for i in range(0, len(pmids), 200):
        sub = pmids[i:i+200]
        params = {"pmids": ",".join(sub), "legacy": "false"}
        r = requests.get(base, headers=HEADERS, params=params, timeout=HTTP_TIMEOUT)
        r.raise_for_status()
        data = r.json().get("data", r.json())
        if isinstance(data, list):
            out.extend(data)
        time.sleep(0.34)
    return out

def icite_neighbors_map(pmids: List[str]) -> Dict[str, set]:
    cache = ICiteCache()
    have = cache.get_many(pmids)
    need = [p for p in pmids if p not in have]
    if need:
        rows = icite_pubs_fetch(need)
        cache.put_many(rows)
        for rec in rows:
            have[str(rec.get("pmid") or rec.get("_id"))] = rec
    m = {}
    for p, rec in have.items():
        cited_by = rec.get("citedByPmids", []) or rec.get("cited_by") or []
        refs     = rec.get("citedPmids", [])   or rec.get("references") or []
        m[p] = set(int(x) for x in (cited_by + refs) if x)
    return m

def icite_degree_total(pmid: int) -> int:
    cache = ICiteCache()
    have = cache.get_many([str(pmid)])
    rec = have.get(str(pmid), {})
    cited_by = rec.get("citedByPmids", []) or rec.get("cited_by") or []
    refs     = rec.get("citedPmids", [])   or rec.get("references") or []
    return len(cited_by) + len(refs)



###############################################################################
### FILE: src/prisma/counts.py
###############################################################################
from __future__ import annotations
from typing import List, Dict, Any
from collections import Counter
from src.config.schema import LedgerRow

def prisma_counts(ledger: List[LedgerRow]) -> Dict[str, Any]:
    N = len(ledger)
    decided = Counter(r.final_decision for r in ledger)
    reasons = Counter(r.final_reason for r in ledger)
    return {
        "records_screened": N,
        "included": decided.get("include", 0),
        "excluded": decided.get("exclude", 0),
        "borderline": decided.get("borderline", 0),
        "exclusions_by_reason": dict(reasons),
    }



###############################################################################
### FILE: src/screen/features.py
###############################################################################
from __future__ import annotations
from typing import Dict, List, Tuple
import numpy as np
import networkx as nx
from src.config.schema import Document, Signals
from src.text.embed import embed_texts
from src.net.icite import icite_neighbors_map

def _band_sem(x: float) -> str:
    if x >= 0.90: return "Very high"
    if x >= 0.85: return "High"
    if x >= 0.75: return "Medium"
    return "Low"

def abstract_len_bin(txt: str) -> str:
    if not txt: return "none"
    n = len(txt)
    if n < 400: return "short"
    if n < 2400: return "normal"
    return "long"

def build_embeddings(docs: List[Document]) -> Tuple[np.ndarray, Dict[str,int]]:
    texts = [ (d.title or "") + "\n" + (d.abstract or "") for d in docs ]
    mat = embed_texts(texts)
    idx = {docs[i].pmid: i for i in range(len(docs))}
    return mat, idx

def cosine(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a,b) / (np.linalg.norm(a)*np.linalg.norm(b) + 1e-12))

def compute_signals(
    docs: List[Document],
    emb: np.ndarray,
    idx: Dict[str,int],
    intent_vec: np.ndarray,
    seed_pmids: List[str],
    year_min: int | None
) -> Dict[str, Signals]:
    seed_idx = [idx[p] for p in seed_pmids if p in idx]
    if seed_idx:
        cent = emb[seed_idx].mean(axis=0); cent /= (np.linalg.norm(cent)+1e-12)
    else:
        cent = intent_vec
    pmids = [d.pmid for d in docs]
    neigh_map = icite_neighbors_map(pmids)
    G = nx.Graph()
    for i,p in enumerate(pmids):
        G.add_node(p)
    for p in pmids:
        ns = neigh_map.get(p, set())
        for q in ns:
            if str(q) in idx: G.add_edge(p, str(q))
    seeds = [p for p in seed_pmids if p in idx]
    if seeds:
        personalize = {p: 1.0/len(seeds) for p in seeds}
        ppr = nx.pagerank(G, alpha=0.85, personalization=personalize, max_iter=100)
        vals = np.array(list(ppr.values()), dtype="float32")
        ranks = {k: 100.0 * (float(np.sum(vals <= v)) / max(1,len(vals))) for k,v in ppr.items()}
    else:
        ppr = {p: 0.0 for p in pmids}
        ranks = {p: 0.0 for p in pmids}
    seeds_set = set(seeds)
    signals: Dict[str, Signals] = {}
    for d in docs:
        v = emb[idx[d.pmid]]
        sem_intent = cosine(v, intent_vec)
        sem_seed   = cosine(v, cent)
        neighbors = neigh_map.get(d.pmid, set())
        links_frac = 0.0
        if neighbors:
            links_frac = len(set(str(x) for x in neighbors) & seeds_set) / float(len(neighbors))
        ys = 0.0
        if year_min and d.year is not None:
            ys = max(0.0, min(1.0, (d.year - year_min) / (2025 - year_min)))
        signals[d.pmid] = Signals(
            sem_intent=sem_intent,
            sem_seed=sem_seed,
            graph_ppr_pct=float(ranks.get(d.pmid, 0.0)),
            graph_links_frac=float(links_frac),
            year_scaled=float(ys),
            abstract_len_bin=abstract_len_bin(d.abstract or "")
        )
    return signals

def signal_card(sig) -> str:
    return (f"Signals:\n"
            f"- Semantic match: {_band_sem(sig.sem_intent)} ({sig.sem_intent:.2f}) to intent; "
            f"seed-centroid {_band_sem(sig.sem_seed)} ({sig.sem_seed:.2f}).\n"
            f"- Graph locality: {'High' if sig.graph_ppr_pct>=90.0 else 'Medium' if sig.graph_ppr_pct>=60.0 else 'Low'} â€” "
            f"links_to_seeds {100.0*sig.graph_links_frac:.1f}%, PPR {sig.graph_ppr_pct:.0f}th pct.\n"
            f"- Abstract: {sig.abstract_len_bin}.")



###############################################################################
### FILE: src/screen/gates.py
###############################################################################
from __future__ import annotations
from typing import Optional, Tuple
from src.config.schema import Document, PICOS, Reason

PRIMARY_TYPES = {"Randomized Controlled Trial","Clinical Trial","Cohort Studies","Case-Control Studies","Observational Study","Controlled Clinical Trial","Prospective Studies"}
NON_PRIMARY_TYPES = {"Review","Meta-Analysis","Editorial","Letter","Comment","News","Case Reports","Guideline"}

def objective_gate(doc: Document, picos: PICOS) -> Optional[Tuple[str, Reason]]:
    if picos.year_min and doc.year is not None and doc.year < int(picos.year_min):
        return ("year", "year")
    if picos.languages and doc.language and doc.language not in picos.languages:
        return ("language", "language")
    if (not (doc.title or "").strip()) and (not (doc.abstract or "").strip()):
        return ("insufficient_info", "insufficient_info")
    if set(doc.pub_types) & NON_PRIMARY_TYPES:
        return ("design_mismatch", "design_mismatch")
    return None

def is_primary_design(doc: Document) -> bool:
    return bool(set(doc.pub_types) & PRIMARY_TYPES)



###############################################################################
### FILE: src/screen/llm_decide.py
###############################################################################
from __future__ import annotations
from typing import List
from src.config.schema import Criteria, Document, DecisionLLM
from src.text.llm import chat_json
from src.text.prompts import P1_SYSTEM
from src.screen.features import signal_card

def llm_decide_batch(criteria: Criteria, docs: List[Document], signals_map: dict, temperature: float = 0.1) -> List[DecisionLLM]:
    out = []
    for d in docs:
        sig = signals_map[d.pmid]
        user = (
            f"CRITERIA_JSON:\n{criteria.model_dump_json()}\n\n"
            f"RECORD:\n{d.model_dump_json()}\n\n"
            f"{signal_card(sig)}\n"
            "Return the JSON now."
        )
        resp = chat_json(P1_SYSTEM, user, temperature=temperature, max_tokens=600)
        out.append(DecisionLLM(**resp))
    return out



###############################################################################
### FILE: src/screen/regressor.py
###############################################################################
from __future__ import annotations
from typing import Dict, List, Tuple
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier

PRIMARY_TYPES = {"Randomized Controlled Trial","Clinical Trial","Cohort Studies","Case-Control Studies","Observational Study","Controlled Clinical Trial","Prospective Studies"}
NON_PRIMARY_TYPES = {"Review","Meta-Analysis","Editorial","Letter","Comment","News","Case Reports","Guideline"}

def featurize_row(sig, doc) -> np.ndarray:
    # Continuous features + simple one-hots
    f = [
        sig.sem_intent, sig.sem_seed, sig.graph_ppr_pct/100.0, sig.graph_links_frac,
        sig.year_scaled,
        1.0 if sig.abstract_len_bin=="none" else 0.0,
        1.0 if sig.abstract_len_bin=="short" else 0.0,
        1.0 if sig.abstract_len_bin=="normal" else 0.0,
        1.0 if sig.abstract_len_bin=="long" else 0.0,
        1.0 if (set(doc.pub_types) & PRIMARY_TYPES) else 0.0,
        1.0 if (set(doc.pub_types) & NON_PRIMARY_TYPES) else 0.0,
    ]
    return np.array(f, dtype="float32")

class OnlineRegressor:
    def __init__(self):
        self.scaler = StandardScaler(with_mean=True, with_std=True)
        self.clf = SGDClassifier(loss="log_loss", penalty="l2", alpha=1e-4, max_iter=1000, tol=1e-3, random_state=17)
        self._fitted = False

    def fit_bootstrap(self, X: np.ndarray, y: np.ndarray):
        # Scale then partial_fit
        self.scaler.fit(X)
        Xs = self.scaler.transform(X)
        self.clf.partial_fit(Xs, y, classes=np.array([0,1]))
        self._fitted = True

    def partial_update(self, X: np.ndarray, y: np.ndarray):
        Xs = self.scaler.transform(X)
        self.clf.partial_fit(Xs, y)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        Xs = self.scaler.transform(X)
        p = self.clf.predict_proba(Xs)[:,1]
        return p



###############################################################################
### FILE: src/text/embed.py
###############################################################################
from __future__ import annotations
import numpy as np, requests
from typing import List
from src.config.defaults import LMSTUDIO_BASE, LMSTUDIO_EMB, HTTP_TIMEOUT, USER_AGENT, EMB_BATCH

HEADERS = {"Content-Type":"application/json","User-Agent":USER_AGENT}

def embed_texts(texts: List[str], batch: int = EMB_BATCH) -> np.ndarray:
    url = f"{LMSTUDIO_BASE.rstrip('/')}/v1/embeddings"
    out: List[List[float]] = []
    for i in range(0, len(texts), batch):
        body = {"model": LMSTUDIO_EMB, "input": texts[i:i+batch]}
        r = requests.post(url, headers=HEADERS, json=body, timeout=HTTP_TIMEOUT)
        r.raise_for_status()
        data = r.json()["data"]
        out.extend(d["embedding"] for d in data)
    arr = np.array(out, dtype="float32")
    arr /= (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12)
    return arr



###############################################################################
### FILE: src/text/llm.py
###############################################################################
from __future__ import annotations
import json, re, requests
from typing import Optional
from src.config.defaults import LMSTUDIO_BASE, LMSTUDIO_CHAT, HTTP_TIMEOUT, USER_AGENT

HEADERS = {"Content-Type":"application/json","User-Agent":USER_AGENT}

def _extract_json(text: str) -> str:
    m = re.search(r'\{.*\}', text, re.S)
    if m:
        return m.group(0)
    m = re.search(r'\[.*\]', text, re.S)
    if m:
        return m.group(0)
    raise ValueError("No JSON block found")

def chat_json(system: str, user: str, temperature: float = 0.1, max_tokens: int = 700) -> dict:
    url = f"{LMSTUDIO_BASE.rstrip('/')}/v1/chat/completions"
    body = {
        "model": LMSTUDIO_CHAT,
        "messages": [{"role":"system","content":system},{"role":"user","content":user}],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False
    }
    r = requests.post(url, headers=HEADERS, json=body, timeout=HTTP_TIMEOUT)
    r.raise_for_status()
    content = r.json()["choices"][0]["message"]["content"]
    js = _extract_json(content)
    try:
        return json.loads(js)
    except Exception:
        js2 = re.sub(r',\s*([}\]])', r'\1', js)
        return json.loads(js2)



###############################################################################
### FILE: src/text/prompts.py
###############################################################################
P0_SYSTEM = """
You are configuring a PRISMA title/abstract screening. From the user's description and preferences,
produce strict, codeable criteria and PubMed boolean queries.
Return JSON ONLY with keys: picos (object with keys population, intervention, comparison, outcomes (array),
study_design (array), year_min (int|nullable), languages (array)),
inclusion_criteria (object), exclusion_criteria (object),
reason_taxonomy (array of enums from: ["design_mismatch","population_mismatch","intervention_mismatch","language","year","insufficient_info","off_topic"]),
boolean_queries (object of strings). Use double quotes. Do not add prose.
"""

def p0_user_prompt(intent_text: str) -> str:
    return f"INTENT/PREFERENCES:\n{intent_text}\n\nProduce the JSON now."

P1_SYSTEM = """
You are a PRISMA title/abstract screener. Decide INCLUDE, EXCLUDE, or BORDERLINE strictly
from CRITERIA_JSON and the record. Use pub_types only for design; never infer design from title words.
Do NOT exclude solely because outcomes are not stated; mark BORDERLINE instead.
Treat Signals as hints (not sufficient reasons).
Return JSON ONLY matching schema:
{ "pmid": "...", "decision": "include|exclude|borderline", "primary_reason": "...",
  "confidence": 0.0, "evidence": { "population_quote": "...", "intervention_quote": "...",
  "design_evidence": "pub_types: [...] or empty", "notes": "..." } }.
"""





