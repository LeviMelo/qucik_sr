{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422a2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== sniff_v4_cell1_core.py =====\n",
    "# Config, typed contracts, utilities (normalization, telemetry), and LM Studio JSON AVR wrappers.\n",
    "\n",
    "import os, re, json, time, math, pathlib, random, unicodedata, textwrap\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Optional, Tuple, Callable, Any\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# ----------------------------\n",
    "# Config / constants\n",
    "# ----------------------------\n",
    "LMSTUDIO_BASE   = os.getenv(\"LMSTUDIO_BASE\", \"http://127.0.0.1:1234\")\n",
    "QWEN_MODEL      = os.getenv(\"QWEN_MODEL\", \"unsloth/qwen3-4b\")                 # protocol, plausibility, synonyms\n",
    "SCREENER_MODEL  = os.getenv(\"SCREENER_MODEL\", \"gemma-3n-e4b-it@q4_k_s\")      # fast checklist screener\n",
    "\n",
    "ENTREZ_EMAIL    = os.getenv(\"ENTREZ_EMAIL\", \"you@example.com\")\n",
    "ENTREZ_API_KEY  = os.getenv(\"ENTREZ_API_KEY\", \"\")\n",
    "\n",
    "HTTP_TIMEOUT    = int(os.getenv(\"HTTP_TIMEOUT\", \"300\"))\n",
    "\n",
    "# Universe budgets/thresholds\n",
    "UNIVERSE_TARGET_MIN  = 50         # desired lower bound after deterministic prefilter\n",
    "UNIVERSE_HARD_MIN    = 10         # absolute floor after relax ladder\n",
    "FETCH_BUDGET_IDS     = 1200       # total efetch budget across all query candidates\n",
    "PAGE_SIZE_ES         = 200        # per-page ESearch fetch\n",
    "EFETCH_BATCH_SIZE    = 200\n",
    "\n",
    "# Rerank / screening\n",
    "SCREEN_TOP_K         = 60\n",
    "PLAUSIBILITY_MIN_INCLUDES = 3\n",
    "\n",
    "# Weights for PICO terms in rerank\n",
    "WEIGHTS = {\"P\": 1.5, \"I\": 1.75, \"C\": 1.0, \"O\": 1.0, \"ANCHOR\": 2.0, \"AVOID\": -2.5}\n",
    "\n",
    "# Output dir\n",
    "OUT_DIR = pathlib.Path(\"sniff_out_v4\")\n",
    "(OUT_DIR / \"logs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# ----------------------------\n",
    "# Knowledge base (designs, languages, pubtype aliases)\n",
    "# ----------------------------\n",
    "KB = {\n",
    "    \"publication_types_allowable_primary\": [\n",
    "        \"Randomized Controlled Trial\",\n",
    "        \"Controlled Clinical Trial\",\n",
    "        \"Clinical Trial\",\n",
    "    ],\n",
    "    \"publication_types_allowable_secondary\": [\n",
    "        \"Comparative Study\", \"Cohort Studies\", \"Case-Control Studies\",\n",
    "        \"Observational Study\", \"Multicenter Study\", \"Cross-Sectional Studies\",\n",
    "        \"Clinical Trial Protocol\", \"Evaluation Study\"\n",
    "    ],\n",
    "    # Extra hard excludes include review flavors (toggle via config below)\n",
    "    \"publication_types_hard_exclude_base\": [\n",
    "        \"Editorial\", \"Letter\", \"Comment\", \"News\", \"Interview\",\n",
    "        \"Practice Guideline\", \"Guideline\", \"Consensus Development Conference\",\n",
    "        \"Case Reports\"\n",
    "    ],\n",
    "    \"review_flavors\": [\"Review\", \"Systematic Review\", \"Meta-Analysis\"],\n",
    "    \"languages_human\": [\"english\",\"spanish\",\"portuguese\",\"french\",\"german\",\"italian\",\"chinese\",\"japanese\",\"korean\"],\n",
    "    \"pubtype_aliases\": {\n",
    "        \"Randomized Controlled Trial\": [\"Randomized Controlled Trial\"],\n",
    "        \"Controlled Clinical Trial\":  [\"Controlled Clinical Trial\"],\n",
    "        \"Clinical Trial\":             [\"Clinical Trial\"],\n",
    "        \"Comparative Study\":          [\"Comparative Study\"],\n",
    "        \"Cohort Studies\":             [\"Cohort Studies\",\"Prospective Studies\",\"Retrospective Studies\"],\n",
    "        \"Case-Control Studies\":       [\"Case-Control Studies\"],\n",
    "        \"Observational Study\":        [\"Observational Study\"],\n",
    "        \"Multicenter Study\":          [\"Multicenter Study\"],\n",
    "        \"Cross-Sectional Studies\":    [\"Cross-Sectional Studies\"],\n",
    "        \"Clinical Trial Protocol\":    [\"Clinical Trial Protocol\",\"Study Protocols\"],\n",
    "        \"Evaluation Study\":           [\"Evaluation Study\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Config toggles\n",
    "INCLUDE_REVIEW_FLAVORS = False   # if True, do NOT hard-exclude reviews in prefilter\n",
    "\n",
    "# Language normalization (PubMed often uses 3-letter codes)\n",
    "LANG_MAP = {\n",
    "    \"eng\":\"english\",\"en\":\"english\",\n",
    "    \"spa\":\"spanish\",\"es\":\"spanish\",\n",
    "    \"por\":\"portuguese\",\"pt\":\"portuguese\",\n",
    "    \"fra\":\"french\",\"fre\":\"french\",\"fr\":\"french\",\n",
    "    \"deu\":\"german\",\"ger\":\"german\",\"de\":\"german\",\n",
    "    \"ita\":\"italian\",\"it\":\"italian\",\n",
    "    \"chi\":\"chinese\",\"zho\":\"chinese\",\"zh\":\"chinese\",\n",
    "    \"jpn\":\"japanese\",\"ja\":\"japanese\",\n",
    "    \"kor\":\"korean\",\"ko\":\"korean\"\n",
    "}\n",
    "\n",
    "# ---- Token policy & optional library-based shortening ----\n",
    "TOKEN_MAX_WORDS = 4                 # final target for P/I/C/O tokens\n",
    "VALIDATE_MAX_WORDS = 12             # schema sanity cap before we post-process\n",
    "\n",
    "# Optional: spaCy models if available (opportunistic, no hard dependency)\n",
    "try:\n",
    "    import spacy\n",
    "    _SPACY_MODELS = {}\n",
    "    def _get_spacy(lang_human: str):\n",
    "        name_map = {\n",
    "            \"english\": \"en_core_web_sm\",\n",
    "            \"spanish\": \"es_core_news_sm\",\n",
    "            \"portuguese\": \"pt_core_news_sm\"\n",
    "        }\n",
    "        name = name_map.get(lang_human)\n",
    "        if not name:\n",
    "            return None\n",
    "        if name in _SPACY_MODELS:\n",
    "            return _SPACY_MODELS[name]\n",
    "        try:\n",
    "            nlp = spacy.load(name, disable=[\"ner\",\"parser\",\"textcat\"])\n",
    "            _SPACY_MODELS[name] = nlp\n",
    "            return nlp\n",
    "        except Exception:\n",
    "            return None\n",
    "except Exception:\n",
    "    spacy = None\n",
    "    _SPACY_MODELS = {}\n",
    "\n",
    "def _split_paren(t: str):\n",
    "    acrs = re.findall(r\"\\(([^)]+)\\)\", t or \"\")\n",
    "    base = re.sub(r\"\\([^)]*\\)\", \"\", t or \"\")\n",
    "    return base.strip(), [a.strip() for a in acrs if a.strip()]\n",
    "\n",
    "def _rule_shorten_one(t: str) -> list:\n",
    "    base, acrs = _split_paren(t)\n",
    "    words = [w for w in re.split(r\"\\W+\", base) if w]\n",
    "    # simple biomedical-ish pruning\n",
    "    stopish = {\"undergoing\",\"used\",\"for\",\"during\",\"repair\",\"scores\",\"consumption\",\n",
    "               \"postoperative\",\"intraoperatively\",\"procedure\",\"surgery\",\n",
    "               \"the\",\"of\",\"and\",\"or\",\"with\",\"in\",\"at\",\"to\"}\n",
    "    words = [w for w in words if w.lower() not in stopish]\n",
    "    s = \" \".join(words[:TOKEN_MAX_WORDS]).strip().lower()\n",
    "    outs = []\n",
    "    if s: outs.append(s)\n",
    "    for a in acrs:\n",
    "        if len(a.split()) <= TOKEN_MAX_WORDS:\n",
    "            al = a.lower()\n",
    "            if al and al not in outs:\n",
    "                outs.append(al)\n",
    "    return outs if outs else [norm_txt(t)[:30]]\n",
    "\n",
    "def _dedup(seq: list) -> list:\n",
    "    seen=set(); out=[]\n",
    "    for x in seq:\n",
    "        xl=(x or \"\").strip().lower()\n",
    "        if xl and xl not in seen:\n",
    "            seen.add(xl); out.append(x)\n",
    "    return out\n",
    "\n",
    "def shorten_tokens(tokens: list, languages: list) -> list:\n",
    "    \"\"\"Prefer spaCy noun-chunk compression if a compatible model is present; else fallback.\"\"\"\n",
    "    outs=[]\n",
    "    lang_try = next((l for l in (languages or []) if l in (\"english\",\"spanish\",\"portuguese\")), None)\n",
    "    nlp = _get_spacy(lang_try) if (lang_try and 'spacy' in globals()) else None\n",
    "    for t in tokens or []:\n",
    "        if not t or not t.strip(): \n",
    "            continue\n",
    "        if len(t.split()) <= TOKEN_MAX_WORDS:\n",
    "            outs.append(t.strip())\n",
    "            continue\n",
    "        if nlp:\n",
    "            doc = nlp(t)\n",
    "            # take the first noun chunk with ≤4 tokens, else head lemma(s), else fallback\n",
    "            cand = None\n",
    "            for nc in doc.noun_chunks:\n",
    "                if len(nc.text.split()) <= TOKEN_MAX_WORDS:\n",
    "                    cand = nc.text\n",
    "                    break\n",
    "            if not cand:\n",
    "                heads = [tok.lemma_ for tok in doc if tok.head == tok]\n",
    "                cand = \" \".join(heads[:TOKEN_MAX_WORDS]) if heads else None\n",
    "            if cand:\n",
    "                outs.append(cand.strip().lower())\n",
    "            else:\n",
    "                outs.extend(_rule_shorten_one(t))\n",
    "        else:\n",
    "            outs.extend(_rule_shorten_one(t))\n",
    "    return _dedup(outs)\n",
    "\n",
    "\n",
    "def normalize_lang(s: Optional[str]) -> str:\n",
    "    if not s: return \"\"\n",
    "    k = s.strip().lower()\n",
    "    return LANG_MAP.get(k, k)\n",
    "\n",
    "# ----------------------------\n",
    "# Contracts (typed records)\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Protocol:\n",
    "    narrative_question: str\n",
    "    inclusion_criteria: List[str]\n",
    "    exclusion_criteria: List[str]\n",
    "    screening_rules_note: Dict[str, str]\n",
    "    pico_tokens: Dict[str, List[str]]\n",
    "    anchors_must_have: List[str]\n",
    "    avoid_terms: List[str]\n",
    "    designs_preference: str\n",
    "    deterministic_filters: Dict[str, Any]  # {\"languages\":[...], \"year_min\": int}\n",
    "    # NEW — user-driven controls\n",
    "    designs_allowed: List[str] = field(default_factory=list)            # optional, from KB primary+secondary\n",
    "    publication_types_allowlist: List[str] = field(default_factory=list)\n",
    "    publication_types_blocklist: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QueryCandidate:\n",
    "    id: str\n",
    "    query: str\n",
    "    origin: str\n",
    "    terms_used: Dict[str, List[str]]\n",
    "    expected_breadth: str\n",
    "\n",
    "@dataclass\n",
    "class PubMedRecord:\n",
    "    pmid: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    year: Optional[int]\n",
    "    language: Optional[str]\n",
    "    pubtypes: List[str]\n",
    "    mesh: List[str]\n",
    "\n",
    "@dataclass\n",
    "class ScreenDecision:\n",
    "    pmid: str\n",
    "    decision: str               # INCLUDE|BORDERLINE|EXCLUDE\n",
    "    why: str\n",
    "    checklist: Dict[str, bool]\n",
    "    mesh_roles: List[Dict[str,str]]\n",
    "\n",
    "# ----------------------------\n",
    "# Telemetry\n",
    "# ----------------------------\n",
    "def log_jsonl(event: str, payload: Dict[str, Any], fname=\"events.jsonl\"):\n",
    "    payload = dict(payload)\n",
    "    payload[\"event\"] = event\n",
    "    payload[\"ts\"] = time.time()\n",
    "    with open(OUT_DIR / \"logs\" / fname, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(payload, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Text normalization & helpers\n",
    "# ----------------------------\n",
    "def norm_txt(s: str) -> str:\n",
    "    s = s or \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"[-/]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def or_block(terms: List[str], field=\"tiab\") -> str:\n",
    "    toks=[]\n",
    "    for t in terms:\n",
    "        t=t.strip()\n",
    "        if not t: continue\n",
    "        toks.append(f\"\\\"{t}\\\"[{field}]\" if (\" \" in t or \"-\" in t) else f\"{t}[{field}]\")\n",
    "    return \"(\" + \" OR \".join(toks) + \")\" if toks else \"\"\n",
    "\n",
    "# ----------------------------\n",
    "# LM Studio client + AVR JSON utilities\n",
    "# ----------------------------\n",
    "class LMClient:\n",
    "    def __init__(self, base=LMSTUDIO_BASE, timeout=HTTP_TIMEOUT):\n",
    "        self.base = base.rstrip(\"/\")\n",
    "        self.timeout = timeout\n",
    "    def chat(self, model: str, system: str, user: str, temperature=0.0) -> str:\n",
    "        url = f\"{self.base}/v1/chat/completions\"\n",
    "        body = {\"model\": model, \"messages\": [\n",
    "            {\"role\":\"system\",\"content\": system},\n",
    "            {\"role\":\"user\",\"content\": user}\n",
    "        ], \"temperature\": float(temperature), \"stream\": False}\n",
    "        r = requests.post(url, json=body, timeout=self.timeout)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "LM = LMClient()\n",
    "\n",
    "_BEGIN = re.compile(r\"BEGIN_JSON\\s*\", re.I)\n",
    "_END   = re.compile(r\"\\s*END_JSON\", re.I)\n",
    "_FENCE = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)```\", re.I)\n",
    "\n",
    "def _sanitize_json_str(s: str) -> str:\n",
    "    s = s.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"').replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n",
    "    s = re.sub(r\",\\s*(\\}|\\])\", r\"\\1\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_json_block_or_fence(txt: str) -> str:\n",
    "    blocks = []\n",
    "    pos=0\n",
    "    while True:\n",
    "        m1 = _BEGIN.search(txt, pos)\n",
    "        if not m1: break\n",
    "        m2 = _END.search(txt, m1.end())\n",
    "        if not m2: break\n",
    "        blocks.append(txt[m1.end():m2.start()])\n",
    "        pos = m2.end()\n",
    "    if blocks:\n",
    "        return _sanitize_json_str(blocks[-1])\n",
    "    fences = _FENCE.findall(txt)\n",
    "    if fences:\n",
    "        return _sanitize_json_str(fences[-1])\n",
    "    # last balanced {...}\n",
    "    s = txt\n",
    "    last_obj=None; stack=0; start=None\n",
    "    for i,ch in enumerate(s):\n",
    "        if ch=='{':\n",
    "            if stack==0: start=i\n",
    "            stack+=1\n",
    "        elif ch=='}':\n",
    "            if stack>0:\n",
    "                stack-=1\n",
    "                if stack==0 and start is not None:\n",
    "                    last_obj = s[start:i+1]\n",
    "    if last_obj:\n",
    "        return _sanitize_json_str(last_obj)\n",
    "    raise ValueError(\"No JSON-like content found\")\n",
    "\n",
    "STRICT_JSON_RULES = (\n",
    "  \"Return ONLY one JSON object. No analysis, no preface, no notes. \"\n",
    "  \"Wrap it EXACTLY with:\\nBEGIN_JSON\\n{...}\\nEND_JSON\"\n",
    ")\n",
    "\n",
    "def get_validated_json(model: str, system_prompt: str, user_prompt: str,\n",
    "                       validator: Callable[[Dict[str,Any]], Tuple[bool,str]],\n",
    "                       retries: int = 2, temperature: float = 0.0) -> Dict[str,Any]:\n",
    "    def _make_fix_hint(err: str) -> str:\n",
    "        e = (err or \"\").lower()\n",
    "        hints = []\n",
    "        if \"tokens too long\" in e:\n",
    "            hints.append(\n",
    "                \"- FIX: Each P/I/C/O token must be ≤ 4 words. If a comparator token has commas, split into separate items. Use short canonical forms (e.g., 'INC', 'Nuss', 'pectus excavatum').\"\n",
    "            )\n",
    "        if \"languages\" in e and \"anchors\" in e:\n",
    "            hints.append(\"- Do NOT include languages or years in anchors_must_have.\")\n",
    "        if \"designs_allowed\" in e:\n",
    "            hints.append(\"- designs_allowed must contain only items from the KB lists (primary/secondary).\")\n",
    "        return (\"\\n\".join(hints)) if hints else \"\"\n",
    "    last_err = \"\"\n",
    "    for i in range(retries+1):\n",
    "        fix = _make_fix_hint(last_err)\n",
    "        up = user_prompt + (\"\\n\\nPrevious error: \" + last_err + (\"\\n\" + fix if fix else \"\") if last_err else \"\")\n",
    "        raw = LM.chat(model, system_prompt, up + \"\\n\\n\" + STRICT_JSON_RULES, temperature=temperature)\n",
    "        log_jsonl(\"llm_json_attempt\", {\"model\": model, \"try\": i, \"chars\": len(raw)})\n",
    "        try:\n",
    "            js = json.loads(extract_json_block_or_fence(raw))\n",
    "        except Exception as e:\n",
    "            last_err = f\"Malformed JSON: {e}\"\n",
    "            print(f\"[AVR] attempt {i} → malformed JSON ({e})\")\n",
    "            if i==retries: raise RuntimeError(f\"LLM failed to produce JSON: {last_err}\")\n",
    "            continue\n",
    "        ok, why = validator(js)\n",
    "        if ok:\n",
    "            return js\n",
    "        last_err = f\"Schema invalid: {why}\"\n",
    "        print(f\"[AVR] attempt {i} → schema invalid: {why}\")\n",
    "        if i==retries:\n",
    "            raise RuntimeError(f\"LLM JSON schema invalid after retries: {why}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Protocol lockdown via LLM (typed)\n",
    "# ----------------------------\n",
    "PROTO_SYSTEM = \"\"\"You are designing a structured, search-ready SR protocol from a natural-language question.\n",
    "\n",
    "HARD CONSTRAINTS (must follow EXACTLY):\n",
    "- Return ONLY one JSON object using the provided schema (no prose).\n",
    "- P/I/C/O tokenization RULES:\n",
    "  - Each token must be ≤ 4 words (short, atomic).\n",
    "  - If a token > 4 words, SPLIT it into multiple tokens, each ≤ 4 words.\n",
    "  - For C (comparators): list EACH comparator as its OWN token (do NOT pack commas into one string).\n",
    "  - Prefer canonical short forms and acronyms (e.g., \"INC\", \"Nuss\", \"pectus excavatum\").\n",
    "  - Do NOT put languages or years into anchors_must_have.\n",
    "- 'designs_preference' must be ONE of: Randomized Controlled Trial, Controlled Clinical Trial, Clinical Trial.\n",
    "- 'deterministic_filters' MUST include: languages (subset of given KB languages, lowercased) and year_min (int).\n",
    "- If the user does not explicitly specify \"anchors_must_have\", set it to [].\n",
    "- Only include up to TWO anchors, and only if they are essential core terms (e.g., \"pectus excavatum\", \"cryoablation\").\n",
    "\n",
    "USER CHOICES:\n",
    "- If the question specifies study designs or publication types to include/exclude, populate:\n",
    "  - \"designs_allowed\": array of designs chosen from the KB primary+secondary lists.\n",
    "  - \"publication_types_allowlist\": exact Publication Type names to force-include.\n",
    "  - \"publication_types_blocklist\": exact Publication Type names to force-exclude.\n",
    "\n",
    "If incoherent, set needs_clarification=true with a brief request. Return ONLY the requested JSON.\"\"\"\n",
    "\n",
    "\n",
    "def proto_user(nlq: str) -> str:\n",
    "    kb_view = {\n",
    "        \"designs_primary\": KB[\"publication_types_allowable_primary\"],\n",
    "        \"designs_secondary\": KB[\"publication_types_allowable_secondary\"],\n",
    "        \"languages\": KB[\"languages_human\"]\n",
    "    }\n",
    "    schema = {\n",
    "      \"narrative_question\": \"<1 paragraph restatement>\",\n",
    "      \"inclusion_criteria\": [\"...\"],\n",
    "      \"exclusion_criteria\": [\"...\"],\n",
    "      \"screening_rules_note\": {\"user_notes\":\"...\", \"llm_guidance\":\"...\"},\n",
    "      \"pico_tokens\": {\"P\":[\"...\"], \"I\":[\"...\"], \"C\":[\"...\"], \"O\":[\"...\"]},\n",
    "      \"anchors_must_have\": [\"...\"],\n",
    "      \"avoid_terms\": [\"...\"],\n",
    "      \"designs_preference\": \"Randomized Controlled Trial\",\n",
    "      \"deterministic_filters\": {\"languages\": [\"english\"], \"year_min\": 2015},\n",
    "      \"designs_allowed\": [],                        # NEW (optional)\n",
    "      \"publication_types_allowlist\": [],            # NEW (optional)\n",
    "      \"publication_types_blocklist\": [],            # NEW (optional)\n",
    "      \"needs_clarification\": False,\n",
    "      \"clarification_request\": \"\"\n",
    "    }\n",
    "    return f\"\"\"Natural-Language Question:\n",
    "<<<\n",
    "{nlq.strip()}\n",
    ">>>\n",
    "\n",
    "Knowledge Base (valid choices):\n",
    "{json.dumps(kb_view, indent=2)}\n",
    "\n",
    "Output schema:\n",
    "{json.dumps(schema, indent=2)}\"\"\"\n",
    "\n",
    "def validate_protocol(js: Dict[str,Any]) -> Tuple[bool,str]:\n",
    "    try:\n",
    "        req = [\"narrative_question\",\"inclusion_criteria\",\"exclusion_criteria\",\n",
    "               \"screening_rules_note\",\"pico_tokens\",\"anchors_must_have\",\"avoid_terms\",\n",
    "               \"designs_preference\",\"deterministic_filters\",\"needs_clarification\",\"clarification_request\"]\n",
    "        for k in req:\n",
    "            if k not in js: return False, f\"missing key {k}\"\n",
    "        if not isinstance(js[\"pico_tokens\"], dict): return False, \"pico_tokens must be object\"\n",
    "        for k in [\"P\",\"I\",\"C\",\"O\"]:\n",
    "            if k not in js[\"pico_tokens\"]: return False, f\"pico_tokens missing {k}\"\n",
    "            if not isinstance(js[\"pico_tokens\"][k], list): return False, f\"pico_tokens[{k}] must be list\"\n",
    "        df = js[\"deterministic_filters\"]\n",
    "        langs = df.get(\"languages\", [])\n",
    "        if not isinstance(langs, list) or not langs: return False, \"languages must be non-empty list\"\n",
    "        # Ensure languages are subset of KB.languages_human\n",
    "        langs_ok = [l for l in langs if l.lower() in KB[\"languages_human\"]]\n",
    "        if not langs_ok: return False, \"no valid languages from KB\"\n",
    "        df[\"languages\"] = [l.lower() for l in langs_ok]\n",
    "        y = df.get(\"year_min\", 0)\n",
    "        if isinstance(y, str) and y.isdigit(): y = int(y); df[\"year_min\"] = y\n",
    "        if not isinstance(df[\"year_min\"], int): return False, \"year_min must be int\"\n",
    "        if js[\"designs_preference\"] not in KB[\"publication_types_allowable_primary\"]:\n",
    "            return False, \"designs_preference must be a primary design\"\n",
    "        # token length safeguard\n",
    "        # Optional fields sanity\n",
    "        for k in [\"designs_allowed\",\"publication_types_allowlist\",\"publication_types_blocklist\"]:\n",
    "            if k in js and not isinstance(js[k], list):\n",
    "                return False, f\"{k} must be a list\"\n",
    "            if k in js:\n",
    "                for v in js[k]:\n",
    "                    if not isinstance(v, str):\n",
    "                        return False, f\"{k} must contain strings\"\n",
    "        # If designs_allowed provided, ensure items are within KB space\n",
    "        if js.get(\"designs_allowed\"):\n",
    "            valid = set(KB[\"publication_types_allowable_primary\"] + KB[\"publication_types_allowable_secondary\"])\n",
    "            bad = [d for d in js[\"designs_allowed\"] if d not in valid]\n",
    "            if bad:\n",
    "                return False, f\"designs_allowed contains invalid: {bad[:2]}\"\n",
    "        return True, \"\"\n",
    "    except Exception as e:\n",
    "        return False, f\"exception: {e}\"\n",
    "\n",
    "def lock_protocol(nlq: str) -> Protocol:\n",
    "    js = get_validated_json(QWEN_MODEL, PROTO_SYSTEM, proto_user(nlq), validate_protocol, retries=2)\n",
    "    js = enforce_token_policy_js(js)  # library-first shorten + hygiene\n",
    "    if js.get(\"needs_clarification\"):\n",
    "        raise RuntimeError(\"Protocol needs clarification: \" + js.get(\"clarification_request\",\"\"))\n",
    "    proto = Protocol(\n",
    "        narrative_question=js[\"narrative_question\"],\n",
    "        inclusion_criteria=js[\"inclusion_criteria\"],\n",
    "        exclusion_criteria=js[\"exclusion_criteria\"],\n",
    "        screening_rules_note=js.get(\"screening_rules_note\", {}),\n",
    "        pico_tokens=js[\"pico_tokens\"],\n",
    "        anchors_must_have=js[\"anchors_must_have\"],\n",
    "        avoid_terms=js[\"avoid_terms\"],\n",
    "        designs_preference=js[\"designs_preference\"],\n",
    "        deterministic_filters=js[\"deterministic_filters\"],\n",
    "        designs_allowed=js.get(\"designs_allowed\", []),\n",
    "        publication_types_allowlist=js.get(\"publication_types_allowlist\", []),\n",
    "        publication_types_blocklist=js.get(\"publication_types_blocklist\", []),\n",
    "    )\n",
    "    log_jsonl(\"protocol_locked\", {\"protocol\": proto.__dict__})\n",
    "    print(\"[S1] Protocol locked. P/I/C/O:\", proto.pico_tokens)\n",
    "    return proto\n",
    "\n",
    "\n",
    "def enforce_token_policy_js(js: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    df = js.get(\"deterministic_filters\", {})\n",
    "    langs = [l.lower() for l in (df.get(\"languages\") or [])]\n",
    "\n",
    "    # 1) Shorten tokens (library-first), and split comparators\n",
    "    for k in [\"P\",\"I\",\"C\",\"O\"]:\n",
    "        toks = js.get(\"pico_tokens\", {}).get(k, []) or []\n",
    "        if k == \"C\":\n",
    "            split = []\n",
    "            for t in toks:\n",
    "                parts = [p.strip() for p in re.split(r\",|;|/\", t) if p.strip()]\n",
    "                split.extend(parts if parts else [t])\n",
    "            toks = split\n",
    "        js[\"pico_tokens\"][k] = shorten_tokens(toks, langs)\n",
    "\n",
    "    # 2) Domain hygiene\n",
    "    # 2a) Drop risky acronyms as standalone tokens (esp. \"inc\")\n",
    "    def _drop_bad_acronyms(arr):\n",
    "        bad = {\"inc\"}  # standalone \"inc\" is toxic in PubMed tiab\n",
    "        return [t for t in arr if t.lower() not in bad]\n",
    "    js[\"pico_tokens\"][\"I\"] = _drop_bad_acronyms(js[\"pico_tokens\"].get(\"I\", []))\n",
    "    js[\"pico_tokens\"][\"C\"] = _drop_bad_acronyms(js[\"pico_tokens\"].get(\"C\", []))\n",
    "\n",
    "    # 2b) Ensure key domain anchors exist in P and I\n",
    "    nq = (js.get(\"narrative_question\") or \"\").lower()\n",
    "    P = js[\"pico_tokens\"].get(\"P\", [])\n",
    "    I = js[\"pico_tokens\"].get(\"I\", [])\n",
    "\n",
    "    # Population must carry the target surgery/disease tokens\n",
    "    must_p = []\n",
    "    if \"pectus\" in nq and not any(\"pectus\" in t for t in P): must_p.append(\"pectus excavatum\")\n",
    "    if \"nuss\" in nq and not any(\"nuss\" in t for t in P):   must_p.append(\"nuss\")\n",
    "    if \"mirpe\" in nq and not any(\"mirpe\" in t for t in P): must_p.append(\"mirpe\")\n",
    "    P = _dedup(P + must_p)\n",
    "\n",
    "    # Intervention must carry cryo terms and intercostal nerve\n",
    "    must_i = []\n",
    "    if not any(\"cryo\" in t for t in I):\n",
    "        must_i += [\"cryoablation\", \"cryoanalgesia\"]\n",
    "    if \"intercostal\" in nq and not any(\"intercostal\" in t for t in I):\n",
    "        must_i.append(\"intercostal nerve\")\n",
    "    I = _dedup(I + must_i)\n",
    "\n",
    "    # Outcomes: normalize common phrasing\n",
    "    O = js[\"pico_tokens\"].get(\"O\", [])\n",
    "    O = [\"opioid consumption\" if t.lower()==\"opioid\" else t for t in O]\n",
    "    js[\"pico_tokens\"][\"P\"] = P\n",
    "    js[\"pico_tokens\"][\"I\"] = I\n",
    "    js[\"pico_tokens\"][\"O\"] = _dedup(O)\n",
    "\n",
    "    # 3) Anchors: derive from P/I essentials; DO NOT trust LLM-provided anchors\n",
    "    anchors = []\n",
    "    for t in P + I:\n",
    "        if any(k in t for k in [\"pectus\",\"nuss\",\"mirpe\",\"cryo\"]):\n",
    "            anchors.append(t)\n",
    "    js[\"anchors_must_have\"] = _dedup(anchors)[:2]  # at most 2\n",
    "    return js\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44fb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== sniff_v4_cell2_pubmed.py =====\n",
    "# PubMed E-utilities: paginated ESearch, batched EFetch, robust XML parse.\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "from typing import List, Tuple\n",
    "\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "\n",
    "def esearch_all_ids(term: str, mindate: Optional[int], page_size: int = PAGE_SIZE_ES) -> Tuple[int, List[str], Dict[str, Any]]:\n",
    "    # usehistory=y to page; pull up to 5000 for sniff\n",
    "    p = {\"db\":\"pubmed\",\"retmode\":\"json\",\"term\":term,\"retmax\":0,\"usehistory\":\"y\",\"email\":ENTREZ_EMAIL}\n",
    "    if ENTREZ_API_KEY: p[\"api_key\"]=ENTREZ_API_KEY\n",
    "    if mindate: p[\"mindate\"]=str(mindate)\n",
    "    r = requests.get(EUTILS+\"/esearch.fcgi\", params=p, timeout=HTTP_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    js = r.json().get(\"esearchresult\", {})\n",
    "    count = int(js.get(\"count\",\"0\"))\n",
    "    webenv = js.get(\"webenv\"); qk = js.get(\"querykey\")\n",
    "    ids=[]\n",
    "    if count>0 and webenv and qk:\n",
    "        max_pull = min(count, 5000)\n",
    "        for start in range(0, max_pull, page_size):\n",
    "            r2 = requests.get(EUTILS+\"/esearch.fcgi\", params={\n",
    "                \"db\":\"pubmed\",\"retmode\":\"json\",\"retmax\":page_size,\"retstart\":start,\n",
    "                \"WebEnv\":webenv,\"query_key\":qk,\"email\":ENTREZ_EMAIL,\n",
    "                **({\"api_key\":ENTREZ_API_KEY} if ENTREZ_API_KEY else {})\n",
    "            }, timeout=HTTP_TIMEOUT)\n",
    "            r2.raise_for_status()\n",
    "            ids.extend(r2.json().get(\"esearchresult\",{}).get(\"idlist\",[]))\n",
    "            time.sleep(0.34)  # polite throttle\n",
    "    return count, [str(x) for x in ids], {\"count\":count}\n",
    "\n",
    "def efetch_xml_batched(pmids: List[str], batch_size: int = EFETCH_BATCH_SIZE) -> str:\n",
    "    if not pmids: return \"\"\n",
    "    xmls=[]\n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        chunk = pmids[i:i+batch_size]\n",
    "        params = {\"db\":\"pubmed\",\"retmode\":\"xml\",\"rettype\":\"abstract\",\"id\":\",\".join(chunk),\"email\":ENTREZ_EMAIL}\n",
    "        if ENTREZ_API_KEY: params[\"api_key\"]=ENTREZ_API_KEY\n",
    "        r = requests.get(EUTILS+\"/efetch.fcgi\", params=params, timeout=HTTP_TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        xmls.append(r.text)\n",
    "        time.sleep(0.34)\n",
    "    return \"\\n\".join(xmls)\n",
    "\n",
    "def parse_pubmed_xml(xml_text: str) -> List[PubMedRecord]:\n",
    "    out=[]\n",
    "    if not xml_text.strip(): return out\n",
    "    root = ET.fromstring(xml_text)\n",
    "\n",
    "    def _join(node):\n",
    "        if node is None: return \"\"\n",
    "        try: return \"\".join(node.itertext())\n",
    "        except Exception: return node.text or \"\"\n",
    "\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = art.findtext(\".//PMID\") or \"\"\n",
    "        title = _join(art.find(\".//ArticleTitle\")).strip()\n",
    "        abs_nodes = art.findall(\".//Abstract/AbstractText\")\n",
    "        abstract = \" \".join(_join(n).strip() for n in abs_nodes) if abs_nodes else \"\"\n",
    "        # year: try multiple fields\n",
    "        year = None\n",
    "        for path in (\".//ArticleDate/Year\",\".//PubDate/Year\",\".//DateCreated/Year\",\".//PubDate/MedlineDate\"):\n",
    "            s = art.findtext(path)\n",
    "            if s:\n",
    "                m = re.search(r\"\\d{4}\", s)\n",
    "                if m: year = int(m.group(0)); break\n",
    "        lang = art.findtext(\".//Language\") or None\n",
    "        lang = normalize_lang(lang)\n",
    "        pubtypes = [pt.text for pt in art.findall(\".//PublicationTypeList/PublicationType\") if pt.text]\n",
    "        mesh = [mh.findtext(\"./DescriptorName\") for mh in art.findall(\".//MeshHeadingList/MeshHeading\") if mh.findtext(\"./DescriptorName\")]\n",
    "        out.append(PubMedRecord(pmid=pmid, title=title, abstract=abstract, year=year, language=lang, pubtypes=pubtypes, mesh=mesh))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d840d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== sniff_v4_cell3_universe_prefilter.py =====\n",
    "# Multi-query candidate generation (+ optional synonym LLM), budget allocation, stratified sampling,\n",
    "# deterministic prefilter with language normalization, and a relax ladder.\n",
    "\n",
    "import hashlib\n",
    "from typing import Set\n",
    "\n",
    "def _norm_terms(terms: List[str]) -> List[str]:\n",
    "    return [t.strip() for t in terms if t and t.strip()]\n",
    "\n",
    "def build_candidates(proto: Protocol, synonyms: Dict[str, List[str]], k_max: int = 8) -> List[QueryCandidate]:\n",
    "    P = [t for t in proto.pico_tokens.get(\"P\", []) if t]\n",
    "    I = [t for t in proto.pico_tokens.get(\"I\", []) if t]\n",
    "    A = proto.anchors_must_have[:2] if proto.anchors_must_have else []\n",
    "\n",
    "    # Never allow standalone \"inc\" in query terms\n",
    "    def _sanitize(terms: List[str]) -> List[str]:\n",
    "        return [t for t in terms if t.lower() not in {\"inc\"}]\n",
    "\n",
    "    P = _sanitize(P); I = _sanitize(I); A = _sanitize(A)\n",
    "\n",
    "    def q_from(parts):\n",
    "        return \" AND \".join(x for x in parts if x)\n",
    "\n",
    "    cands = []\n",
    "\n",
    "    # Q0: baseline P ∧ I (NO anchors)\n",
    "    q0 = q_from([or_block(P,\"tiab\"), or_block(I,\"tiab\")])\n",
    "    cands.append((\"baseline\", {\"P\":P, \"I\":I, \"A\":[]}, q0, \"pi_only\"))\n",
    "\n",
    "    # Q1: PI + ≤2 anchors (only if anchors exist)\n",
    "    if A:\n",
    "        q1 = q_from([or_block(P,\"tiab\"), or_block(I,\"tiab\"), or_block(A,\"tiab\")])\n",
    "        cands.append((\"pi_anchors\", {\"P\":P,\"I\":I,\"A\":A}, q1, \"pi_plus_anchors\"))\n",
    "\n",
    "    # Q2: broad cryo/pectus emphasis (resilient to phrasing)\n",
    "    cryo = [t for t in I if \"cryo\" in t] or [\"cryoablation\",\"cryoanalgesia\"]\n",
    "    pec  = [t for t in P if any(k in t for k in [\"pectus\",\"nuss\",\"mirpe\"])] or [\"pectus excavatum\",\"nuss\",\"mirpe\"]\n",
    "    q2 = q_from([or_block(pec,\"tiab\"), or_block(cryo,\"tiab\")])\n",
    "    cands.append((\"broad_cryo_pectus\", {\"P\":pec,\"I\":cryo,\"A\":[]}, q2, \"broad_cryo_pectus\"))\n",
    "\n",
    "    # Q3–Q4: synonym mixes (safe subset)\n",
    "    def mix(root, alts, cap=2):\n",
    "        alts = [a for a in (alts or []) if a and a.lower()!=root.lower()]\n",
    "        return [root] + alts[:cap]\n",
    "    synP = {p: mix(p, synonyms.get(p, [])) for p in P}\n",
    "    synI = {i: mix(i, synonyms.get(i, [])) for i in I}\n",
    "    for _ in range(2):\n",
    "        Ps = [random.choice(v) for v in synP.values()] if synP else P\n",
    "        Is = [random.choice(v) for v in synI.values()] if synI else I\n",
    "        q = q_from([or_block(Ps,\"tiab\"), or_block(Is,\"tiab\")])\n",
    "        cands.append((\"synmix\", {\"P\":Ps,\"I\":Is,\"A\":[]}, q, \"syn_mix\"))\n",
    "\n",
    "    # Materialize & prune by token Jaccard (less aggressive)\n",
    "    def tokset(parts):\n",
    "        toks=set()\n",
    "        for arr in (parts[\"P\"] + parts[\"I\"] + parts.get(\"A\", [])):\n",
    "            toks.update(arr)\n",
    "        return set(map(str.lower, toks))\n",
    "\n",
    "    out=[]\n",
    "    for origin, parts, query, tag in cands:\n",
    "        cid = hashlib.md5((origin+query).encode()).hexdigest()[:8]\n",
    "        out.append(QueryCandidate(id=cid, query=query, origin=origin, terms_used=parts, expected_breadth=tag))\n",
    "\n",
    "    keep=[]\n",
    "    for c in out:\n",
    "        if not keep:\n",
    "            keep.append(c); continue\n",
    "        sims=[]\n",
    "        for k in keep:\n",
    "            a, b = tokset(c.terms_used), tokset(k.terms_used)\n",
    "            j = len(a & b) / max(1, len(a | b))\n",
    "            sims.append(j)\n",
    "        # keep if not too similar to ALL previous\n",
    "        if (max(sims) if sims else 0.0) < 0.8:  # looser prune (keep more)\n",
    "            keep.append(c)\n",
    "\n",
    "    keep = keep[:k_max]\n",
    "    log_jsonl(\"query_candidates\", {\"count\": len(keep), \"candidates\": [asdict(c) for c in keep]})\n",
    "    print(f\"[S2] Built {len(keep)} query candidates.\")\n",
    "    return keep\n",
    "\n",
    "# Synonyms via LLM (one call). Keep conservative, ≤2 alternates per term.\n",
    "SYN_SYS = \"\"\"You propose brief, domain-appropriate synonyms/alternates for search tokens (≤ 3 words). Keep strictly on-topic.\n",
    "Return JSON: {\"P\": {\"token\":[alts...]}, \"I\": {\"token\":[alts...]}}. No notes.\"\"\"\n",
    "def syn_user(proto: Protocol) -> str:\n",
    "    return f\"\"\"Tokens to expand:\n",
    "P: {proto.pico_tokens.get(\"P\", [])}\n",
    "I: {proto.pico_tokens.get(\"I\", [])}\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"P\":{{}}, \"I\":{{}}}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "def validate_syn(js: Dict[str,Any]) -> Tuple[bool,str]:\n",
    "    if not isinstance(js, dict): return False, \"not object\"\n",
    "    for k in [\"P\",\"I\"]:\n",
    "        if k not in js or not isinstance(js[k], dict): return False, f\"missing {k} object\"\n",
    "        for root, alts in js[k].items():\n",
    "            if not isinstance(alts, list): return False, f\"{k}.{root} must be list\"\n",
    "            for a in alts:\n",
    "                if not isinstance(a, str): return False, f\"{k}.{root} contains non-string\"\n",
    "                if len(a.split()) > 4: return False, f\"alternate too long: {a}\"\n",
    "    return True, \"\"\n",
    "\n",
    "def _syn_ok(orig: str, alt: str) -> bool:\n",
    "    o = norm_txt(orig); a = norm_txt(alt)\n",
    "    if not a or a == o: \n",
    "        return False\n",
    "    # block dangerous \"inc\"/\"incision\"/company expansions\n",
    "    if re.search(r\"\\binc(ision|orporat|orporated)?\\b\", a):\n",
    "        return False\n",
    "    # require topical overlap\n",
    "    if any(s in o for s in [\"cryo\", \"cryoablation\", \"cryoanalgesia\", \"intercostal\"]) and \\\n",
    "       any(s in a for s in [\"cryo\", \"cryoablation\", \"cryoanalgesia\", \"intercostal\"]):\n",
    "        return True\n",
    "    if any(s in o for s in [\"pectus\", \"nuss\", \"mirpe\", \"chest wall\"]) and \\\n",
    "       any(s in a for s in [\"pectus\", \"nuss\", \"mirpe\", \"chest wall\"]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _filter_synonyms(syn: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    out={}\n",
    "    for k, alts in (syn or {}).items():\n",
    "        keep=[a for a in alts if _syn_ok(k, a)]\n",
    "        if keep:\n",
    "            out[k]=_dedup(keep)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_synonyms_via_llm_once(proto: Protocol) -> Dict[str, List[str]]:\n",
    "    try:\n",
    "        js = get_validated_json(QWEN_MODEL, SYN_SYS, syn_user(proto), validate_syn, retries=1)\n",
    "        synP = _filter_synonyms(js.get(\"P\",{}))\n",
    "        synI = _filter_synonyms(js.get(\"I\",{}))\n",
    "        all_syn = {}\n",
    "        for k,v in synP.items(): all_syn[k]=v\n",
    "        for k,v in synI.items(): all_syn[k]=list(dict.fromkeys(all_syn.get(k,[])+v))\n",
    "        log_jsonl(\"synonyms_mined\", {\"synonyms\": all_syn})\n",
    "        return all_syn\n",
    "    except Exception as e:\n",
    "        log_jsonl(\"synonyms_failed\", {\"error\": str(e)})\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Deterministic prefilter\n",
    "def passes_prefilter(rec: PubMedRecord, proto: Protocol) -> Tuple[bool, str]:\n",
    "    ymin = int(proto.deterministic_filters[\"year_min\"])\n",
    "    langs = [x.lower() for x in proto.deterministic_filters[\"languages\"]]\n",
    "    if rec.year and rec.year < ymin: \n",
    "        return False, f\"year<{ymin}\"\n",
    "    if rec.language and rec.language.lower() not in langs: \n",
    "        return False, f\"lang={rec.language}\"\n",
    "\n",
    "    rpts = set(rec.pubtypes or [])\n",
    "\n",
    "    # Build hard excludes: KB base + (optionally) reviews + user blocklist\n",
    "    hard = set(KB[\"publication_types_hard_exclude_base\"])\n",
    "    review_flavors = set(KB[\"review_flavors\"])\n",
    "    # If user explicitly allowlists any review flavor, DO NOT hard-exclude them\n",
    "    user_allow = set(proto.publication_types_allowlist or [])\n",
    "    if review_flavors & user_allow:\n",
    "        pass\n",
    "    else:\n",
    "        if not INCLUDE_REVIEW_FLAVORS:\n",
    "            hard |= review_flavors\n",
    "    # Add user blocklist\n",
    "    hard |= set(proto.publication_types_blocklist or [])\n",
    "    if rpts & hard:\n",
    "        return False, f\"pubtype_hard_exclude={list(rpts & hard)}\"\n",
    "\n",
    "    # Build allow-space:\n",
    "    # 1) designs_allowed (if provided) else KB primary+secondary\n",
    "    allowed_designs = proto.designs_allowed or (KB[\"publication_types_allowable_primary\"] + KB[\"publication_types_allowable_secondary\"])\n",
    "    alias_allow = set()\n",
    "    for d in allowed_designs:\n",
    "        alias_allow |= set(KB[\"pubtype_aliases\"].get(d, [d]))\n",
    "    # 2) user publication_types_allowlist (exact names)\n",
    "    alias_allow |= set(user_allow)\n",
    "\n",
    "    # If record matches any allowed alias/name, allow\n",
    "    if rpts & alias_allow:\n",
    "        return True, \"allow_design_or_allowlist\"\n",
    "\n",
    "    # Neutral/unknown types → keep (sniff shouldn’t over-prune)\n",
    "    return True, \"neutral\"\n",
    "\n",
    "\n",
    "def apply_prefilter(records: List[PubMedRecord], proto: Protocol) -> Tuple[List[PubMedRecord], Dict[str,int]]:\n",
    "    kept=[]; drops=Counter()\n",
    "    for r in records:\n",
    "        ok, why = passes_prefilter(r, proto)\n",
    "        if ok: kept.append(r)\n",
    "        else: drops[why]+=1\n",
    "    stats=dict(drops)\n",
    "    log_jsonl(\"prefilter_result\", {\"kept\": len(kept), \"total\": len(records), \"drops\": stats})\n",
    "    # quick distributions\n",
    "    years=[r.year for r in kept if r.year]\n",
    "    if years:\n",
    "        print(f\"   [Prefilter] Kept={len(kept)}/{len(records)} | year min/median/max = {min(years)}/{sorted(years)[len(years)//2]}/{max(years)}\")\n",
    "    else:\n",
    "        print(f\"   [Prefilter] Kept={len(kept)}/{len(records)}\")\n",
    "    return kept, stats\n",
    "\n",
    "# Relax ladder (after prefilter)\n",
    "from dataclasses import replace as _dc_replace\n",
    "\n",
    "def relax_ladder_fetch(proto: Protocol, base_records: List[PubMedRecord]) -> List[PubMedRecord]:\n",
    "    # Work on copies; never mutate the locked proto\n",
    "    steps = []\n",
    "    y0 = int(proto.deterministic_filters[\"year_min\"])\n",
    "    steps.append((\"widen_year_5\",  _dc_replace(proto, deterministic_filters={**proto.deterministic_filters, \"year_min\": max(1990, y0-5)})))\n",
    "    steps.append((\"widen_year_10\", _dc_replace(proto, deterministic_filters={**proto.deterministic_filters, \"year_min\": max(1990, y0-10)})))\n",
    "    steps.append((\"all_langs\",     _dc_replace(proto, deterministic_filters={**proto.deterministic_filters, \"languages\": KB[\"languages_human\"]})))\n",
    "    steps.append((\"allow_reviews\", None))  # toggle global flag locally\n",
    "\n",
    "    kept = []\n",
    "    for name, p2 in steps:\n",
    "        print(f\"   [Relax] Attempt: {name}\")\n",
    "        if name == \"allow_reviews\":\n",
    "            global INCLUDE_REVIEW_FLAVORS\n",
    "            old = INCLUDE_REVIEW_FLAVORS\n",
    "            try:\n",
    "                INCLUDE_REVIEW_FLAVORS = True\n",
    "                k, _ = apply_prefilter(base_records, proto)  # use original proto but with reviews allowed\n",
    "            finally:\n",
    "                INCLUDE_REVIEW_FLAVORS = old\n",
    "        else:\n",
    "            k, _ = apply_prefilter(base_records, p2)\n",
    "        kept = k\n",
    "        if len(kept) >= UNIVERSE_TARGET_MIN:\n",
    "            print(f\"   [Relax] Success: kept={len(kept)}\")\n",
    "            return kept\n",
    "    print(\"   [Relax] Ladder exhausted.\")\n",
    "    return kept\n",
    "\n",
    "\n",
    "# Build universe: multi-query → allocate budget → stratified sample → efetch → prefilter (+ relax if needed)\n",
    "def build_universe_multiquery(proto: Protocol) -> Tuple[str, List[PubMedRecord], Dict[str, Any]]:\n",
    "    print(\"[S2] Universe definition with multi-query & budget…\")\n",
    "    synonyms = get_synonyms_via_llm_once(proto)\n",
    "    candidates = build_candidates(proto, synonyms, k_max=8)\n",
    "\n",
    "    per_query_counts=[]\n",
    "    all_ids=[]\n",
    "    for c in candidates:\n",
    "        count, ids_full, meta = esearch_all_ids(c.query, proto.deterministic_filters[\"year_min\"], page_size=PAGE_SIZE_ES)\n",
    "        per_query_counts.append({\"cid\": c.id, \"origin\": c.origin, \"count\": count})\n",
    "        # store only meta; sampling is deferred until after allocation\n",
    "        all_ids.append({\"cid\": c.id, \"ids_full\": ids_full, \"count\": count})\n",
    "        print(f\"   [Candidate {c.id}:{c.origin}] hits={count}\")\n",
    "        log_jsonl(\"candidate_hits\", {\"cid\": c.id, \"origin\": c.origin, \"count\": count})\n",
    "\n",
    "    # Allocate FETCH_BUDGET_IDS proportionally to log(1+count); floor 100 per nonzero; cap by available.\n",
    "    weights = []\n",
    "    for pc in per_query_counts:\n",
    "        w = math.log1p(pc[\"count\"]) if pc[\"count\"]>0 else 0.0\n",
    "        weights.append(w)\n",
    "    total_w = sum(weights) or 1.0\n",
    "    alloc = {}\n",
    "    for pc, w in zip(per_query_counts, weights):\n",
    "        if pc[\"count\"]<=0: \n",
    "            alloc[pc[\"cid\"]] = 0\n",
    "        else:\n",
    "            alloc[pc[\"cid\"]] = max(100, int(FETCH_BUDGET_IDS * (w/total_w)))\n",
    "\n",
    "    # Stratified sampling across each id list (first/middle/last via stride)\n",
    "    sampled_pmids=[]\n",
    "    for pack in all_ids:\n",
    "        cid = pack[\"cid\"]; ids_full = pack[\"ids_full\"]; n = alloc.get(cid, 0)\n",
    "        if not ids_full or n<=0: continue\n",
    "        step = max(1, len(ids_full)//n)\n",
    "        sampled_pmids.extend(ids_full[::step][:n])\n",
    "\n",
    "    # Dedup\n",
    "    pmids = list(dict.fromkeys(sampled_pmids))\n",
    "    print(f\"   [Sampling] Sampled dedup PMIDs = {len(pmids)} (budget {FETCH_BUDGET_IDS})\")\n",
    "    log_jsonl(\"sampling_summary\", {\"sampled_pmids\": len(pmids), \"alloc\": alloc, \"per_query_counts\": per_query_counts})\n",
    "\n",
    "    # Fetch and parse\n",
    "    xml = efetch_xml_batched(pmids, batch_size=EFETCH_BATCH_SIZE)\n",
    "    recs = parse_pubmed_xml(xml)\n",
    "    print(f\"   [Fetch] Parsed records = {len(recs)}\")\n",
    "\n",
    "    # Prefilter\n",
    "    kept, drop_stats = apply_prefilter(recs, proto)\n",
    "    if len(kept) < UNIVERSE_TARGET_MIN:\n",
    "        print(f\"   [Prefilter] Post-filter kept={len(kept)} < target {UNIVERSE_TARGET_MIN}\")\n",
    "        kept = relax_ladder_fetch(proto, recs)\n",
    "\n",
    "    # Baseline string (best “baseline” candidate’s query), for report reproducibility\n",
    "    baseline = next((c.query for c in candidates if c.origin==\"baseline\"), candidates[0].query if candidates else \"\")\n",
    "    return baseline, kept, {\"per_query_counts\": per_query_counts, \"drop_stats\": drop_stats}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80cdb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== sniff_v4_cell4_rerank_screen_plaus.py =====\n",
    "# Phrase-aware reranker (1–3 grams + literal hits fallback),\n",
    "# strict screener AVR, and senior plausibility PASS/FAIL.\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def rerank_records(records: List[PubMedRecord], proto: Protocol, weights: Dict[str,float]=WEIGHTS, alpha=0.3) -> List[PubMedRecord]:\n",
    "    texts=[norm_txt((r.title or \"\")+\" \"+(r.abstract or \"\")) for r in records]\n",
    "    use_sklearn = True\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vec = TfidfVectorizer(ngram_range=(1,3), max_features=100_000, lowercase=False)\n",
    "        X = vec.fit_transform(texts); vocab=vec.vocabulary_\n",
    "        def tfidf(term, i):\n",
    "            j = vocab.get(term.lower())\n",
    "            return float(X[i,j]) if j is not None else 0.0\n",
    "    except Exception:\n",
    "        use_sklearn = False\n",
    "        def tfidf(term, i):\n",
    "            # crude ngram TF proxy: count literal occurrences\n",
    "            return float(texts[i].count(term.lower()))\n",
    "\n",
    "    def literal_hits(term, i):  # phrase support\n",
    "        return texts[i].count(term.lower())\n",
    "\n",
    "    weighted_terms=[]\n",
    "    for k in [\"P\",\"I\",\"C\",\"O\"]:\n",
    "        for t in proto.pico_tokens.get(k, []):\n",
    "            weighted_terms.append((norm_txt(t), weights[k]))\n",
    "    for t in proto.anchors_must_have:\n",
    "        weighted_terms.append((norm_txt(t), weights[\"ANCHOR\"]))\n",
    "    for t in proto.avoid_terms:\n",
    "        weighted_terms.append((norm_txt(t), weights[\"AVOID\"]))\n",
    "\n",
    "    scored=[]\n",
    "    for i, r in enumerate(records):\n",
    "        s=0.0\n",
    "        for term,w in weighted_terms:\n",
    "            s += w*(tfidf(term,i) + alpha*literal_hits(term,i))\n",
    "        scored.append((s, r))\n",
    "    scored.sort(key=lambda x:x[0], reverse=True)\n",
    "    ordered=[r for s,r in scored]\n",
    "    log_jsonl(\"rerank_done\", {\"records\": len(records), \"sklearn\": use_sklearn})\n",
    "    print(f\"[S2.5] Rerank complete. Sending top {min(SCREEN_TOP_K,len(ordered))} to screener.\")\n",
    "    return ordered\n",
    "\n",
    "# Screener AVR\n",
    "SCREEN_SYS = \"\"\"You are a strict but realistic title+abstract screener. Decision rules:\n",
    "INCLUDE requires: P (population/context) true AND I (intervention) true AND (O outcomes OR D design) true.\n",
    "- P: matches the target clinical context (synonyms acceptable).\n",
    "- I: intercostal nerve cryoablation / cryoanalgesia used intraoperatively for the target surgery.\n",
    "- O: any acute postoperative analgesia outcomes acceptable (pain, opioid use, LOS, early complications). Do not nitpick day windows.\n",
    "- D: randomized/comparative preferred, strong cohorts acceptable.\n",
    "Return ONLY JSON with:\n",
    "{\"pmid\":\"...\",\"decision\":\"INCLUDE|BORDERLINE|EXCLUDE\",\"why\":\"...\",\"checklist\":{\"P\":bool,\"I\":bool,\"O\":bool,\"D\":bool},\"mesh_roles\":[{\"mesh\":\"...\",\"role\":\"P|I|C|O|G\"}]}\"\"\"\n",
    "\n",
    "def screen_user(proto: Protocol, rec: PubMedRecord) -> str:\n",
    "    return f\"\"\"Protocol (narrative):\n",
    "{proto.narrative_question}\n",
    "\n",
    "P: {proto.pico_tokens.get(\"P\", [])}\n",
    "I: {proto.pico_tokens.get(\"I\", [])}\n",
    "C: {proto.pico_tokens.get(\"C\", [])}\n",
    "O: {proto.pico_tokens.get(\"O\", [])}\n",
    "Anchors: {proto.anchors_must_have}\n",
    "Avoid: {proto.avoid_terms}\n",
    "Design preference: {proto.designs_preference}\n",
    "Inclusion criteria: {proto.inclusion_criteria}\n",
    "Exclusion criteria: {proto.exclusion_criteria}\n",
    "\n",
    "Record:\n",
    "PMID: {rec.pmid}\n",
    "Title: {rec.title}\n",
    "PubTypes: {rec.pubtypes}\n",
    "MeSH: {rec.mesh}\n",
    "Abstract:\n",
    "{rec.abstract}\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{rec.pmid}\",\"decision\":\"BORDERLINE\",\"why\":\"\",\"checklist\":{{\"P\":false,\"I\":false,\"O\":false,\"D\":false}},\"mesh_roles\":[]}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "def validate_screen(js: Dict[str,Any]) -> Tuple[bool,str]:\n",
    "    if js.get(\"decision\") not in [\"INCLUDE\",\"BORDERLINE\",\"EXCLUDE\"]:\n",
    "        return False, \"bad decision\"\n",
    "    ch = js.get(\"checklist\", {})\n",
    "    for k in [\"P\",\"I\",\"O\",\"D\"]:\n",
    "        if not isinstance(ch.get(k), bool): return False, f\"checklist.{k} must be bool\"\n",
    "    m = js.get(\"mesh_roles\", [])\n",
    "    if not isinstance(m, list): return False, \"mesh_roles must be list\"\n",
    "    for it in m:\n",
    "        if not isinstance(it, dict): return False, \"mesh_roles items must be dict\"\n",
    "        if \"mesh\" not in it or \"role\" not in it: return False, \"mesh_roles items need mesh & role\"\n",
    "    return True, \"\"\n",
    "\n",
    "def screen_top_k(reranked: List[PubMedRecord], proto: Protocol, top_k: int = SCREEN_TOP_K) -> Tuple[List[PubMedRecord], List[PubMedRecord]]:\n",
    "    cand = reranked[:top_k]\n",
    "    includes=[]; borderlines=[]; n_exc=0\n",
    "    for r in cand:\n",
    "        js = get_validated_json(SCREENER_MODEL, SCREEN_SYS, screen_user(proto, r), validate_screen, retries=2)\n",
    "        d = js.get(\"decision\"); why = js.get(\"why\",\"\"); chk=js.get(\"checklist\",{})\n",
    "        r_meta = {\"pmid\": r.pmid, \"decision\": d, \"why\": why, \"checklist\": chk}\n",
    "        if d in [\"INCLUDE\",\"BORDERLINE\"]:\n",
    "            # attach only roles that exist in record.mesh (guard hallucinations)\n",
    "            valid_mesh = set(r.mesh or [])\n",
    "            mesh_roles = [mr for mr in js.get(\"mesh_roles\",[]) if mr.get(\"mesh\") in valid_mesh]\n",
    "            r._mesh_roles = mesh_roles\n",
    "        log_jsonl(\"screen_decision\", r_meta)\n",
    "        if d==\"INCLUDE\": includes.append(r)\n",
    "        elif d==\"BORDERLINE\": borderlines.append(r)\n",
    "        else: n_exc+=1\n",
    "        # brief console\n",
    "        print(f\"  [Screen] PMID {r.pmid} -> {d}  chk={chk}  why={why[:100]}\")\n",
    "    print(f\"[S3] Tallies: INCLUDE={len(includes)} BORDERLINE={len(borderlines)} EXCLUDE={n_exc}\")\n",
    "    return includes, borderlines\n",
    "\n",
    "# Plausibility (senior guard)\n",
    "PLAUS_SYS = \"\"\"You validate that an already-INCLUDED record matches the core P+I of the protocol.\n",
    "Return JSON: {\"pmid\":\"...\",\"verdict\":\"PASS|FAIL\",\"why\":\"...\"}\"\"\"\n",
    "\n",
    "def plaus_user(proto: Protocol, rec: PubMedRecord) -> str:\n",
    "    core = f\"P core: {proto.pico_tokens.get('P', [])}; I core: {proto.pico_tokens.get('I', [])}; Anchors: {proto.anchors_must_have}\"\n",
    "    return f\"\"\"Protocol core:\n",
    "{core}\n",
    "\n",
    "Record:\n",
    "PMID: {rec.pmid}\n",
    "Title: {rec.title}\n",
    "Abstract:\n",
    "{rec.abstract}\n",
    "\n",
    "BEGIN_JSON\n",
    "{{\"pmid\":\"{rec.pmid}\",\"verdict\":\"PASS\",\"why\":\"\"}}\n",
    "END_JSON\"\"\"\n",
    "\n",
    "def validate_plaus(js: Dict[str,Any]) -> Tuple[bool,str]:\n",
    "    v = js.get(\"verdict\")\n",
    "    if v not in [\"PASS\",\"FAIL\"]: return False, \"verdict must be PASS|FAIL\"\n",
    "    if \"pmid\" not in js: return False, \"missing pmid\"\n",
    "    return True, \"\"\n",
    "\n",
    "def plausibility_filter(includes: List[PubMedRecord], proto: Protocol) -> List[PubMedRecord]:\n",
    "    confirmed=[]\n",
    "    for r in includes:\n",
    "        js = get_validated_json(QWEN_MODEL, PLAUS_SYS, plaus_user(proto, r), validate_plaus, retries=1)\n",
    "        if js.get(\"verdict\")==\"PASS\":\n",
    "            confirmed.append(r)\n",
    "        else:\n",
    "            print(f\"   [Plausibility] DROP {r.pmid} — {js.get('why','')}\")\n",
    "    print(f\"[S3.5] Confirmed={len(confirmed)} / Includes={len(includes)}\")\n",
    "    log_jsonl(\"plausibility_done\", {\"confirmed\": len(confirmed), \"includes\": len(includes)})\n",
    "    return confirmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a54ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S1] Protocol locked. P/I/C/O: {'P': ['children adolescents minimally invasive', 'of pectus excavatum (Nuss/MIRPE)', 'nuss', 'mirpe'], 'I': ['intercostal nerve cryoablation analgesia', 'during Nuss/MIRPE'], 'C': ['thoracic epidural', 'paravertebral block', 'intercostal nerve block', 'erector spinae plane block', 'systemic multimodal analgesia'], 'O': ['opioid consumption', 'in-hospital and at discharge', 'pain scores']}\n",
      "[S2] Universe definition with multi-query & budget…\n",
      "[S2] Built 1 query candidates.\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, 'Foi forçado o cancelamento de uma conexão existente pelo host remoto', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\util\\ssl_.py:480\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    478\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\util\\ssl_.py:524\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\ssl.py:1041\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1040\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\ssl.py:1319\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1318\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mConnectionResetError\u001b[39m: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mProtocolError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\util\\util.py:38\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\util\\ssl_.py:480\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    478\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\urllib3\\util\\ssl_.py:524\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\ssl.py:1041\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1040\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\ssl.py:1319\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1318\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mProtocolError\u001b[39m: ('Connection aborted.', ConnectionResetError(10054, 'Foi forçado o cancelamento de uma conexão existente pelo host remoto', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    103\u001b[39m EXAMPLE_NLQ = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[33mPopulation = children/adolescents undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[33mIntervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m \u001b[33mScreening notes: Be conservative; INCLUDE if P & I present and (O or D) is present; do not exclude for lack of exact day window if acute postop outcomes are clearly reported.\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[33m\"\"\"\u001b[39m.strip()\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# To execute:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43msniff_v4_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEXAMPLE_NLQ\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36msniff_v4_run\u001b[39m\u001b[34m(USER_NLQ)\u001b[39m\n\u001b[32m     73\u001b[39m proto = lock_protocol(USER_NLQ)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# S2 universe\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m baseline_query, kept_records, u_meta = \u001b[43mbuild_universe_multiquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kept_records:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFatal: no records after prefilter & relax ladder.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 248\u001b[39m, in \u001b[36mbuild_universe_multiquery\u001b[39m\u001b[34m(proto)\u001b[39m\n\u001b[32m    246\u001b[39m all_ids=[]\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     count, ids_full, meta = \u001b[43mesearch_all_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeterministic_filters\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myear_min\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPAGE_SIZE_ES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     per_query_counts.append({\u001b[33m\"\u001b[39m\u001b[33mcid\u001b[39m\u001b[33m\"\u001b[39m: c.id, \u001b[33m\"\u001b[39m\u001b[33morigin\u001b[39m\u001b[33m\"\u001b[39m: c.origin, \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m: count})\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# store only meta; sampling is deferred until after allocation\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mesearch_all_ids\u001b[39m\u001b[34m(term, mindate, page_size)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ENTREZ_API_KEY: p[\u001b[33m\"\u001b[39m\u001b[33mapi_key\u001b[39m\u001b[33m\"\u001b[39m]=ENTREZ_API_KEY\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mindate: p[\u001b[33m\"\u001b[39m\u001b[33mmindate\u001b[39m\u001b[33m\"\u001b[39m]=\u001b[38;5;28mstr\u001b[39m(mindate)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEUTILS\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/esearch.fcgi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHTTP_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m r.raise_for_status()\n\u001b[32m     16\u001b[39m js = r.json().get(\u001b[33m\"\u001b[39m\u001b[33mesearchresult\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Galaxy\\miniconda3\\envs\\litx\\Lib\\site-packages\\requests\\adapters.py:659\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    644\u001b[39m     resp = conn.urlopen(\n\u001b[32m    645\u001b[39m         method=request.method,\n\u001b[32m    646\u001b[39m         url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    655\u001b[39m         chunked=chunked,\n\u001b[32m    656\u001b[39m     )\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ConnectTimeoutError):\n\u001b[32m    663\u001b[39m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: ('Connection aborted.', ConnectionResetError(10054, 'Foi forçado o cancelamento de uma conexão existente pelo host remoto', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "# ===== sniff_v4_cell5_orchestrate.py =====\n",
    "# MeSH vernacular mining, strategy validation with recall check, final artifacts,\n",
    "# and a single orchestrator to run the full Sniff v4 pipeline.\n",
    "\n",
    "def vernacular_from_includes(includes: List[PubMedRecord]) -> Dict[str, List[str]]:\n",
    "    roles = {\"P\":set(),\"I\":set(),\"C\":set(),\"O\":set(),\"G\":set()}\n",
    "    for r in includes:\n",
    "        for mr in getattr(r, \"_mesh_roles\", []):\n",
    "            m = mr.get(\"mesh\"); role = mr.get(\"role\",\"G\")\n",
    "            if m and role in roles: roles[role].add(m)\n",
    "    return {k: sorted(v) for k,v in roles.items()}\n",
    "\n",
    "def validate_strategy(universe_baseline_query: str, confirmed: List[PubMedRecord], proto: Protocol, vernac: Dict[str,List[str]]) -> Dict[str,str]:\n",
    "    topic_tokens = list(dict.fromkeys((vernac.get(\"P\",[]) + vernac.get(\"I\",[]) + vernac.get(\"O\",[]))))\n",
    "    topic_filter = or_block(topic_tokens, \"tiab\") if topic_tokens else \"\"\n",
    "    # design filter from preference\n",
    "    pref = proto.designs_preference\n",
    "    aliases = KB[\"pubtype_aliases\"].get(pref, [pref])\n",
    "    design_filter = \" OR \".join(f'\"{a}\"[Publication Type]' for a in aliases)\n",
    "\n",
    "    # Recall proxy: ensure each confirmed appears to match topic_filter lexically\n",
    "    recall_ok = True\n",
    "    if topic_filter:\n",
    "        toks = re.findall(r'\"([^\"]+)\"\\[tiab\\]|(\\w+)\\[tiab\\]', topic_filter)\n",
    "        flat = [a or b for a,b in toks if (a or b)]\n",
    "        for r in confirmed:\n",
    "            low = norm_txt((r.title or \"\")+\" \"+(r.abstract or \"\"))\n",
    "            if not any(norm_txt(t) in low for t in flat):\n",
    "                recall_ok = False\n",
    "                print(f\"   [S4] Recall risk: topic_filter might drop PMID {r.pmid}\")\n",
    "    if not recall_ok:\n",
    "        topic_filter = \"\"  # relax to baseline\n",
    "\n",
    "    print(\"[S4] Recommended filters:\")\n",
    "    print(\"   topic_filter:\", topic_filter or \"<none>\")\n",
    "    print(\"   design_filter:\", design_filter)\n",
    "    return {\"topic_filter\": topic_filter, \"design_filter\": design_filter}\n",
    "\n",
    "def write_artifacts(proto: Protocol, universe_query: str, recs: List[PubMedRecord],\n",
    "                    includes: List[PubMedRecord], confirmed: List[PubMedRecord],\n",
    "                    vernac: Dict[str,List[str]], recommended: Dict[str,str], warn: List[str]) -> None:\n",
    "    artifacts = {\n",
    "        \"locked_protocol\": asdict(proto),\n",
    "        \"universe_baseline_query\": universe_query,\n",
    "        \"recommended_filters\": recommended,\n",
    "        \"ground_truth_pmids\": [r.pmid for r in confirmed],\n",
    "        \"includes_pmids\": [r.pmid for r in includes],\n",
    "        \"mesh_vernacular\": vernac,\n",
    "        \"warnings\": warn\n",
    "    }\n",
    "    (OUT_DIR/\"sniff_artifacts.json\").write_text(json.dumps(artifacts, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # Text report\n",
    "    lines=[]\n",
    "    lines.append(\"# SNIFF VALIDATION ENGINE REPORT (v4)\\n\")\n",
    "    lines.append(\"## Protocol (narrative)\\n\" + textwrap.fill(proto.narrative_question, 100) + \"\\n\")\n",
    "    lines.append(\"**Deterministic filters**: languages=\" + \", \".join(proto.deterministic_filters[\"languages\"]) +\n",
    "                 f\" ; year_min={proto.deterministic_filters['year_min']}\\n\")\n",
    "    lines.append(\"**Universe baseline query**:\\n\\n```\\n\" + universe_query + \"\\n```\\n\")\n",
    "    lines.append(\"**Recommended filters**:\\n\\n- topic_filter: \" + (recommended[\"topic_filter\"] or \"<none>\") +\n",
    "                 \"\\n- design_filter: \" + recommended[\"design_filter\"] + \"\\n\")\n",
    "    lines.append(f\"**Ground truth (confirmed) includes** (n={len(artifacts['ground_truth_pmids'])}): \" + \", \".join(artifacts[\"ground_truth_pmids\"]) + \"\\n\")\n",
    "    lines.append(\"**MeSH vernacular (from confirmed)**:\\n\\n```\\n\" + json.dumps(vernac, indent=2, ensure_ascii=False) + \"\\n```\\n\")\n",
    "    if warn:\n",
    "        lines.append(\"**WARNINGS**\\n- \" + \"\\n- \".join(warn) + \"\\n\")\n",
    "    (OUT_DIR/\"sniff_report.md\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(\"  wrote:\", OUT_DIR/\"sniff_artifacts.json\", \"and\", OUT_DIR/\"sniff_report.md\")\n",
    "\n",
    "# Orchestrator\n",
    "def sniff_v4_run(USER_NLQ: str):\n",
    "    warnings=[]\n",
    "    # S1 protocol\n",
    "    proto = lock_protocol(USER_NLQ)\n",
    "    # S2 universe\n",
    "    baseline_query, kept_records, u_meta = build_universe_multiquery(proto)\n",
    "    if not kept_records:\n",
    "        raise RuntimeError(\"Fatal: no records after prefilter & relax ladder.\")\n",
    "    # S2.5 rerank\n",
    "    ordered = rerank_records(kept_records, proto, WEIGHTS)\n",
    "    # S3 screen (top-K)\n",
    "    includes, borderlines = screen_top_k(ordered, proto, SCREEN_TOP_K)\n",
    "    if not includes and borderlines:\n",
    "        # mine some MeSH from borderlines to keep vernac alive; tag as weak (not stored separately here)\n",
    "        print(\"   [S3] No includes; using borderlines for vernacular seeding.\")\n",
    "        includes = borderlines[:3]  # small seed, clearly tagged by context (you can split if you prefer)\n",
    "    if not includes:\n",
    "        raise RuntimeError(\"Fatal: no includes after screening.\")\n",
    "    # S3.5 plausibility\n",
    "    confirmed = plausibility_filter(includes, proto)\n",
    "    if len(confirmed) < PLAUSIBILITY_MIN_INCLUDES:\n",
    "        warnings.append(f\"Insufficient confirmed includes after plausibility ({len(confirmed)}<{PLAUSIBILITY_MIN_INCLUDES}).\")\n",
    "    # S3.9 vernacular\n",
    "    vernac = vernacular_from_includes(confirmed if confirmed else includes)\n",
    "    # S4 strategy validation\n",
    "    recommended = validate_strategy(baseline_query, confirmed if confirmed else includes, proto, vernac)\n",
    "    # S5 finalize\n",
    "    write_artifacts(proto, baseline_query, kept_records, includes, confirmed if confirmed else includes, vernac, recommended, warnings)\n",
    "    print(\"[DONE] Sniff v4 complete.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Example run (your pediatric INC/MIRPE NLQ)\n",
    "# ----------------------------\n",
    "EXAMPLE_NLQ = \"\"\"\n",
    "Population = children/adolescents undergoing minimally invasive repair of pectus excavatum (Nuss/MIRPE).\n",
    "Intervention = intercostal nerve cryoablation (INC) used intraoperatively for analgesia during Nuss/MIRPE.\n",
    "Comparators = thoracic epidural, paravertebral block, intercostal nerve block, erector spinae plane block, or systemic multimodal analgesia.\n",
    "Outcomes = postoperative opioid consumption (in-hospital and at discharge) and pain scores (abstract-level timing not strictly required).\n",
    "Study designs = RCTs preferred; if absent, include comparative cohorts/case-control/observational.\n",
    "Year_min = 2010.\n",
    "Languages = English, Portuguese, Spanish.\n",
    "Screening notes: Be conservative; INCLUDE if P & I present and (O or D) is present; do not exclude for lack of exact day window if acute postop outcomes are clearly reported.\n",
    "\"\"\".strip()\n",
    "\n",
    "# To execute:\n",
    "sniff_v4_run(EXAMPLE_NLQ)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
