Project structure for '/c/Users/Galaxy/LEVI/projects/Python/qucik_sr/src':
===============================================================================
  __init__.py
  __pycache__/__init__.cpython-312.pyc
  cli/__init__.py
  cli/__pycache__/__init__.cpython-312.pyc
  cli/__pycache__/main.cpython-312.pyc
  cli/main.py
  config/__init__.py
  config/__pycache__/__init__.cpython-312.pyc
  config/__pycache__/defaults.cpython-312.pyc
  config/__pycache__/schema.cpython-312.pyc
  config/defaults.py
  config/schema.py
  graph/__init__.py
  graph/__pycache__/__init__.cpython-312.pyc
  graph/__pycache__/cile.cpython-312.pyc
  graph/cile.py
  io/__init__.py
  io/__pycache__/__init__.cpython-312.pyc
  io/__pycache__/docdb.cpython-312.pyc
  io/__pycache__/store.cpython-312.pyc
  io/__pycache__/vecdb.cpython-312.pyc
  io/docdb.py
  io/store.py
  io/vecdb.py
  net/__init__.py
  net/__pycache__/__init__.cpython-312.pyc
  net/__pycache__/entrez.cpython-312.pyc
  net/__pycache__/icite.cpython-312.pyc
  net/entrez.py
  net/icite.py
  prisma/__init__.py
  prisma/__pycache__/__init__.cpython-312.pyc
  prisma/__pycache__/counts.cpython-312.pyc
  prisma/counts.py
  screen/__init__.py
  screen/__pycache__/__init__.cpython-312.pyc
  screen/__pycache__/features.cpython-312.pyc
  screen/__pycache__/gates.cpython-312.pyc
  screen/__pycache__/llm_decide.cpython-312.pyc
  screen/__pycache__/regressor.cpython-312.pyc
  screen/features.py
  screen/gates.py
  screen/llm_decide.py
  screen/regressor.py
  text/__init__.py
  text/__pycache__/__init__.cpython-312.pyc
  text/__pycache__/embed.cpython-312.pyc
  text/__pycache__/llm.cpython-312.pyc
  text/__pycache__/prompts.cpython-312.pyc
  text/embed.py
  text/llm.py
  text/prompts.py



###############################################################################
### FILE: cli/main.py
###############################################################################
def _fallback_boolean_queries(level: str = "strict") -> Dict[str, str]:
    """
    Deterministic query packs for pectus excavatum (Nuss/MIRPE) + intercostal cryo*.
    `level` controls how aggressive we filter inside PubMed; we prefer recall, then
    let downstream gates prune by design/year/language.

    Levels:
      - "strict": TA-constrained, includes intercostal + cryo synonyms, no design filters.
      - "medium": relax TA constraints on the cryo block; keep pectus TA.
      - "loose": drop human/age, keep pectus+cryo core.

    Year filter is applied in esearch via mindate (not embedded in the string).
    """
    # Pectus / Nuss / MIRPE synonyms
    pectus = '(' + ' OR '.join([
        '"pectus excavatum"[Title/Abstract]',
        '"funnel chest"[Title/Abstract]',
        '"Nuss procedure"[Title/Abstract]',
        'Nuss[Title/Abstract]',
        '"minimally invasive repair"[Title/Abstract]',
        'MIRPE[Title/Abstract]'
    ]) + ')'

    # Intercostal + cryo synonyms
    intercostal = '(' + ' OR '.join([
        '"intercostal nerve"[Title/Abstract]',
        '"intercostal nerves"[Title/Abstract]'
    ]) + ')'

    cryo = '(' + ' OR '.join([
        'cryoablat*[Title/Abstract]',
        'cryoanalges*[Title/Abstract]',
        'cryoneurolys*[Title/Abstract]',
        '"cryo-analges*"[Title/Abstract]',
        '"cryo-neurolys*"[Title/Abstract]'
    ]) + ')'

    # Humans/age hints (not hard filters if we need recall)
    humans_age = '(' + ' OR '.join([
        'Humans[MeSH Terms]',
        'adolescent[MeSH Terms]',
        'adult[MeSH Terms]',
        'adolescent[Title/Abstract]',
        'adult[Title/Abstract]'
    ]) + ')'

    # Packs
    if level == "strict":
        return {
            # title/abstract on both sides; this is usually enough and precise
            "ta_strict": f'({pectus}) AND ({intercostal}) AND ({cryo}) AND {humans_age}',
            # variant that allows “cryo*” without explicit intercostal mention
            "ta_strict_no_intercostal": f'({pectus}) AND (cryo*[Title/Abstract]) AND {humans_age}',
        }

    if level == "medium":
        return {
            # allow intercostal in any field; keep pectus in TA
            "ta_medium": f'({pectus}) AND (("intercostal nerve"[All Fields] OR "intercostal nerves"[All Fields]) AND cryo*[All Fields])',
            # allow cryo synonyms anywhere + pectus TA
            "ta_medium_cryo_wide": f'({pectus}) AND (cryoablat*[All Fields] OR cryoanalges*[All Fields] OR cryoneurolys*[All Fields])',
        }

    # "loose"
    return {
        # keep core: pectus + cryo*; drop humans/age completely
        "ta_loose_core": f'({pectus}) AND (cryo*[All Fields])',
        # broad “Nuss AND cryo” anywhere
        "nuss_cryo_any": '(Nuss[All Fields] AND cryo*[All Fields])',
    }



###############################################################################
### FILE: config/defaults.py
###############################################################################
from __future__ import annotations
import os

# ---- External services ----
LMSTUDIO_BASE    = os.getenv("LMSTUDIO_BASE", "http://127.0.0.1:1234")
LMSTUDIO_EMB     = os.getenv("LMSTUDIO_EMB_MODEL", "text-embedding-qwen3-embedding-0.6b")
LMSTUDIO_CHAT    = os.getenv("LMSTUDIO_CHAT_MODEL", "gemma-3n-e2b-it")

ENTREZ_EMAIL     = os.getenv("ENTREZ_EMAIL", "you@example.com")
ENTREZ_API_KEY   = os.getenv("ENTREZ_API_KEY", "")
HTTP_TIMEOUT     = int(os.getenv("HTTP_TIMEOUT", "30"))
USER_AGENT       = os.getenv("USER_AGENT", "sr-screener/0.1 (+local)")

ICITE_BASE       = os.getenv("ICITE_BASE", "https://icite.od.nih.gov/api/pubs")

# ---- Retrieval ----
ESEARCH_RETMAX_PER_QUERY = int(os.getenv("ESEARCH_RETMAX_PER_QUERY", "2000"))

# ---- FS (define early; used by VEC_DB_PATH) ----
DATA_DIR         = os.getenv("DATA_DIR", "data")
RUNS_DIR         = os.getenv("RUNS_DIR", "runs")

# ---- Embeddings ----
EMB_BATCH        = int(os.getenv("EMB_BATCH", "48"))

# ---- Embedding controls / cache ----
EMB_AUTO_BATCH        = os.getenv("EMB_AUTO_BATCH", "1") == "1"
EMB_MAX_CHARS_PER_DOC = int(os.getenv("EMB_MAX_CHARS_PER_DOC", "12000"))
EMB_RETRY_BACKOFF_S   = float(os.getenv("EMB_RETRY_BACKOFF_S", "1.2"))
EMB_RETRY_MAX         = int(os.getenv("EMB_RETRY_MAX", "4"))

# ---- Vector DB (sqlite) ----
VEC_DB_PATH      = os.getenv("VEC_DB_PATH", os.path.join(DATA_DIR, "cache", "vectors.sqlite3"))

# ---- Seeds & thresholds ----
SEED_SEM_TAU_HI  = float(os.getenv("SEED_SEM_TAU_HI", "0.92"))
SEED_MIN_COUNT   = int(os.getenv("SEED_MIN_COUNT", "8"))
SEED_RELAX_STEP  = float(os.getenv("SEED_RELAX_STEP", "0.02"))

# ---- CILE knobs ----
CILE_REL_GATE_FRAC      = float(os.getenv("CILE_REL_GATE_FRAC", "0.08"))
CILE_EXT_BUDGET         = int(os.getenv("CILE_EXT_BUDGET", "2500"))
CILE_MAX_ACCEPT         = int(os.getenv("CILE_MAX_ACCEPT", "600"))
CILE_HUB_QUARANTINE     = os.getenv("CILE_HUB_QUARANTINE", "1") == "1"
CILE_MIN_HUB_SOFT       = int(os.getenv("CILE_MIN_HUB_SOFT", "450"))

# ---- Regressor thresholds ----
REG_P_HI         = float(os.getenv("REG_P_HI", "0.85"))
REG_P_LO         = float(os.getenv("REG_P_LO", "0.15"))

# ---- LLM budget ----
LLM_BUDGET       = int(os.getenv("LLM_BUDGET", "150"))

# ---- Languages & year defaults ----
DEFAULT_LANGS    = os.getenv("DEFAULT_LANGS", "English,Portuguese,Spanish").split(",")
DEFAULT_YEAR_MIN = int(os.getenv("DEFAULT_YEAR_MIN", "2000"))

# ---- FS ----
DATA_DIR         = os.getenv("DATA_DIR", "data")
RUNS_DIR         = os.getenv("RUNS_DIR", "runs")



###############################################################################
### FILE: config/schema.py
###############################################################################
# src/config/schema.py
from __future__ import annotations
from typing import List, Dict, Optional, Literal, Any
from pydantic import BaseModel, Field

Reason = Literal[
    "design_mismatch","population_mismatch","intervention_mismatch",
    "language","year","insufficient_info","off_topic"
]

class PICOS(BaseModel):
    population: str
    intervention: str
    comparison: Optional[str] = None
    outcomes: List[str]
    study_design: List[str]
    year_min: Optional[int] = None
    languages: List[str] = ["English"]

class Criteria(BaseModel):
    picos: PICOS
    # <-- be permissive; the LLM may return lists/ints/bools etc.
    inclusion_criteria: Dict[str, Any] = Field(default_factory=dict)
    exclusion_criteria: Dict[str, Any] = Field(default_factory=dict)
    # be flexible here too
    reason_taxonomy: List[str] = Field(default_factory=list)
    boolean_queries: Dict[str, str] = Field(default_factory=dict)

class Document(BaseModel):
    pmid: str
    title: Optional[str] = ""
    abstract: Optional[str] = ""
    year: Optional[int] = None
    journal: Optional[str] = None
    language: Optional[str] = None
    pub_types: List[str] = []
    doi: Optional[str] = None

class Signals(BaseModel):
    sem_intent: float
    sem_seed: float
    graph_ppr_pct: float
    graph_links_frac: float
    year_scaled: float
    abstract_len_bin: Literal["none","short","normal","long"]

class DecisionLLM(BaseModel):
    pmid: str
    decision: Literal["include","exclude","borderline"]
    primary_reason: Reason
    confidence: float
    evidence: Dict[str, str]

class LedgerRow(BaseModel):
    pmid: str
    lane_before_llm: Literal["auto_exclude","auto_include","sent_to_llm","model_exclude","model_include","uncertain"]
    gate_reason: Optional[Reason] = None
    model_p: Optional[float] = None
    llm: Optional[DecisionLLM] = None
    final_decision: Literal["include","exclude","borderline"]
    final_reason: Reason
    signals: Signals
    pub_types: List[str]
    year: Optional[int]
    title: str
    abstract: str



###############################################################################
### FILE: graph/cile.py
###############################################################################
from __future__ import annotations
from typing import List, Dict, Set, Tuple
import random
from src.net.icite import icite_neighbors_map, icite_degree_total

def one_wave_expand(
    seeds_pos: List[int],
    H_existing: Set[int] | None,
    rel_gate_frac: float = 0.08,
    external_budget: int = 2500,
    max_accept: int = 600,
    hub_quarantine_external: bool = True,
    min_hub_soft: int = 450,
    rng_seed: int = 13
) -> Tuple[Set[int], Dict[str,int]]:
    random.seed(rng_seed)
    seeds_pos = [int(x) for x in seeds_pos]
    H_set = set(H_existing or set())
    neigh_map = icite_neighbors_map([str(s) for s in seeds_pos])
    neighborhood: Set[int] = set()
    for s in seeds_pos:
        neighborhood |= set(neigh_map.get(str(s), set()))
        neighborhood.add(s)
    candidates = list(neighborhood - H_set)
    accepted: List[Tuple[int,int]] = []
    total_ext = 0
    tmp = []
    for v in candidates:
        deg_tot = icite_degree_total(v)
        if deg_tot <= 0:
            continue
        vn = neigh_map.get(str(v))
        if vn is None:
            vn = icite_neighbors_map([str(v)]).get(str(v), set())
        rel = len(set(int(x) for x in vn) & set(seeds_pos)) / max(1, deg_tot)
        if rel < rel_gate_frac:
            continue
        ext = len(set(int(x) for x in vn) - H_set)
        if hub_quarantine_external and ext >= min_hub_soft:
            continue
        tmp.append((v, ext))
    tmp.sort(key=lambda t: (t[1], t[0]))
    for v, ext in tmp:
        if total_ext + ext > external_budget:
            continue
        accepted.append((v, ext))
        total_ext += ext
        if len(accepted) >= max_accept:
            break
    acc_nodes = set(v for v, _ in accepted)
    meta = {"H_prev": len(H_set), "neighborhood": len(neighborhood), "candidates": len(candidates),
            "accepted": len(acc_nodes), "sum_ext_after": total_ext}
    return (H_set | acc_nodes), meta



###############################################################################
### FILE: io/docdb.py
###############################################################################
from __future__ import annotations
import sqlite3, pathlib, json
from typing import Dict, List
from src.config.defaults import DATA_DIR

DB_PATH = pathlib.Path(DATA_DIR) / "cache" / "docs.sqlite3"
DB_PATH.parent.mkdir(parents=True, exist_ok=True)

class DocDB:
    def __init__(self, path: pathlib.Path = DB_PATH):
        self.conn = sqlite3.connect(str(path))
        self.conn.execute("""CREATE TABLE IF NOT EXISTS docs(
            pmid TEXT PRIMARY KEY,
            json TEXT NOT NULL
        )""")
        self.conn.commit()

    def get_many(self, pmids: List[str]) -> Dict[str, dict]:
        if not pmids: return {}
        q = ",".join("?"*len(pmids))
        cur = self.conn.execute(f"SELECT pmid,json FROM docs WHERE pmid IN ({q})", pmids)
        out = {}
        for pmid, blob in cur.fetchall():
            try: out[pmid] = json.loads(blob)
            except Exception: pass
        return out

    def put_many(self, rows: Dict[str, dict]) -> int:
        if not rows: return 0
        data = [(pmid, json.dumps(rec)) for pmid, rec in rows.items()]
        self.conn.executemany("INSERT OR REPLACE INTO docs(pmid,json) VALUES(?,?)", data)
        self.conn.commit()
        return len(data)



###############################################################################
### FILE: io/store.py
###############################################################################
from __future__ import annotations
import os, pathlib, json, csv
from typing import Any, List, Dict
from src.config.defaults import RUNS_DIR

def ensure_run_dir(out_dir: str) -> pathlib.Path:
    p = pathlib.Path(out_dir) if out_dir else pathlib.Path(RUNS_DIR) / "run"
    p.mkdir(parents=True, exist_ok=True)
    return p

def write_json(path: pathlib.Path, obj: Any):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def write_tsv(path: pathlib.Path, rows: List[Dict[str,Any]], fieldnames: List[str]):
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, delimiter="\t", fieldnames=fieldnames)
        w.writeheader()
        for r in rows:
            w.writerow(r)



###############################################################################
### FILE: io/vecdb.py
###############################################################################
from __future__ import annotations
import sqlite3, pathlib, hashlib
from typing import Dict, List, Tuple, Optional
import numpy as np
from src.config.defaults import VEC_DB_PATH

_PATH = pathlib.Path(VEC_DB_PATH)
_PATH.parent.mkdir(parents=True, exist_ok=True)

def _hash_text(model: str, text: str) -> str:
    h = hashlib.blake2b(digest_size=16)
    h.update(model.encode("utf-8", "ignore") + b"\x00" + text.encode("utf-8", "ignore"))
    return h.hexdigest()

class VecDB:
    def __init__(self, path: pathlib.Path = _PATH):
        self.conn = sqlite3.connect(str(path))
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS embeds(
          pmid TEXT NOT NULL,
          model TEXT NOT NULL,
          hash TEXT NOT NULL,
          dim  INTEGER NOT NULL,
          vec  BLOB NOT NULL,
          PRIMARY KEY (pmid, model)
        )""")
        self.conn.execute("CREATE INDEX IF NOT EXISTS ix_embeds_hash ON embeds(hash)")
        self.conn.commit()

    def get_many(self, keys: List[Tuple[str,str]]) -> Dict[Tuple[str,str], Tuple[str,int,bytes]]:
        if not keys: return {}
        q = ",".join(["(?,?)"]*len(keys))
        flat = []
        for pmid, model in keys: flat.extend([pmid, model])
        cur = self.conn.execute(f"SELECT pmid,model,hash,dim,vec FROM embeds WHERE (pmid,model) IN ({q})", flat)
        out = {}
        for pmid, model, h, dim, blob in cur.fetchall():
            out[(pmid, model)] = (h, int(dim), blob)
        return out

    def upsert_many(self, rows: List[Tuple[str,str,str,int,bytes]]) -> int:
        if not rows: return 0
        self.conn.executemany("INSERT OR REPLACE INTO embeds(pmid,model,hash,dim,vec) VALUES(?,?,?,?,?)", rows)
        self.conn.commit()
        return len(rows)

    @staticmethod
    def make_hash(model: str, text: str) -> str:
        return _hash_text(model, text)

    @staticmethod
    def pack_vec(x: np.ndarray) -> bytes:
        assert x.dtype == np.float32
        return x.tobytes()

    @staticmethod
    def unpack_vec(blob: bytes, dim: int) -> np.ndarray:
        arr = np.frombuffer(blob, dtype=np.float32)
        if arr.size != dim:
            raise ValueError(f"vec size {arr.size} != dim {dim}")
        return arr



###############################################################################
### FILE: net/entrez.py
###############################################################################
from __future__ import annotations
import requests, re, time, math, concurrent.futures, xml.etree.ElementTree as ET
from typing import List, Dict, Any, Iterable, Optional, Tuple
import logging
from src.config.defaults import ENTREZ_EMAIL, ENTREZ_API_KEY, HTTP_TIMEOUT, USER_AGENT
from src.io.docdb import DocDB

EUTILS = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
HEADERS = {"User-Agent": USER_AGENT, "Accept": "application/json"}
log = logging.getLogger("entrez")

# add near top of file (after imports)
_LANG_MAP = {
    "eng": "English", "en": "English", "english": "English",
    "spa": "Spanish", "es": "Spanish", "spanish": "Spanish",
    "por": "Portuguese", "pt": "Portuguese", "portuguese": "Portuguese",
    "fra": "French", "fre": "French", "fr": "French", "french": "French",
    "ger": "German", "deu": "German", "de": "German", "german": "German",
    # extend as needed
}

def _norm_lang_name(s: str | None) -> str | None:
    if not s:
        return None
    key = s.strip().lower()
    return _LANG_MAP.get(key, s)

def esearch(query: str, db: str = "pubmed", retmax: int = 10000, mindate: Optional[int]=None, maxdate: Optional[int]=None, sort: str="date") -> List[str]:
    params = {"db": db, "term": query, "retmode": "json", "retmax": retmax, "sort": sort, "email": ENTREZ_EMAIL}
    if ENTREZ_API_KEY: params["api_key"] = ENTREZ_API_KEY
    if mindate: params["mindate"] = str(mindate)
    if maxdate: params["maxdate"] = str(maxdate)
    r = requests.get(f"{EUTILS}/esearch.fcgi", headers=HEADERS, params=params, timeout=HTTP_TIMEOUT)
    r.raise_for_status()
    return r.json().get("esearchresult", {}).get("idlist", [])

def _parse_pubmed_xml(xml_text: str) -> Dict[str, Dict[str,Any]]:
    out: Dict[str,Dict[str,Any]] = {}
    root = ET.fromstring(xml_text)
    def _join(node) -> str:
        if node is None: return ""
        try: return "".join(node.itertext())
        except Exception: return (getattr(node, "text", None) or "")
    for art in root.findall(".//PubmedArticle"):
        pmid = art.findtext(".//PMID") or ""
        title = _join(art.find(".//ArticleTitle")).strip()
        abs_nodes = art.findall(".//Abstract/AbstractText")
        abstract = " ".join(_join(n).strip() for n in abs_nodes) if abs_nodes else ""
        year = None
        for path in (".//ArticleDate/Year",".//PubDate/Year",".//DateCreated/Year",".//PubDate/MedlineDate"):
            s = art.findtext(path)
            if s:
                m = re.search(r"\d{4}", s)
                if m: year = int(m.group(0)); break
        journal = art.findtext(".//Journal/Title") or ""
        pubtypes = [pt.text for pt in art.findall(".//PublicationTypeList/PublicationType") if pt.text]
        doi = None
        for idn in art.findall(".//ArticleIdList/ArticleId"):
            if (idn.attrib.get("IdType","").lower()=="doi") and idn.text:
                doi = idn.text.strip().lower()
        lang_code = art.findtext(".//Language")
        lang = _norm_lang_name(lang_code)
        out[pmid] = {"pmid": pmid, "title": title, "abstract": abstract, "year": year,
                     "journal": journal, "pub_types": pubtypes, "doi": doi, "language": lang}
    return out

def _efetch_chunk(chunk: List[str]) -> Dict[str, Dict[str,Any]]:
    params = {"db":"pubmed", "retmode":"xml", "rettype":"abstract", "id":",".join(chunk), "email":ENTREZ_EMAIL}
    if ENTREZ_API_KEY: params["api_key"] = ENTREZ_API_KEY
    r = requests.get(f"{EUTILS}/efetch.fcgi", headers={"User-Agent":USER_AGENT}, params=params, timeout=HTTP_TIMEOUT)
    r.raise_for_status()
    return _parse_pubmed_xml(r.text)

def efetch_abstracts(pmids: Iterable[str], chunk_size: int = 200, workers: int = 3, use_cache: bool = True) -> Dict[str, Dict[str,Any]]:
    """Polite parallel efetch with caching + progress logs."""
    pmids = [str(p) for p in pmids]
    if not pmids: return {}

    cache = DocDB() if use_cache else None
    have: Dict[str,Dict[str,Any]] = {}
    todo = pmids

    if cache:
        cached = cache.get_many(pmids)
        if cached:
            for k, v in cached.items():
                have[k] = v
            todo = [p for p in pmids if p not in cached]

    log.info(f"efetch: total={len(pmids)} (cache hit={len(have)}, miss={len(todo)}), chunk={chunk_size}, workers={workers}")

    if not todo:
        return have

    chunks = [todo[i:i+chunk_size] for i in range(0, len(todo), chunk_size)]
    # Courtesy: limit concurrency; add slight pacing between submissions to avoid burst.
    results: Dict[str,Dict[str,Any]] = {}
    submitted = 0
    t0 = time.monotonic()

    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:
        futs = []
        for ch in chunks:
            futs.append(ex.submit(_efetch_chunk, ch))
            submitted += 1
            time.sleep(0.10)  # tiny stagger

        done = 0
        for fut in concurrent.futures.as_completed(futs):
            try:
                part = fut.result()
                results.update(part)
            except Exception as e:
                log.warning(f"efetch chunk failed: {e}")
            done += 1
            if done % 3 == 0 or done == len(chunks):
                dt = time.monotonic() - t0
                log.info(f"efetch progress {done}/{len(chunks)} chunks | items={sum(len(c) for c in chunks[:done])}/{len(todo)} | {dt:.1f}s")

            # polite global pacing (aim ~3-5 req/s overall)
            time.sleep(0.05)

    # merge + persist
    have.update(results)
    if cache and results:
        cache.put_many(results)

    return have



###############################################################################
### FILE: net/icite.py
###############################################################################
from __future__ import annotations
import requests, sqlite3, json, pathlib, time
from typing import Dict, List
from src.config.defaults import ICITE_BASE, HTTP_TIMEOUT, USER_AGENT, DATA_DIR

HEADERS = {"User-Agent": USER_AGENT, "Accept": "application/json"}
CACHE_DIR = pathlib.Path(DATA_DIR) / "cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)
DB_PATH = CACHE_DIR / "icite.sqlite3"

class ICiteCache:
    def __init__(self, path: pathlib.Path = DB_PATH):
        self._conn = sqlite3.connect(str(path))
        self._conn.execute("""CREATE TABLE IF NOT EXISTS pubs(
            pmid TEXT PRIMARY KEY, json TEXT NOT NULL
        )""")
        self._conn.commit()

    def get_many(self, pmids: List[str]) -> Dict[str, dict]:
        if not pmids: return {}
        q = ",".join("?"*len(pmids))
        cur = self._conn.execute(f"SELECT pmid,json FROM pubs WHERE pmid IN ({q})", pmids)
        out = {}
        for pmid, blob in cur.fetchall():
            try: out[pmid] = json.loads(blob)
            except Exception: pass
        return out

    def put_many(self, rows: List[dict]) -> int:
        data = []
        for rec in rows:
            pmid = str(rec.get("pmid") or rec.get("_id") or "")
            if not pmid: continue
            data.append((pmid, json.dumps(rec)))
        if not data: return 0
        self._conn.executemany("INSERT OR REPLACE INTO pubs(pmid,json) VALUES(?,?)", data)
        self._conn.commit()
        return len(data)

def icite_pubs_fetch(pmids: List[str]) -> List[dict]:
    out: List[dict] = []
    base = ICITE_BASE.rstrip("/")
    for i in range(0, len(pmids), 200):
        sub = pmids[i:i+200]
        params = {"pmids": ",".join(sub), "legacy": "false"}
        r = requests.get(base, headers=HEADERS, params=params, timeout=HTTP_TIMEOUT)
        r.raise_for_status()
        data = r.json().get("data", r.json())
        if isinstance(data, list):
            out.extend(data)
        time.sleep(0.34)
    return out

def icite_neighbors_map(pmids: List[str]) -> Dict[str, set]:
    cache = ICiteCache()
    have = cache.get_many(pmids)
    need = [p for p in pmids if p not in have]
    if need:
        rows = icite_pubs_fetch(need)
        cache.put_many(rows)
        for rec in rows:
            have[str(rec.get("pmid") or rec.get("_id"))] = rec
    m = {}
    for p, rec in have.items():
        cited_by = rec.get("citedByPmids", []) or rec.get("cited_by") or []
        refs     = rec.get("citedPmids", [])   or rec.get("references") or []
        m[p] = set(int(x) for x in (cited_by + refs) if x)
    return m

def icite_degree_total(pmid: int) -> int:
    cache = ICiteCache()
    have = cache.get_many([str(pmid)])
    rec = have.get(str(pmid), {})
    cited_by = rec.get("citedByPmids", []) or rec.get("cited_by") or []
    refs     = rec.get("citedPmids", [])   or rec.get("references") or []
    return len(cited_by) + len(refs)



###############################################################################
### FILE: prisma/counts.py
###############################################################################
from __future__ import annotations
from typing import List, Dict, Any
from collections import Counter
from src.config.schema import LedgerRow

def prisma_counts(ledger: List[LedgerRow]) -> Dict[str, Any]:
    N = len(ledger)
    decided = Counter(r.final_decision for r in ledger)
    reasons = Counter(r.final_reason for r in ledger)
    return {
        "records_screened": N,
        "included": decided.get("include", 0),
        "excluded": decided.get("exclude", 0),
        "borderline": decided.get("borderline", 0),
        "exclusions_by_reason": dict(reasons),
    }



###############################################################################
### FILE: screen/features.py
###############################################################################
from __future__ import annotations
from typing import Dict, List, Tuple
import numpy as np
import networkx as nx
import logging
from src.config.schema import Document, Signals
from src.text.embed import embed_texts, embed_docs_with_cache
from src.net.icite import icite_neighbors_map

log = logging.getLogger("features")

def _band_sem(x: float) -> str:
    if x >= 0.90: return "Very high"
    if x >= 0.85: return "High"
    if x >= 0.75: return "Medium"
    return "Low"

def abstract_len_bin(txt: str) -> str:
    if not txt: return "none"
    n = len(txt)
    if n < 400: return "short"
    if n < 2400: return "normal"
    return "long"

def build_embeddings(docs: List[Document]) -> Tuple[np.ndarray, Dict[str,int]]:
    pmids = [d.pmid for d in docs]
    texts = [ (d.title or "") + "\n" + (d.abstract or "") for d in docs ]
    mat, idx = embed_docs_with_cache(pmids, texts)
    return mat, idx

def cosine(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a,b) / (np.linalg.norm(a)*np.linalg.norm(b) + 1e-12))

def compute_signals(
    docs: List[Document],
    emb: np.ndarray,
    idx: Dict[str,int],
    intent_vec: np.ndarray,
    seed_pmids: List[str],
    year_min: int | None
) -> Dict[str, Signals]:
    seed_idx = [idx[p] for p in seed_pmids if p in idx]
    if seed_idx:
        cent = emb[seed_idx].mean(axis=0); cent /= (np.linalg.norm(cent)+1e-12)
    else:
        cent = intent_vec
    pmids = [d.pmid for d in docs]
    neigh_map = icite_neighbors_map(pmids)
    G = nx.Graph()
    for i,p in enumerate(pmids):
        G.add_node(p)
    for p in pmids:
        ns = neigh_map.get(p, set())
        for q in ns:
            if str(q) in idx: G.add_edge(p, str(q))
    seeds = [p for p in seed_pmids if p in idx]
    if seeds:
        personalize = {p: 1.0/len(seeds) for p in seeds}
        ppr = nx.pagerank(G, alpha=0.85, personalization=personalize, max_iter=100)
        vals = np.array(list(ppr.values()), dtype="float32")
        ranks = {k: 100.0 * (float(np.sum(vals <= v)) / max(1,len(vals))) for k,v in ppr.items()}
    else:
        ppr = {p: 0.0 for p in pmids}
        ranks = {p: 0.0 for p in pmids}
    seeds_set = set(seeds)
    signals: Dict[str, Signals] = {}
    for d in docs:
        v = emb[idx[d.pmid]]
        sem_intent = cosine(v, intent_vec)
        sem_seed   = cosine(v, cent)
        neighbors = neigh_map.get(d.pmid, set())
        links_frac = 0.0
        if neighbors:
            links_frac = len(set(str(x) for x in neighbors) & seeds_set) / float(len(neighbors))
        ys = 0.0
        if year_min and d.year is not None:
            ys = max(0.0, min(1.0, (d.year - year_min) / (2025 - year_min)))
        signals[d.pmid] = Signals(
            sem_intent=sem_intent,
            sem_seed=sem_seed,
            graph_ppr_pct=float(ranks.get(d.pmid, 0.0)),
            graph_links_frac=float(links_frac),
            year_scaled=float(ys),
            abstract_len_bin=abstract_len_bin(d.abstract or "")
        )
    return signals

def signal_card(sig) -> str:
    return (f"Signals:\n"
            f"- Semantic match: {_band_sem(sig.sem_intent)} ({sig.sem_intent:.2f}) to intent; "
            f"seed-centroid {_band_sem(sig.sem_seed)} ({sig.sem_seed:.2f}).\n"
            f"- Graph locality: {'High' if sig.graph_ppr_pct>=90.0 else 'Medium' if sig.graph_ppr_pct>=60.0 else 'Low'} — "
            f"links_to_seeds {100.0*sig.graph_links_frac:.1f}%, PPR {sig.graph_ppr_pct:.0f}th pct.\n"
            f"- Abstract: {sig.abstract_len_bin}.")



###############################################################################
### FILE: screen/gates.py
###############################################################################
from __future__ import annotations
from typing import Optional, Tuple
from src.config.schema import Document, PICOS, Reason

PRIMARY_TYPES = {"Randomized Controlled Trial","Clinical Trial","Cohort Studies","Case-Control Studies","Observational Study","Controlled Clinical Trial","Prospective Studies"}
NON_PRIMARY_TYPES = {"Review","Meta-Analysis","Editorial","Letter","Comment","News","Case Reports","Guideline"}
LANG_MAP = {
    "eng": "English", "en": "English", "english": "English",
    "spa": "Spanish", "es": "Spanish", "spanish": "Spanish",
    "por": "Portuguese", "pt": "Portuguese", "portuguese": "Portuguese",
}
REV_LANG = {}

for code_or_name, pretty in LANG_MAP.items():
    REV_LANG.setdefault(pretty, set()).add(code_or_name)

def _norm_lang(s: str | None) -> str | None:
    if not s:
        return None
    return LANG_MAP.get(s.strip().lower(), s)

def _lang_ok(doc_lang: str | None, allowed: list[str]) -> bool:
    if not doc_lang or not allowed:
        return True
    dl = _norm_lang(doc_lang)
    allowed_norm = {_norm_lang(a) or a for a in allowed}
    if dl in allowed_norm:
        return True
    # also accept if doc code matches one of the allowed names
    for a in allowed_norm:
        for alias in REV_LANG.get(a, set()):
            if doc_lang.strip().lower() == alias:
                return True
    return False

def objective_gate(doc: Document, picos: PICOS) -> Optional[Tuple[str, Reason]]:
    if picos.year_min and doc.year is not None and doc.year < int(picos.year_min):
        return ("year", "year")
    if picos.languages and doc.language and not _lang_ok(doc.language, picos.languages):
        return ("language", "language")
    if (not (doc.title or "").strip()) and (not (doc.abstract or "").strip()):
        return ("insufficient_info", "insufficient_info")
    if set(doc.pub_types) & NON_PRIMARY_TYPES:
        return ("design_mismatch", "design_mismatch")
    return None

def is_primary_design(doc: Document) -> bool:
    return bool(set(doc.pub_types) & PRIMARY_TYPES)



###############################################################################
### FILE: screen/llm_decide.py
###############################################################################
# src/screen/llm_decide.py
from __future__ import annotations
from typing import List, Dict, Any
import math
from src.config.schema import Criteria, Document, DecisionLLM
from src.text.llm import chat_json
from src.text.prompts import P1_SYSTEM
from src.screen.features import signal_card

_ALLOWED = {
    "design_mismatch","population_mismatch","intervention_mismatch",
    "language","year","insufficient_info","off_topic"
}

# JSON Schema LM Studio can enforce (strict)
DECISION_SCHEMA: Dict[str, Any] = {
    "name": "decision",
    "schema": {
        "type": "object",
        "additionalProperties": False,
        "properties": {
            "pmid": {"type": "string"},
            "decision": {"type": "string", "enum": ["include","exclude","borderline"]},
            "primary_reason": {"type": "string", "enum": sorted(list(_ALLOWED))},
            "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0},
            "evidence": {
                "type": "object",
                "additionalProperties": False,
                "properties": {
                    "population_quote": {"type": "string"},
                    "intervention_quote": {"type": "string"},
                    "design_evidence": {"type": "string"},
                    "notes": {"type": "string"}
                },
                "required": ["population_quote","intervention_quote","design_evidence","notes"]
            }
        },
        "required": ["pmid","decision","primary_reason","confidence","evidence"]
    }
}

def _norm_str(x: Any) -> str:
    if x is None: return ""
    s = str(x)
    return s.strip()

def _canonical_reason(txt: str) -> str:
    t = _norm_str(txt).lower()
    if t in _ALLOWED:
        return t
    # heuristic mapping from free-text to our enums
    if any(k in t for k in ["observational", "cohort", "case-control", "registry", "review", "meta-"]):
        return "design_mismatch"
    if any(k in t for k in ["randomized", "rct", "trial design mismatch", "not randomized"]):
        return "design_mismatch"
    if any(k in t for k in ["population", "pediatric", "child", "children", "adult", "adolescent", "mismatch pop"]):
        return "population_mismatch"
    if any(k in t for k in ["intervention", "wrong treatment", "not cryoablation", "different procedure"]):
        return "intervention_mismatch"
    if "language" in t:
        return "language"
    if "year" in t or "date" in t or "older than" in t:
        return "year"
    if any(k in t for k in ["insufficient", "no abstract", "missing", "unclear"]):
        return "insufficient_info"
    if any(k in t for k in ["off-topic", "not relevant", "unrelated", "irrelevant"]):
        return "off_topic"
    # default bucket
    return "off_topic"

def _normalize_evidence(e: Any) -> Dict[str,str]:
    if not isinstance(e, dict): e = {}
    return {
        "population_quote": _norm_str(e.get("population_quote", "")),
        "intervention_quote": _norm_str(e.get("intervention_quote", "")),
        "design_evidence": _norm_str(e.get("design_evidence", "")),
        "notes": _norm_str(e.get("notes", "")),
    }

def _coerce_decision(js: Dict[str, Any]) -> DecisionLLM:
    pmid = _norm_str(js.get("pmid", ""))
    decision = _norm_str(js.get("decision", "")).lower()
    if decision not in {"include","exclude","borderline"}:
        # weak fallback
        decision = "borderline"
    reason = _canonical_reason(js.get("primary_reason",""))
    # keep in [0,1]
    try:
        conf = float(js.get("confidence", 0.5))
        if not math.isfinite(conf): conf = 0.5
    except Exception:
        conf = 0.5
    conf = max(0.0, min(1.0, conf))
    ev = _normalize_evidence(js.get("evidence", {}))
    return DecisionLLM(pmid=pmid, decision=decision, primary_reason=reason, confidence=conf, evidence=ev)

def llm_decide_batch(criteria: Criteria, docs: List[Document], signals_map: dict, temperature: float = 0.1) -> List[DecisionLLM]:
    out: List[DecisionLLM] = []
    for d in docs:
        sig = signals_map[d.pmid]
        user = (
            f"CRITERIA_JSON:\n{criteria.model_dump_json()}\n\n"
            f"RECORD:\n{d.model_dump_json()}\n\n"
            f"{signal_card(sig)}\n"
            "Return the JSON now."
        )
        # 1) Try with schema (strict)
        try:
            raw = chat_json(P1_SYSTEM, user, temperature=temperature, max_tokens=600, schema=DECISION_SCHEMA)
            out.append(_coerce_decision(raw))
            continue
        except Exception:
            pass

        # 2) Fall back to plain JSON with retries/repair inside chat_json
        raw = chat_json(P1_SYSTEM, user, temperature=0.0, max_tokens=600, schema=None)
        out.append(_coerce_decision(raw))
    return out



###############################################################################
### FILE: screen/regressor.py
###############################################################################
from __future__ import annotations
from typing import Dict, List, Tuple
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier

PRIMARY_TYPES = {"Randomized Controlled Trial","Clinical Trial","Cohort Studies","Case-Control Studies","Observational Study","Controlled Clinical Trial","Prospective Studies"}
NON_PRIMARY_TYPES = {"Review","Meta-Analysis","Editorial","Letter","Comment","News","Case Reports","Guideline"}

def featurize_row(sig, doc) -> np.ndarray:
    # Continuous features + simple one-hots
    f = [
        sig.sem_intent, sig.sem_seed, sig.graph_ppr_pct/100.0, sig.graph_links_frac,
        sig.year_scaled,
        1.0 if sig.abstract_len_bin=="none" else 0.0,
        1.0 if sig.abstract_len_bin=="short" else 0.0,
        1.0 if sig.abstract_len_bin=="normal" else 0.0,
        1.0 if sig.abstract_len_bin=="long" else 0.0,
        1.0 if (set(doc.pub_types) & PRIMARY_TYPES) else 0.0,
        1.0 if (set(doc.pub_types) & NON_PRIMARY_TYPES) else 0.0,
    ]
    return np.array(f, dtype="float32")

class OnlineRegressor:
    def __init__(self):
        self.scaler = StandardScaler(with_mean=True, with_std=True)
        self.clf = SGDClassifier(loss="log_loss", penalty="l2", alpha=1e-4, max_iter=1000, tol=1e-3, random_state=17)
        self._fitted = False

    def fit_bootstrap(self, X: np.ndarray, y: np.ndarray):
        # Scale then partial_fit
        self.scaler.fit(X)
        Xs = self.scaler.transform(X)
        self.clf.partial_fit(Xs, y, classes=np.array([0,1]))
        self._fitted = True

    def partial_update(self, X: np.ndarray, y: np.ndarray):
        Xs = self.scaler.transform(X)
        self.clf.partial_fit(Xs, y)

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        Xs = self.scaler.transform(X)
        p = self.clf.predict_proba(Xs)[:,1]
        return p



###############################################################################
### FILE: text/embed.py
###############################################################################
from __future__ import annotations
import os, time, json, math, subprocess, shutil, logging
from typing import List, Dict, Tuple, Optional
import numpy as np, requests

from src.config.defaults import (
    LMSTUDIO_BASE, LMSTUDIO_EMB, HTTP_TIMEOUT, USER_AGENT,
    EMB_BATCH, EMB_AUTO_BATCH, EMB_MAX_CHARS_PER_DOC,
    EMB_RETRY_BACKOFF_S, EMB_RETRY_MAX
)
from src.io.vecdb import VecDB

HEADERS = {"Content-Type":"application/json","User-Agent":USER_AGENT}
log = logging.getLogger("embed")

def _setup_logger_once():
    if not logging.getLogger().handlers:
        logging.basicConfig(
            level=os.environ.get("LOGLEVEL","INFO"),
            format="%(asctime)s | %(levelname)s | %(name)s: %(message)s",
            datefmt="%H:%M:%S"
        )

_setup_logger_once()

def _estimate_tokens(s: str) -> int:
    # Rough: ~ 1 token ≈ 4 chars (English-ish)
    return max(1, int(len(s) / 4))

def _truncate(s: str, max_chars: int) -> str:
    if max_chars and len(s) > max_chars:
        return s[:max_chars]
    return s

def _detect_vram_mb() -> Optional[int]:
    # Try torch first
    try:
        import torch  # type: ignore
        if torch.cuda.is_available():
            free, total = torch.cuda.mem_get_info()  # bytes
            return int(free // (1024*1024))
    except Exception:
        pass
    # nvidia-smi
    nvsmi = shutil.which("nvidia-smi")
    if nvsmi:
        try:
            out = subprocess.check_output(
                [nvsmi, "--query-gpu=memory.free", "--format=csv,noheader,nounits"],
                stderr=subprocess.DEVNULL, text=True, timeout=2.0
            )
            vals = [int(x.strip()) for x in out.strip().splitlines() if x.strip().isdigit()]
            if vals:
                # if multi-GPU, take max free
                return max(vals)
        except Exception:
            pass
    return None

def _choose_batch_size(auto: bool, default_batch: int) -> int:
    if not auto:
        return default_batch
    free_mb = _detect_vram_mb()
    if free_mb is None:
        # unknown; be conservative
        return min(default_batch, 16)
    # Coarse heuristic: allow more if plenty free, otherwise tighten
    if free_mb < 2000:   return 8
    if free_mb < 4000:   return 12
    if free_mb < 8000:   return 16
    if free_mb < 12000:  return 20
    if free_mb < 20000:  return 24
    return min(32, default_batch)

def _post_embeddings(payload_inputs: List[str], model: str, timeout: int, retries: int) -> List[List[float]]:
    url = f"{LMSTUDIO_BASE.rstrip('/')}/v1/embeddings"
    body = {"model": model, "input": payload_inputs}
    backoff = EMB_RETRY_BACKOFF_S
    for attempt in range(retries+1):
        try:
            r = requests.post(url, headers=HEADERS, json=body, timeout=timeout)
            r.raise_for_status()
            data = r.json()["data"]
            return [d["embedding"] for d in data]
        except Exception as e:
            if attempt >= retries:
                raise
            time.sleep(backoff)
            backoff *= 1.8
    raise RuntimeError("unreachable")

def embed_texts(texts: List[str], batch: Optional[int] = None, model: Optional[str] = None) -> np.ndarray:
    """
    Generic embedders (no cache). Still auto-batches & truncates.
    """
    model = model or LMSTUDIO_EMB
    bsz = _choose_batch_size(EMB_AUTO_BATCH, batch if batch is not None else EMB_BATCH)
    work: List[str] = [_truncate(t or "", EMB_MAX_CHARS_PER_DOC) for t in texts]
    N = len(work)
    out: List[List[float]] = []
    t0 = time.time()
    log.info(f"Embedding {N} texts | model={model} | batch={bsz} | max_chars={EMB_MAX_CHARS_PER_DOC}")
    for i in range(0, N, bsz):
        sub = work[i:i+bsz]
        embs = _post_embeddings(sub, model=model, timeout=HTTP_TIMEOUT, retries=EMB_RETRY_MAX)
        out.extend(embs)
        if (i//bsz) % 5 == 0:
            done = i+len(sub)
            log.info(f"  progress {done}/{N} ({100.0*done/max(1,N):.1f}%)")
    arr = np.array(out, dtype="float32")
    # normalize
    arr /= (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12)
    log.info(f"Embedding done in {time.time()-t0:.1f}s")
    return arr

def embed_docs_with_cache(pmids: List[str], texts: List[str], model: Optional[str] = None) -> Tuple[np.ndarray, Dict[str,int]]:
    """
    pmids/texts aligned. Uses sqlite cache keyed by (pmid, model) + content hash.
    Auto-batches requests and logs progress. Returns matrix in pmids order.
    """
    assert len(pmids) == len(texts)
    model = model or LMSTUDIO_EMB
    bsz = _choose_batch_size(EMB_AUTO_BATCH, EMB_BATCH)

    # Precompute trunc, hash
    trunc = [_truncate(t or "", EMB_MAX_CHARS_PER_DOC) for t in texts]
    hashes = [VecDB.make_hash(model, t) for t in trunc]

    db = VecDB()
    have = db.get_many([(pmids[i], model) for i in range(len(pmids))])

    # Decide which need (missing or hash changed)
    to_do_idx: List[int] = []
    cached_vecs: Dict[int, np.ndarray] = {}
    need_hash_rows: List[Tuple[str,str]] = []
    for i, p in enumerate(pmids):
        key = (p, model)
        if key in have:
            h, dim, blob = have[key]
            if h == hashes[i]:
                cached_vecs[i] = VecDB.unpack_vec(blob, dim)
            else:
                to_do_idx.append(i)
        else:
            to_do_idx.append(i)

    log.info(f"Embeddings cache: hit={len(cached_vecs)} | miss={len(to_do_idx)} | model={model} | batch={bsz}")

    # Batch the misses
    rows_for_db: List[Tuple[str,str,str,int,bytes]] = []
    if to_do_idx:
        N = len(to_do_idx)
        t0 = time.time()
        for s in range(0, N, bsz):
            idxs = to_do_idx[s:s+bsz]
            payload = [trunc[i] for i in idxs]
            embs = _post_embeddings(payload, model=model, timeout=HTTP_TIMEOUT, retries=EMB_RETRY_MAX)
            for j, i in enumerate(idxs):
                vec = np.array(embs[j], dtype="float32")
                vec /= (np.linalg.norm(vec) + 1e-12)
                cached_vecs[i] = vec
                rows_for_db.append((pmids[i], model, hashes[i], int(vec.size), VecDB.pack_vec(vec)))
            if (s//bsz) % 5 == 0:
                done = s + len(idxs)
                log.info(f"  embed new {done}/{N} ({100.0*done/max(1,N):.1f}%)")
        db.upsert_many(rows_for_db)
        log.info(f"Embedded {N} new vecs in {time.time()-t0:.1f}s (cached saved to DB).")

    # Assemble output in pmids order
    dim = next(iter(cached_vecs.values())).size if cached_vecs else 0
    mat = np.zeros((len(pmids), dim), dtype="float32")
    for i, p in enumerate(pmids):
        mat[i,:] = cached_vecs[i]
    idx = {pmids[i]: i for i in range(len(pmids))}
    return mat, idx



###############################################################################
### FILE: text/llm.py
###############################################################################
# src/text/llm.py
from __future__ import annotations
import json, re, requests, time
from typing import Optional, Dict, Any
from src.config.defaults import LMSTUDIO_BASE, LMSTUDIO_CHAT, HTTP_TIMEOUT, USER_AGENT

HEADERS = {"Content-Type":"application/json","User-Agent":USER_AGENT}

_FENCE_RE = re.compile(r"^\s*```[a-zA-Z0-9]*\s*|\s*```\s*$", re.M)

def _strip_md_fences(s: str) -> str:
    return _FENCE_RE.sub("", s).strip()

def _extract_json_block(text: str) -> str:
    m = re.search(r"\{[\s\S]*\}", text)
    if m: return m.group(0)
    m = re.search(r"\[[\s\S]*\]", text)
    if m: return m.group(0)
    return text.strip()

def _quick_sanitize(js: str) -> str:
    s = js
    s = s.replace("\u201c", '"').replace("\u201d", '"').replace("\u2018", "'").replace("\u2019", "'")
    s = re.sub(r",\s*(\}|\])", r"\1", s)
    s = s.replace("```", "")
    return s

def _post_chat(body: dict) -> str:
    url = f"{LMSTUDIO_BASE.rstrip('/')}/v1/chat/completions"
    r = requests.post(url, headers=HEADERS, json=body, timeout=HTTP_TIMEOUT)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def _repair_json_via_llm(bad: str) -> str:
    # Last-ditch fixer: rewrite as strict JSON
    system = (
        "You are a JSON fixer. Convert the given content into STRICT, VALID JSON.\n"
        "Rules:\n"
        "- Use only double-quoted keys and values.\n"
        "- Escape internal quotes inside strings.\n"
        "- No markdown, no comments, no trailing commas.\n"
        "- Keep existing keys if present.\n"
        "Return JSON ONLY."
    )
    body = {
        "model": LMSTUDIO_CHAT,
        "messages": [{"role":"system","content":system},{"role":"user","content":bad}],
        "temperature": 0.0,
        "max_tokens": 900,
        "stream": False
    }
    content = _post_chat(body)
    content = _strip_md_fences(content)
    repaired = _extract_json_block(content)
    repaired = _quick_sanitize(repaired)
    return repaired

def _mk_schema_response_format(schema: dict) -> dict:
    # LM Studio supports JSON Schema with strict mode
    # https://lmstudio.ai/docs/guides/structured-output
    return {
        "type": "json_schema",
        "json_schema": {
            "name": schema.get("name", "payload"),
            "schema": schema["schema"],
            "strict": True
        }
    }

def chat_json(
    system: str,
    user: str,
    temperature: float = 0.1,
    max_tokens: int = 700,
    schema: Optional[dict] = None,
    retries: int = 2
) -> dict:
    """
    Try 1: JSON Schema (strict) if provided.
    Try 2: response_format=json_object (plain JSON mode).
    Try 3: re-ask with stricter system "JSON ONLY".
    Try 4: string-repair via fixer LLM.
    """
    # --- Attempt 1: schema (if provided)
    if schema is not None:
        body = {
            "model": LMSTUDIO_CHAT,
            "messages": [{"role":"system","content":system},{"role":"user","content":user}],
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False,
            "response_format": _mk_schema_response_format(schema),
        }
        try:
            content = _post_chat(body)
            raw = _quick_sanitize(_extract_json_block(_strip_md_fences(content)))
            return json.loads(raw)
        except Exception:
            # fall through to next attempts
            pass

    # --- Attempt 2: json_object mode
    body2 = {
        "model": LMSTUDIO_CHAT,
        "messages": [{"role":"system","content":system},{"role":"user","content":user}],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False,
        "response_format": {"type": "json_object"}  # usually supported; if not, server ignores
    }
    try:
        content = _post_chat(body2)
        raw = _quick_sanitize(_extract_json_block(_strip_md_fences(content)))
        return json.loads(raw)
    except Exception:
        pass

    # --- Attempt 3: re-ask with an ultra-strict system prompt
    strict_system = (
        "Return STRICT JSON only. No prose. No markdown. No trailing commas. "
        "If an enum is requested, ONLY use one of the allowed values."
    )
    body3 = {
        "model": LMSTUDIO_CHAT,
        "messages": [
            {"role":"system","content":strict_system + "\n\n" + system},
            {"role":"user","content":user}
        ],
        "temperature": 0.0,
        "max_tokens": max_tokens,
        "stream": False
    }
    try:
        content = _post_chat(body3)
        raw = _quick_sanitize(_extract_json_block(_strip_md_fences(content)))
        return json.loads(raw)
    except Exception:
        pass

    # --- Attempt 4: repair whatever we got from attempt 2/3 if any, else from the user prompt echo
    # Re-run the best-effort request once and try repair
    try:
        content = _post_chat(body2)
    except Exception:
        content = _post_chat(body3)

    raw = _quick_sanitize(_extract_json_block(_strip_md_fences(content)))
    try:
        return json.loads(raw)
    except Exception:
        repaired = _repair_json_via_llm(raw)
        return json.loads(repaired)



###############################################################################
### FILE: text/prompts.py
###############################################################################
P0_SYSTEM = """
You are configuring a PRISMA title/abstract screening. From the user's description and preferences,
produce strict, machine-parseable criteria and PubMed boolean queries.

Output requirements:
- Return JSON ONLY (no code fences).
- All strings must be valid JSON strings (escape inner double quotes in query strings).
- Keys must include exactly:
  picos (object with keys: population, intervention, comparison, outcomes (array of strings),
         study_design (array of strings), year_min (int or null), languages (array of full language names)),
  inclusion_criteria (object),
  exclusion_criteria (object),
  reason_taxonomy (array of enums from: ["design_mismatch","population_mismatch","intervention_mismatch","language","year","insufficient_info","off_topic"]),
  boolean_queries (object of strings).

Do not add any prose.
"""


def p0_user_prompt(intent_text: str) -> str:
    return f"INTENT/PREFERENCES:\n{intent_text}\n\nProduce the JSON now."

P1_SYSTEM = """
You are a PRISMA title/abstract screener. Decide INCLUDE, EXCLUDE, or BORDERLINE strictly
from CRITERIA_JSON and the record. Use pub_types only for design; never infer design from title words.
Do NOT exclude solely because outcomes are not stated; mark BORDERLINE instead.
Treat Signals as hints (not sufficient reasons).
Return JSON ONLY matching schema:
{ "pmid": "...", "decision": "include|exclude|borderline", "primary_reason": "...",
  "confidence": 0.0, "evidence": { "population_quote": "...", "intervention_quote": "...",
  "design_evidence": "pub_types: [...] or empty", "notes": "..." } }.
"""



